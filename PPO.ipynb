{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"16Qu1EYnYFWWAJFxt_lfOtT1JDd8iaGYh","authorship_tag":"ABX9TyPmZquR7sh5YupncyPem1e2"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"THYBlH_niIxB"},"source":["# Note versione"]},{"cell_type":"markdown","metadata":{"id":"Yc_bqk0viIuc"},"source":["Considero questa versione come base per PPO.\n","\n","L'agente riceve previsioni del futuro prezzo di chiusura fatte dalla rete di regressione come osservazione, a cui viene concatenata la composizione del portafoglio."]},{"cell_type":"markdown","metadata":{"id":"Q4mywZkniImU"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"1YWMjC7S4mFT"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.distributions import Normal\n","from sklearn.preprocessing import StandardScaler\n","\n","import math\n","import copy\n","import random\n","import pickle\n","import pandas as pd\n","import numpy as np\n","from matplotlib.pyplot import *\n","\n","try:\n","    import wandb\n","except:\n","    !pip install wandb -qqq\n","    import wandb\n","\n","import time\n","from datetime import datetime\n","\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","\n","import os\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fztc6UBPiWJi"},"source":["# Useful functions"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YEK_F-g5CpT","executionInfo":{"status":"ok","timestamp":1649612082362,"user_tz":-120,"elapsed":10,"user":{"displayName":"Gabriele Agostino","userId":"09755571968699249879"}},"outputId":"1f45f7ff-207c-4cc0-fe4b-76eb261f2711"},"source":["DEVICE = torch.device( \"cuda\" )\n","print(\"DEVICE: \", torch.cuda.get_device_name(DEVICE))\n","\n","def from_numpy( x ):\n","    return torch.from_numpy( x ).type( torch.float ).to( DEVICE )\n","\n","def to_numpy( x ):\n","    return x.detach().cpu().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE:  Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","metadata":{"id":"VyMPerLJ5Cmw"},"source":["class SingleInstanceMetaClass(type):\n","    def __init__(self, name, bases, dic):\n","        self.__single_instance = None\n","        super().__init__(name, bases, dic)\n"," \n","    def __call__(cls, *args, **kwargs):\n","        if cls.__single_instance:\n","            return cls.__single_instance\n","        single_obj = cls.__new__(cls)\n","        single_obj.__init__(*args, **kwargs)\n","        cls.__single_instance = single_obj\n","        return single_obj"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyK-fsWjiqEK"},"source":["# Regression net"]},{"cell_type":"code","metadata":{"id":"I-G3L1r744v-"},"source":["class NormalizationLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, epsilon = 1e-6 ):\n","        super( NormalizationLayer, self ).__init__()\n","        self.epsilon = epsilon\n","        self.w = nn.Parameter( torch.ones( d_model ) )\n","        self.b = nn.Parameter( torch.zeros( d_model ) )\n","        \n","    def forward( self, x ):\n","        mean = x.mean( dim = -1, keepdim = True )\n","        std = x.std( dim = -1, keepdim = True )\n","        return self.w * ( x - mean ) / ( std + self.epsilon ) + self.b\n","\n","# as in https://timeseriestransformer.readthedocs.io/en/latest/README.html#installation the embedding layer is replaced by a generic linear layer\n","class EmbeddingLayer( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( EmbeddingLayer, self ).__init__()\n","        self.embedding = nn.Linear(in_features, out_features)\n","        \n","    def forward( self, x ):\n","        return self.embedding(x)\n","\n","\n","class Time2Vec( nn.Module ):\n","    \"\"\"\n","    Time2Vec implementation\n","    \n","    parameters\n","    in_features: int\n","        number of features of the data\n","    out_features: int\n","        number of out features (k in the original paper)\n","    activation_function: function or function like\n","        the activation function. If none, sin is used\n","    \"\"\"\n","    \n","    def __init__( self, in_features, out_features, activation_function = None ):\n","        super(Time2Vec, self).__init__()\n","        \n","        #i = 0\n","        self.linear_transformation = nn.Linear( in_features, 1, bias = True )\n","        \n","        #1 <= i <= k\n","        self.periodic_transformation = nn.Linear( in_features, out_features - 1, bias = True)\n","        \n","        if activation_function == None: \n","            self.activation_function = torch.sin\n","        \n","    def forward( self, x ):\n","        # x has shape (sequence_length, in_features)\n","        \n","        # linear_x has shape (sequence_length, 1)\n","        linear_x = self.linear_transformation( x )\n","        \n","        # periodic_x has shape (sequence_length, out_features - 1)\n","        periodic_x = self.activation_function( self.periodic_transformation(x) )\n","        \n","        # periodic_x has shape (sequence_length, out_features )\n","        out = torch.cat( [linear_x, periodic_x], dim = -1 )\n","        \n","        return out\n","\n","\n","class Query( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Query, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","        \n","           \n","            \n","class Key( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Key, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","    \n","    \n","            \n","class Value( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Value, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","    \n","\n","class MultiHeadAttention( nn.Module ):\n","    \n","    def __init__( self, in_features, d_model, num_heads ):\n","        super( MultiHeadAttention, self ).__init__()\n","        \n","        assert d_model % num_heads == 0\n","        \n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.depth = d_model // num_heads\n","        \n","        self.query = Query( in_features, d_model ) \n","        self.key = Key( in_features, d_model ) \n","        self.value = Value( in_features, d_model )\n","        \n","    def attention( self, query, key, value ):\n","        matmul_qk = torch.matmul( query, key.transpose(-2, -1) )  \n","        scaled_attention_logits = matmul_qk / math.sqrt( self.depth )\n","        attention_weights = F.softmax( scaled_attention_logits, dim = -1 )\n","        output = torch.matmul( attention_weights, value )\n","        return output, attention_weights\n","        \n","    def forward( self, query, key, value ):\n","        \n","        companies = query.size(0)\n","\n","        #linear transformation [ assets, sequence_length, d_model]\n","        query = self.query( query )\n","        key = self.key( key )\n","        value = self.value( value )\n"," \n","        # splitting in num_heads -> [ assets, sequence_length, num_heads, depth]\n","        query = query.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","        key = key.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","        value = value.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","\n","        # [ assets, sequence_length, num_heads, depth] \n","        # -> [ assets, num_heads, sequence_length, depth]\n","        query = query.transpose( 2, 1 )\n","        key = key.transpose( 2, 1 )\n","        value = value.transpose( 2, 1 )\n","\n","        # applying attention\n","        # output [ assets, num_heads, sequence_length, depth]\n","        # attention_weights [ assets, num_heads, sequence_length_q, sequence_length_k]\n","        output, attention_weights = self.attention( query, key, value )\n","        \n","        # [ assets, num_heads, sequence_length, depth]\n","        # -> [ assets, sequence_length, num_heads, depth]\n","        output = output.transpose( 2, 1 )\n","        \n","        # [ assets, seq_len, d_model ]\n","        return output.contiguous().view( companies, -1 , self.d_model)\n","\n","\n","class FeedForward( nn.Module ):\n","    \n","    def __init__( self, in_features, n_layers, d_layers, out_features, dropout ):\n","        super( FeedForward, self ).__init__()\n","        \n","        layers = nn.ModuleList([])\n","        \n","        if n_layers > 1:\n","            layers.append( nn.Linear( in_features, d_layers ) )\n","            layers.append( nn.LeakyReLU() )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( nn.Linear( d_layers, d_layers))\n","                layers.append( nn.LeakyReLU() )\n","                layers.append( nn.Dropout( dropout ) )\n","            layers.append( nn.Linear( d_layers, out_features ) )\n","        else:\n","            layers.append( nn.Linear( in_features, out_features ))\n","\n","        self.net = nn.Sequential( *layers )\n","        \n","    def forward( self, x ):\n","        x = self.net(x)\n","        return x\n","\n","\n","class EncoderLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, in_features, n_layers_ff, d_layers_ff, num_heads, dropout ):\n","        super( EncoderLayer, self ).__init__()\n","        self.norm_layer1 = NormalizationLayer( d_model )\n","        self.norm_layer2 = NormalizationLayer( d_model )\n","        self.dropout_layer1 = nn.Dropout( dropout )\n","        self.dropout_layer2 = nn.Dropout( dropout )\n","        self.mha = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.ffnn = FeedForward( in_features = d_model,\n","                                 n_layers = n_layers_ff,\n","                                 d_layers = d_layers_ff,\n","                                 out_features = d_model, \n","                                 dropout = dropout )\n","        \n","    def forward( self, x ):\n","        x2 = self.mha( x, x, x )\n","        x = self.norm_layer1( x + self.dropout_layer1(x2) )\n","        x2 = self.ffnn( x )\n","        return self.norm_layer2( x + self.dropout_layer2(x2) )\n","\n","\n","class DecoderLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, in_features, n_layers_ff, d_layers_ff, num_heads, dropout  ):\n","        super( DecoderLayer, self ).__init__()\n","        self.norm_layer1 = NormalizationLayer( d_model )\n","        self.norm_layer2 = NormalizationLayer( d_model )\n","        self.norm_layer3 = NormalizationLayer( d_model )\n","        self.dropout_layer1 = nn.Dropout( dropout )\n","        self.dropout_layer2 = nn.Dropout( dropout )\n","        self.dropout_layer3 = nn.Dropout( dropout )\n","        self.mha1 = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.mha2 = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.ffnn = FeedForward( in_features = d_model,\n","                                 n_layers = n_layers_ff,\n","                                 d_layers = d_layers_ff,\n","                                 out_features = d_model, \n","                                 dropout = dropout )\n","        \n","    def forward( self, x, encoder_output ):\n","        x2 = self.mha1( x, x, x )\n","        x = self.norm_layer1( x + self.dropout_layer1(x2) )\n","        x2 = self.mha2( query = x, key = encoder_output, value = encoder_output )\n","        x = self.norm_layer2( x + self.dropout_layer2(x2) )\n","        x2 = self.ffnn( x )\n","\n","        return self.norm_layer3( x + self.dropout_layer3(x2) )\n","\n","\n","\n","class Encoder( nn.Module ):\n","    \n","    def __init__( self, \n","                 d_model,\n","                 num_layers,\n","                 num_heads,\n","                 t2v_units,\n","                 sequence_length,\n","                 num_features,\n","                 num_ff_layers,\n","                 dim_ff_layers,\n","                 dropout\n","                ):\n","        super( Encoder, self ).__init__()\n","        \n","        self.t2v_layer = Time2Vec( in_features = num_features,\n","                                   out_features =  t2v_units )\n","        self.embedding_layer = EmbeddingLayer( num_features + t2v_units, d_model )\n","        self.encoder_layers = self.get_layers( num_layers = num_layers,\n","                                               d_model = d_model, \n","                                               num_ff_layers = num_ff_layers, \n","                                               dim_ff_layers = dim_ff_layers, \n","                                               num_heads = num_heads,\n","                                               dropout = dropout )\n","        \n","    def get_layers( self, num_layers, d_model, num_ff_layers, dim_ff_layers, num_heads, dropout ):\n","        return nn.ModuleList( [EncoderLayer( d_model = d_model, \\\n","                                             in_features = d_model, \\\n","                                             n_layers_ff = num_ff_layers, \\\n","                                             d_layers_ff = dim_ff_layers, \\\n","                                             num_heads = num_heads,\n","                                             dropout = dropout ) \\\n","                                for _ in range(num_layers)] )\n","        \n","    def forward( self, x ):\n","        #input is [companies, sequence_length, features]\n","\n","        #t2v output is [companies, sequence_length, t2v_units]\n","        x2 = self.t2v_layer(x)\n","\n","        #x is [companies, sequence_length, features + t2v_units]\n","        x = torch.cat( [ x, x2 ], dim = -1)\n","\n","        #x is [companies, sequence_length, d_model]\n","        x = self.embedding_layer( x )\n","\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x)\n","\n","        return x\n","\n","\n","class Decoder( nn.Module ):\n","    \n","    def __init__( self,\n","                  d_model,\n","                  num_layers, \n","                  num_heads, \n","                  t2v_units, \n","                  sequence_length,\n","                  num_features,\n","                  num_ff_layers,\n","                  dim_ff_layers, \n","                  dropout ):\n","        super( Decoder, self ).__init__()\n","        self.t2v_layer = Time2Vec( in_features = num_features,\n","                                   out_features =  t2v_units )\n","        self.embedding_layer = EmbeddingLayer( num_features + t2v_units, d_model )\n","        self.decoder_layers = self.get_layers( num_layers = num_layers,\n","                                               d_model = d_model, \n","                                               num_ff_layers = num_ff_layers, \n","                                               dim_ff_layers = dim_ff_layers, \n","                                               num_heads = num_heads, \n","                                               dropout = dropout )\n","        \n","    def get_layers( self, num_layers, d_model, num_ff_layers, dim_ff_layers, num_heads, dropout ):\n","        return nn.ModuleList( [DecoderLayer( d_model = d_model, \\\n","                                             in_features = d_model, \\\n","                                             n_layers_ff = num_ff_layers, \\\n","                                             d_layers_ff = dim_ff_layers, \\\n","                                             num_heads = num_heads,\n","                                             dropout = dropout ) \\\n","                                for _ in range(num_layers)] )\n","        \n","    def forward( self, x, encoder_output ):\n","        #input x is [companies, sequence_length, features]\n","        #encoder output is [ companies, sequence_length, encoder dimension]\n","\n","        #t2v output is [batch size, companies, sequence_length, t2v_units]\n","        x2 = self.t2v_layer(x)\n","\n","        #x is [batch size, companies, sequence_length, features + t2v_units]\n","        x = torch.cat( [ x, x2 ], dim = -1)\n","\n","        #x is [batch size, companies, sequence_length, d_model]\n","        x = self.embedding_layer( x )\n","\n","        for decoder_layer in self.decoder_layers:\n","            x = decoder_layer(x, encoder_output)\n","\n","        return x\n","\n","\n","\n","class Transformer( nn.Module ):\n","\n","    def __init__( self, \n","                 dim_transformer, \n","                 encoder_sequence_length, \n","                 decoder_sequence_length,\n","                 num_layers, \n","                 num_heads,\n","                 t2v_units,\n","                 num_features, \n","                 num_ff_layers, \n","                 dim_ff_layers,\n","                 dropout\n","                ):\n","        super ( Transformer, self ).__init__()\n","        self.encoder = Encoder( d_model = dim_transformer, \n","                                num_layers = num_layers,\n","                                num_heads = num_heads,\n","                                t2v_units = t2v_units,\n","                                sequence_length = encoder_sequence_length,\n","                                num_features = num_features,\n","                                num_ff_layers = num_ff_layers,\n","                                dim_ff_layers = dim_ff_layers,\n","                                dropout = dropout\n","                               )\n","        self.decoder = Decoder( d_model = dim_transformer, \n","                                num_layers = num_layers,\n","                                num_heads = num_heads,\n","                                t2v_units = t2v_units,\n","                                sequence_length = decoder_sequence_length,\n","                                num_features = num_features,\n","                                num_ff_layers = num_ff_layers,\n","                                dim_ff_layers = dim_ff_layers,\n","                                dropout = dropout\n","                               ) \n","        \n","        self.decoder_sequence_ffnn = FeedForward( in_features = decoder_sequence_length, n_layers = num_ff_layers, d_layers = dim_ff_layers, out_features = 1, dropout = dropout )\n","\n","\n","        self.decoder_sequence_length = decoder_sequence_length\n","        \n","    def forward( self, x ):\n","        \n","        #[  encoder_sequence_length, companies, features] -> [ companies, encoder_sequence_length, features]\n","        x = x.transpose(1,0) \n","\n","        #input x is for encoder [ companies, encoder_sequence_length, features]\n","        xe = x\n","        \n","        #input xd is for decoder [ companies, decoder_sequence_length, features]\n","        xd = x[:,-self.decoder_sequence_length:]\n","\n","        #[ companies, encoder_sequence_length, d_model_encoder]\n","        encoder_output = self.encoder(xe)\n","\n","        #[ companies, decoder_sequence_length, d_model_decoder]\n","        decoder_output = self.decoder(xd, encoder_output)\n","\n","        #[ companies, decoder_sequence_length, d_model_decoder] -> [ companies, d_model_decoder]\n","        output = self.decoder_sequence_ffnn( decoder_output.transpose(-2,-1) ).squeeze()\n","\n","        return output\n","\n","\n","class RegressionTransformer( nn.Module ):\n","\n","    def __init__( self, params ):\n","        super( RegressionTransformer, self ).__init__()\n","\n","        self.transformer = Transformer( params.dim_transformer, \n","                                        params.encoder_sequence_length, \n","                                        params.decoder_sequence_length,\n","                                        params.num_layers, \n","                                        params.num_heads,\n","                                        params.t2v_units,\n","                                        params.num_features, \n","                                        params.num_ff_layers, \n","                                        params.dim_ff_layers,\n","                                        params.dropout\n","                                        )\n","        \n","        self.ffnn = FeedForward( params.dim_transformer, params.regression_ff_layers, params.dim_regression_ff_layers, 1, params.dropout)\n","\n","    def forward( self, x):\n","        x = self.transformer(x)\n","        x = self.ffnn(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C52Dak9VitpQ"},"source":["# Dataloader and Dataset"]},{"cell_type":"code","metadata":{"id":"-J6gJ5ru44te"},"source":["class Dataloader( metaclass=SingleInstanceMetaClass ):\n","    \"\"\"\n","    Loads return and prices from the previously constructed dataset, a DataFrame, saved in pickle format\n","    \"\"\"\n","\n","    def __init__( self, file_path, moving_average ):\n","        \"\"\"\n","        file path: \n","            the path of the Dataframe in pickle format\n","        moving average:\n","            moving average window size applied to data\n","        \"\"\"\n","        #dataframe is loaded\n","        self.data_df = self.load_df( file_path, moving_average )\n","\n","        #number of assets\n","        self.assets = self.data_df.columns.get_level_values(0).unique()\n","        \n","        #number of features \n","        self.features = self.data_df.columns.get_level_values(1).unique()\n","\n","    def load_df( self, file_path: str, moving_average: int ) -> pd.DataFrame :\n","        \"\"\"\n","        file path: \n","            the path of the Dataframe in pickle format\n","        moving average:\n","            moving average window size applied to data\n","\n","        returns:\n","            dataframe from file_path\n","        \"\"\"\n","        data_df = pd.read_pickle( file_path )\n","        data_df =  data_df.rolling( moving_average ).mean().dropna()\n","        return data_df\n","\n","    def load_prices( self, ) -> np.ndarray:\n","        \"\"\"\n","        returns numpy array of shape (number of days, number of assets, number of features)\n","        containing the OHCLV prices \n","        \"\"\"\n","        prices = []\n","        for asset in self.assets:\n","            to_append = self.data_df[asset][self.features].values \n","            prices.append( to_append )\n","\n","        #prices is [ days, assets, features ]\n","        prices = np.stack( prices, axis = 1)\n","        return prices\n","\n","    def load_returns( self, ):\n","        \"\"\"\n","        returns numpy array of shape (number of days, number of assets, number of features)\n","        \"\"\"\n","        self.returns_df = self.data_df.pct_change().fillna(0)\n","        returns = []\n","        for asset in self.assets:\n","            to_append = self.returns_df[asset][self.features].values\n","            returns.append( to_append )\n","        \n","        #returns is [ days, assets, features ]\n","        returns = np.stack( returns, axis = 1)\n","        return returns\n","\n","class Dataset( metaclass=SingleInstanceMetaClass ):\n","\n","    def __init__( self, params ):\n","        # dataloader instance \n","        self.loader = Dataloader( params.file_path, params.moving_average )\n","\n","        # we store the data for env accessibility\n","        self.returns = self.loader.load_returns() # returns are clippend and standardized\n","        self.true_returns = self.loader.load_returns() # returns are pct variations \n","        self.prices = self.loader.load_prices()\n","\n","        # split the indices for training, validation and testing\n","        self.split_indices( params.test_portion, params.val_portion )\n","\n","        # clipping an normalizing the data\n","        self.scale_data( params.feature_clip, params.vol_clip)\n","\n","        # other useful parameters\n","        #self.episode_length = params.b_size\n","        self.episode_length = params.episode_length\n","        self.encoder_sequence_length = params.encoder_sequence_length\n","\n","    def split_indices( self, test_portion, val_portion ):\n","        \"\"\"\n","        test_portion: float\n","            test portion of the dataset\n","        val_portion: float\n","            validation portion of the dataset\n","\n","        split dataset indices in self.train_indices, self.val_indices, self.test_indices : np.ndarray \n","        in accord with the portions. validation portion and test portion are at the end of dataset \n","        i.e. closer to present. \n","        \"\"\"\n","\n","        num_periods = self.returns.shape[0]\n","        start_train_set_index = 0\n","        start_val_set_index = int( num_periods *( 1 - (test_portion + val_portion) ) )\n","        start_test_set_index = int( num_periods * ( 1 - val_portion ) )\n","\n","        self.train_indices = np.arange(start_train_set_index, start_val_set_index)\n","        self.val_indices = np.arange(start_val_set_index, start_test_set_index)\n","        self.train_val_indices = np.arange(start_train_set_index, start_test_set_index)\n","        self.test_indices = np.arange(start_test_set_index, num_periods)\n","    \n","\n","    def scale_data( self, feature_clip = .02, vol_clip = .8):\n","        \"\"\"\n","        feature_clip: float\n","            clipping value for the OHCL features\n","        vol_clip: float\n","            clipping value for Volume\n","\n","        clip OHCLV data and for each asset, a StardardScaler scales OHCL data and another StandardScaler scales Volumes data\n","        Standardized data is stored self.returns\n","        Non standardized data is in sefl.true_returns  \n","        \"\"\"\n","\n","        feature_returns = self.returns[:,:,:-1]\n","        volumes_returns = self.returns[:,:,-1:]\n","\n","        clipped_features = np.clip( feature_returns, - feature_clip , feature_clip )\n","        clipped_volumes = np.clip( volumes_returns, - vol_clip, vol_clip )\n","\n","\n","        features_std = clipped_features.copy()\n","        volumes_std = clipped_volumes.copy()\n","\n","        feature_scalers = {}\n","        volume_scalers = {}\n","\n","        #scale training data\n","        for i in range(features_std.shape[1]):\n","            feature_scalers[i] = StandardScaler()\n","            volume_scalers[i] = StandardScaler()\n","            features_std[self.train_val_indices, i, :] = feature_scalers[i].fit_transform(features_std[self.train_val_indices, i, :]) \n","            volumes_std[self.train_val_indices,i,:] = volume_scalers[i].fit_transform( volumes_std[self.train_val_indices,i,:])\n","\n","            #scale validation data\n","            #features_std[self.val_indices, i, :] = feature_scalers[i].transform(features_std[self.val_indices, i, :]) \n","            #volumes_std[self.val_indices,i,:] = volume_scalers[i].transform( volumes_std[self.val_indices,i,:])\n","\n","            #scale test data\n","            features_std[self.test_indices, i, :] = feature_scalers[i].transform(features_std[self.test_indices, i, :]) \n","            volumes_std[self.test_indices,i,:] = volume_scalers[i].transform( volumes_std[self.test_indices,i,:])\n","\n","        self.returns = np.concatenate([features_std, volumes_std], axis = -1)\n","\n","\n","    def load_sequence_indices( self, ):\n","        \"\"\"\n","        return sequence_indices_encoder, reward_returns_indices\n","        sequence_indices_encoder are used for selecting transformer input from self.returns or self.true_returns in the training phase\n","        reward_returns_indices are used for reward or target \n","        \"\"\"\n","        #questo metodo viene usato nella parte di RL, in modo da poter eventualmente modificare load_sequence in caso si voglia fare multi step forecasting o altre modifiche alla \n","        #regressione con il transformer\n","        indices = self.train_indices[ self.encoder_sequence_length : - self.episode_length ]\n","\n","        starting_index = np.random.choice( indices )\n","        sequence_indices = np.arange( starting_index, starting_index + self.episode_length )\n","        sequence_indices_encoder = []\n","        for i in range( self.episode_length ):\n","            sequence_indices_encoder.append( np.arange( sequence_indices[i] - self.encoder_sequence_length, sequence_indices[i] ) )\n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_encoder = 1 + np.array( sequence_indices_encoder )\n","\n","        reward_returns_indices = sequence_indices + 1\n","\n","        return sequence_indices_encoder, reward_returns_indices\n","\n","    def load_test_indices( self, ):\n","        \"\"\"\n","        return sequence_indices_encoder, reward_returns_indices\n","        sequence_indices_encoder are used for selecting transformer input from self.returns or self.true_returns in the training phase\n","        reward_returns_indices are used for reward or target \n","        \"\"\"\n","        #questo metodo viene usato nella parte di RL, in modo da poter eventualmente modificare load_sequence in caso si voglia fare multi step forecasting o altre modifiche alla \n","        #regressione con il transformer\n","        indices = self.test_indices[ self.encoder_sequence_length : ]\n","\n","        sequence_indices_transformer = []\n","        for index in indices:\n","            sequence_indices_transformer.append( np.arange( index - self.encoder_sequence_length, index ) )\n","            \n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","        sequence_indices_transformer = sequence_indices_transformer[:-1]\n","\n","        reward_returns_indices = indices + 1\n","        reward_returns_indices = reward_returns_indices[:-1]\n","\n","        return sequence_indices_transformer, reward_returns_indices\n","\n","    \n","    def load_sequence_and_targets( self, ):\n","        \"\"\"\n","        return transformer_input_sequence, regression_target_sequence\n","        sequence_indices_encoder sequence of standardized data to be used as transformer input\n","        regression_target_sequence are closing returns, target for regression\n","        \"\"\"\n","        indices = self.train_val_indices[ self.encoder_sequence_length : - self.episode_length ]\n","\n","        starting_index = np.random.choice( indices )\n","        sequence_indices = np.arange( starting_index, starting_index + self.episode_length )\n","        sequence_indices_transformer = []\n","        for i in range( self.episode_length ):\n","            sequence_indices_transformer.append( np.arange( sequence_indices[i] - self.encoder_sequence_length, sequence_indices[i] ) )\n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","\n","        target_returns_indices = sequence_indices + 1\n","\n","        transformer_input_sequence = self.returns[ sequence_indices_transformer ]\n","\n","        #since the prediction is passed as input for actor and critic, standardized returns are target\n","        regression_target_sequence = self.returns[ target_returns_indices, :, -2 ]\n","\n","        return transformer_input_sequence, regression_target_sequence\n","\n","    def load_test_sequence_and_targets( self, ):\n","        \"\"\"\n","        return transformer_input_sequence, regression_target_sequence\n","        sequence_indices_encoder sequence of standardized data to be used as transformer input\n","        regression_target_sequence are closing returns, target for regression\n","        \"\"\"\n","        indices = self.test_indices[ self.encoder_sequence_length : ]\n","\n","        sequence_indices_transformer = []\n","        for index in indices:\n","            sequence_indices_transformer.append( np.arange( index - self.encoder_sequence_length, index ) )\n","            \n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","        sequence_indices_transformer = sequence_indices_transformer[:-1]\n","\n","        target_returns_indices = indices + 1\n","        target_returns_indices = target_returns_indices[:-1]\n","\n","        transformer_input_sequence = self.returns[ sequence_indices_transformer ]\n","\n","        regression_target_sequence = self.returns[ target_returns_indices, :, -2 ]\n","\n","        return transformer_input_sequence, regression_target_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsQ9PbHSiyHZ"},"source":["# Environment"]},{"cell_type":"code","metadata":{"id":"arSlHXUs44qv"},"source":["class Sequence():\n","\n","    def __init__(self, num_assets, sequence_len, composition_difference_coef, risk_coef):\n","\n","        self.num_assets = num_assets\n","        self.sequence_len = sequence_len\n","        self.composition_difference_coef = composition_difference_coef\n","        self.risk_coef = risk_coef\n","\n","\n","    def encode( self, ):\n","        #next observation is concatenation of transformer output and portfolio composition\n","        next_obs = np.concatenate( [self.predicted_sequence[ self._idx ], self.portfolio_composition] )\n","        return next_obs\n","\n","    def reset_portfolio_composition( self, ):\n","        portfolio_composition = np.ones((self.num_assets,)) / (self.num_assets)\n","        return portfolio_composition\n","\n","\n","    def reset( self, predicted_sequence, closing_reward_returns ):\n","        #current index in the sequence\n","        self._idx = 0\n","\n","        #output of the transformer, i.e. prediction of closing returns \n","        self.predicted_sequence = predicted_sequence\n","        \n","        # true returns for the day i.e. input of transformer\n","        self.closing_reward_returns = closing_reward_returns\n","\n","        #current portfolio composition\n","        self.portfolio_composition = self.reset_portfolio_composition()\n","\n","        self.done = False\n","\n","        obs = self.encode()\n","        return obs\n","\n","    \n","    def step( self, action ):\n","\n","        #calculating reward\n","        reward = self.reward( action )\n","\n","        #updating the portfolio composition\n","        self._idx += 1\n","        next_portfolio_composition = to_numpy( F.softmax(from_numpy(action), dim = -1) )\n","        self.portfolio_composition = next_portfolio_composition\n","\n","        #checking if done\n","        self.done = self._idx + 1 == self.sequence_len \n","        #getting next_observation\n","        obs = self.encode()\n","        \n","        return obs, reward, self.done\n","\n","    def reward( self, action ):\n","\n","        next_portfolio_composition = to_numpy( F.softmax(from_numpy(action), dim = -1) )\n","\n","        returns = self.closing_reward_returns[self._idx]\n","        \n","        portfolio_elements_returns = (next_portfolio_composition * returns)\n","        portfolio_returns = portfolio_elements_returns.sum()\n","        pc_difference =  np.absolute(next_portfolio_composition - self.portfolio_composition)\n","        pc_difference_sum = pc_difference.sum()\n","        portfolio_std = portfolio_elements_returns.std()\n","\n","        reward = portfolio_returns - self.composition_difference_coef * pc_difference_sum - self.risk_coef * portfolio_std\n","\n","        #reward *= 100\n","\n","        return reward\n","\n","\n","\n","class CustomEnv( gym.Env ):\n","    #required for gym.Env compatibility\n","    metadata = {'render.modes': ['human']}\n","\n","    \n","    def __init__(self,):\n","        super(CustomEnv, self).__init__()\n","\n","        # internal value of parameters\n","        self.params = Parameters()\n","\n","        #parameters to be used in env\n","        self.num_assets = self.params.num_assets + 1\n","        self.prediction_shape = self.params.num_assets \n","\n","        # for memory reasons (vec env), dataset is passed as argument\n","        self.dataset = Dataset( self.params )\n","\n","        #for reward\n","        self._true_returns = self.dataset.true_returns\n","\n","        #for observations\n","        self._returns = self.dataset.returns\n","\n","        # closing prices\n","        self._prices = self.dataset.prices[:,:,-2]\n","\n","        #a sequence object that produces observations, compute rewards and keeps track of portfolio composition \n","        self._sequence = Sequence( sequence_len = self.params.episode_length, \n","                                   num_assets = self.num_assets, \n","                                   composition_difference_coef = self.params.composition_difference_coef, \n","                                   risk_coef = self.params.risk_coef) \n","\n","        self.regression_net = self.load_regression_net()\n","\n","\n","        self.action_space = spaces.Box( low = 0, \n","                                        high = 1., \n","                                        shape = (self.num_assets,), \n","                                        dtype = np.float32)\n","        \n","        self.observation_space = spaces.Box( low = -np.inf,\n","                                             high = np.inf,\n","                                             shape= (self.prediction_shape + self.num_assets,), \n","                                             dtype= np.float32)\n","        \n","    def load_regression_net( self, trained_regression_model_path = \"/content/drive/MyDrive/0_Codice tesi/RUN_DEF/pesi/regression_weights.pt\" ):\n","        regression_net = torch.load( trained_regression_model_path )\n","        return regression_net\n","\n","    def step( self, action ):\n","\n","        next_obs, reward, done = self._sequence.step( action )\n","        #info is a dictionary \n","        info = {}\n","        return next_obs, reward, done, info\n","    \n","\n","    def reset( self, ):\n","\n","        sequence_indices_transformer, reward_returns_indices = self.dataset.load_sequence_indices()\n","        transformer_in_sequence = from_numpy(self._returns[ sequence_indices_transformer ])\n","\n","        predicted_sequence = []\n","        for transformer_in in transformer_in_sequence:\n","            with torch.no_grad():\n","                pred = self.regression_net( transformer_in ).flatten() # after flatten dim: (num_assets,)\n","            predicted_sequence.append( to_numpy(pred) )\n","        predicted_sequence = np.array( predicted_sequence ) # dim: (sequence_len, num_assets,)\n","\n","        closing_reward_returns = np.concatenate( [np.zeros((self.params.episode_length,1) ), self._true_returns[ reward_returns_indices, :, -2 ]], axis = -1)\n","\n","        obs = self._sequence.reset( predicted_sequence, closing_reward_returns )\n","\n","        return obs\n","\n","    def render( self, ):\n","        pass\n","\n","    def close( self, ):\n","        pass\n","\n","    def test_reset( self, ):\n","        # a run on validation set\n","        sequence_indices_transformer, reward_returns_indices = self.dataset.load_test_indices()\n","        transformer_in_sequence = from_numpy(self._returns[ sequence_indices_transformer ])\n","\n","        predicted_sequence = []\n","        for transformer_in in transformer_in_sequence:\n","            with torch.no_grad():\n","                pred = self.regression_net( transformer_in ).flatten() # after flatten dim: (num_assets,)\n","            predicted_sequence.append( to_numpy(pred) )\n","        predicted_sequence = np.array( predicted_sequence ) # dim: (sequence_len, num_assets,)\n","\n","        #true sequence to confront with predicted in plot\n","        true_sequence = self._returns[ reward_returns_indices, :, -2 ]\n","\n","        #selecting the closing returns in validation set for reward\n","        closing_reward_returns = np.concatenate( [np.zeros((predicted_sequence.shape[0],1) ), self._true_returns[ reward_returns_indices, :, -2 ]], axis = -1) \n","\n","        #a validation sequence with different sequence length from the training one is created\n","        self._test_sequence = Sequence( sequence_len = predicted_sequence.shape[0], \n","                                       num_assets = self.num_assets, \n","                                       composition_difference_coef = self.params.composition_difference_coef, \n","                                       risk_coef = self.params.risk_coef) \n","        \n","        obs = self._test_sequence.reset( predicted_sequence, closing_reward_returns )\n","\n","        closing_prices = self._prices[ reward_returns_indices ]\n","\n","        return obs, closing_reward_returns, closing_prices, true_sequence, predicted_sequence\n","\n","    def test_step( self, action ):\n","        next_obs, reward, done = self._test_sequence.step( action )\n","        return next_obs, reward, done\n","\n","    def test( self, ):\n","        #a run on test set\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFDphPgi-RRl"},"source":["def make_env():\n","\n","    def child( ):\n","        env = CustomEnv()\n","        env = gym.wrappers.RecordEpisodeStatistics( env )\n","        return env\n","\n","    return child"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGUORHTFi2DD"},"source":["# Actor Critic"]},{"cell_type":"code","metadata":{"id":"M-SSbjp9FtDw"},"source":["def layer_init( layer, std = np.sqrt(2), bias_const = 0.0 ):\n","    \"\"\"\n","    PPO specific layer initialization\n","    \n","    parameters\n","    \n","    std: float or float-like\n","        default np.sqrt(2)\n","        in actor last layer set to 0.01\n","        in critic last layer set to 1.\n","        \n","    bias: float \n","        default 0\n","        do not change\n","    \"\"\"\n","    \n","    if isinstance( layer, nn.Linear ):\n","        torch.nn.init.orthogonal_( layer.weight, std )\n","        torch.nn.init.constant_( layer.bias, bias_const )\n","    return layer\n","\n","class Critic( nn.Module ):\n","    \"\"\"\n","    Critic architecture for Actor-Critic\n","    \n","    parameters\n","    input_shape: int\n","        the single observation shape of the vector environment \n","        can be obtained as np.array(vec_env.single_observation_space.shape).prod()\n","    \"\"\"\n","    \n","    def __init__( self, input_shape, n_layers, d_layers ):\n","        super( Critic, self ).__init__()\n","\n","        layers = nn.ModuleList([])\n","\n","        if n_layers > 1:\n","            layers.append( layer_init( nn.Linear( input_shape, d_layers ) ) ) #std is sqrt(2)\n","            layers.append( nn.Tanh(), )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( layer_init( nn.Linear( d_layers, d_layers ) ) )  #std is sqrt(2)\n","                layers.append( nn.Tanh() )\n","            layers.append( layer_init( nn.Linear( d_layers,1 ), std = 1.) ) #std is 1.\n","        else:\n","            layers.append( layer_init( nn.Linear( input_shape, 1 ), std = 1. ) ) #std is 1.\n","\n","        self.net = nn.Sequential( *layers )\n","        \n","    def forward( self, x):\n","        x = self.net( x )\n","        return x\n","\n","\n","\n","\n","class Actor( nn.Module ):\n","    \"\"\"\n","    Actor architecture for Actor-Critic\n","    \n","    parameters\n","    input_shape: int\n","        the single observation shape of the vector environment \n","        can be obtained as np.array(vec_env.single_observation_space.shape).prod()\n","        \n","    action_number: int\n","        the number of action in the single environment of the vector environment \n","        can be obtained as np.prod(vec_env.single_action_space.shape)\n","    \"\"\"\n","    \n","    def __init__( self, input_shape, action_number, n_layers, d_layers ):\n","        super( Actor, self ).__init__()\n","\n","        layers = nn.ModuleList([])\n","\n","        if n_layers > 1:\n","            layers.append( layer_init( nn.Linear( input_shape, d_layers ) ) ) #std is sqrt(2)\n","            layers.append( nn.Tanh(), )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( layer_init( nn.Linear( d_layers, d_layers ) ) )  #std is sqrt(2)\n","                layers.append( nn.Tanh() )\n","            layers.append( layer_init( nn.Linear( d_layers, action_number ), std = .01) ) #std is .01 \n","        else:\n","            layers.append( layer_init( nn.Linear( input_shape, action_number ), std = .01) ) #std is .01 \n","\n","        self.net = nn.Sequential( *layers )\n","        \n","    def forward( self, x):\n","        x = self.net( x )\n","        return x\n","\n","\n","\n","class Agent( nn.Module ):\n","    \n","    \"\"\"\n","    the PPO agent\n","    \n","    parameters\n","    vec_env: gym.vector.SyncVectorEnv\n","        the vectorized environment in use\n","    \"\"\"\n","    \n","    def __init__( self, vec_env, n_layers, d_layers ):\n","        super( Agent, self).__init__()\n","        self.critic = Critic( np.array(vec_env.single_observation_space.shape).prod(), \n","                              n_layers,\n","                              d_layers )\n","        self.actor = Actor( np.array(vec_env.single_observation_space.shape).prod(), \n","                            np.array(vec_env.single_action_space.shape).prod(), \n","                            n_layers,\n","                            d_layers )\n","        self.actor_logstd = nn.Parameter( torch.zeros(1, np.prod(vec_env.single_action_space.shape)) )\n","        \n","    #inference for critic\n","    def get_value( self, x ):\n","        # x is observation (number of envs, features in single vec observation)\n","        #returns tensor (num_envs, 1)\n","        return self.critic( x )\n","    \n","    def get_action_and_value( self, x, action = None ):\n","        \n","        #first i get un normalized action probabilities\n","        action_mean = self.actor( x )\n","        action_logstd = self.actor_logstd.expand_as( action_mean )\n","        action_std = torch.exp( action_logstd )\n","        \n","        probs = Normal( action_mean, action_std )\n","        \n","        #in the rollout phase, we sample actions\n","        if action is None:\n","            action = probs.sample()\n","           \n","        #log probabilities of the action\n","        logprobs = probs.log_prob( action ).sum(1)\n","        \n","        #entropy of the distribution\n","        entropy = probs.entropy().sum(1)\n","        \n","        value = self.critic( x )\n","        \n","        return action, logprobs, entropy, value"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Coy6urmoi5Ec"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"0gsdC1dF-vGZ"},"source":["class Parameters(metaclass=SingleInstanceMetaClass):\n","\n","    def __init__(self,):\n","\n","        self.file_path = \n","        self.etfs = ['XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLRE', 'XLU', 'XLV', 'XLY']\n","        self.num_assets = len(self.etfs) #number of assets\n","        self.moving_average = 10 # moving average smoothing to be applied to data\n","        self.val_portion = .1 #validation portion in dataset\n","        self.test_portion = .1 #test portion in dataset\n","\n","        self.feature_clip = 0.02\n","        self.vol_clip = 0.8\n","\n","        self.encoder_sequence_length = 60 # sequence length for encoder input\n","        self.decoder_sequence_length = 20 # sequence length for decoder input\n","        self.dim_transformer = 64  #transformer model dimension\n","\n","        self.num_features = 5 #OHCLV\n","\n","        #PPO\n","\n","        self.seed = 1 # seed for reproducibility\n","\n","        self.num_envs = 8 #number of parallel environments\n","        self.episode_length = 200 #number of steps in each environment per policy rollout\n","\n","        self.num_ac_layers = 2 # number of feedforward layers in actor and critic\n","        self.dim_ac_layers = 64 #dimension of of feedforward layers in actor and critic\n","\n","        self.composition_difference_coef = 0.0025 #(0.25%) #coefficient for reward calculation\n","        self.risk_coef = 10 #coefficient for reward calculation\n","\n","        self.total_timesteps = 2000000 #the envvironment steps\n","        self.num_steps = self.episode_length #steps in rollout\n","\n","        self.batch_size = self.num_envs * self.num_steps\n","        self.num_updates = self.total_timesteps // self.batch_size #number of updates\n","\n","        self.anneal_lr = True\n","        self.learning_rate = 1e-4\n","        self.beta1 = .9 # beta1 parameter in adam optimizer\n","        self.beta2 = .99 # beta2 parameter in adam optimizer\n","       \n","        self.adam_eps = 1e-05\n","\n","        self.gae = True #use GAE for advantage calculations\n","        self.gamma = .99\n","        self.gae_lambda = .95\n","\n","        self.num_minibatches = 16\n","        self.minibatch_size = self.batch_size // self.num_minibatches\n","\n","        self.num_update_epochs = 10 #number of times the policy is updated\n","\n","        self.adv_normalization = True # normalize advantages\n","\n","        self.clip_coef = .2 #log probabilities ratio clip value\n","        self.anneal_clip_coef = False #linear decrease of clip coef from init value to 0, but decreases performances\n","        self.clip_values = True # clip value function loss\n","\n","        self.ent_loss_coef = 1e-3\n","        self.v_loss_coef = 0.5\n","\n","        self.max_grad_norm = .5 #global maximum gradient clipping\n","\n","        self.target_kl = None # 0.015 default in openai spinning. Altrimenti, None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fTHorQ-i7NB"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"VY4sKYTXF1b0","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"1f9adfa5-2dd1-4adc-b760-f40b9a871fb1"},"source":["p = Parameters()\n","\n","run_name = f\"PortfolioPPO__{p.seed}__{int(time.time())}\"\n","\n","\n","\n","wandb.init( \n","    project = ,\n","    entity = ,\n","    sync_tensorboard=True,\n","    config = vars(p),\n","    name = run_name,\n","    monitor_gym = True,\n","    save_code = True\n",")\n","\n","writer = SummaryWriter()\n","writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(p).items()])),\n","    )\n","\n","\n","\n","#setting the seed\n","random.seed( p.seed )\n","np.random.seed( p.seed )\n","torch.manual_seed( p.seed )\n","torch.backends.cudnn.deterministic = True\n","\n","vec_env = gym.vector.SyncVectorEnv( [make_env() for i in range( p.num_envs ) ] )\n","\n","agent = Agent( vec_env, p.num_ac_layers, p.dim_ac_layers ).to( DEVICE )\n","optim = Adam( agent.parameters(), lr=p.learning_rate, betas=(p.beta1, p.beta2), eps=p.adam_eps)\n","\n","#Per i rollouts, non uso una classe ma questi tensori\n","#obs is ( number of steps, number of envs, features in single vec observation )\n","obs = torch.zeros((p.num_steps, p.num_envs) + vec_env.single_observation_space.shape).to( DEVICE )\n","\n","#actions is ( number of steps, number of envs, features in single vec action )\n","#features in single vec action is in this case 1\n","actions = torch.zeros((p.num_steps, p.num_envs) + vec_env.single_action_space.shape).to( DEVICE )\n","\n","#these are ( number of steps, number of envs )\n","logprobs = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","rewards = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","dones = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","values = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","\n","\n","# a global step counter\n","global_step = 0\n","\n","# for time tracking\n","#start_time = time.time()\n","\n","# to store initial observation\n","next_obs = from_numpy( vec_env.reset() )\n","\n","# to store initial termination conditions\n","next_done = torch.zeros( p.num_envs ).to( DEVICE )\n","\n","for update in range(1, p.num_updates + 1):\n","    \n","    #un update è una iterazione del training loop così composto\n","    #0.update lr\n","    #1.policy rollouts\n","    #2.process rollouts data\n","    #3.policy training\n","    \n","    #0.update lr\n","    if p.anneal_lr:\n","        # decrescita lineare dal valore iniziale fino a 0\n","        # decremento ogni training loop\n","        frac = 1. - (update - 1)/p.num_updates\n","        learning_rate_now = frac * p.learning_rate\n","        optim.param_groups[0]['lr'] = learning_rate_now\n","        \n","    if p.anneal_clip_coef:\n","        frac = 1. - (update - 1)/p.num_updates\n","        clip_coef_now = frac * p.clip_coef\n","        p.clip_coef = clip_coef_now\n","        \n","        \n","    #1.policy_rollouts\n","    for step in range(0, p.num_steps ):\n","        global_step += 1 * p.num_envs\n","        \n","        obs[step] = next_obs\n","        dones[step] = next_done\n","        \n","        with torch.no_grad():\n","            action, logprob, _, value = agent.get_action_and_value( next_obs )\n","            values[step] = value.flatten()\n","\n","        actions[step] = action\n","        logprobs[step] = logprob \n","        \n","        #step dell'environment\n","        next_obs, reward, done, info = vec_env.step( to_numpy(action ) )\n","\n","        rewards[step] = from_numpy( reward )\n","        \n","        next_obs = from_numpy( next_obs )\n","        next_done = from_numpy( done )\n","        \n","        #a print\n","        for item in info:\n","            if \"episode\" in item.keys():\n","                print(f\"global step:{global_step}, episode return:{item['episode']['r']} \")\n","                writer.add_scalar(\"Charts/episode_return\", item[\"episode\"][\"r\"], global_step ) \n","            break\n","                \n","    #2.process data\n","    \n","    #calcolo \n","    #viene fatto bootstrap if not done: 18:08 nel video\n","    with torch.no_grad():\n","        next_value = agent.get_value(next_obs) #(num_envs, 1)\n","        next_value = next_value.reshape(1, -1) #(1,num_envs)\n","        if p.gae: #da implementazione originale\n","            advantages = torch.zeros_like(rewards).to( DEVICE )\n","            lastgaelam = 0\n","            for t in reversed(range(p.num_steps)):\n","                if t == p.num_steps - 1:\n","                    nextnonterminal = 1.0 - next_done\n","                    nextvalues = next_value\n","                else:\n","                    nextnonterminal = 1.0 - dones[t + 1]\n","                    nextvalues = values[t + 1]\n","                delta = rewards[t] + p.gamma * nextvalues * nextnonterminal - values[t]\n","                advantages[t] = lastgaelam = delta + p.gamma * p.gae_lambda * nextnonterminal * lastgaelam\n","            returns = advantages + values\n","        else: #modo tipico di calcolare advantages\n","            returns = torch.zeros_like(rewards).to( DEVICE )\n","            for t in reversed(range(p.num_steps)):\n","                if t == p.num_steps - 1:\n","                    nextnonterminal = 1.0 - next_done\n","                    next_return = next_value\n","                else:\n","                    nextnonterminal = 1.0 - dones[t + 1]\n","                    next_return = returns[t + 1]\n","                returns[t] = rewards[t] + p.gamma * nextnonterminal * next_return\n","            advantages = returns - values\n","        # i returns sono differenti nei due modi (lo posso vedere stampando returns.sum())\n","    \n","    #per usare minibatches, appiattisco i dati calcolati in modo da avere la batch per esteso\n","    # in generale (num_steps, num_envs, single environment shape) -> (batch_size, single environment shape)\n","\n","    b_obs = obs.reshape((-1,) + vec_env.single_observation_space.shape )\n","    b_actions = actions.reshape((-1,) + vec_env.single_action_space.shape )\n","    b_logprobs = logprobs.reshape(-1)\n","    b_advantages = advantages.reshape(-1)\n","    b_returns = returns.reshape(-1)\n","    b_values = values.reshape(-1)\n","    \n","    \n","    #3.policy training\n","    b_indices = np.arange(p.batch_size)\n","    #clipped_fractions = [] # tengo conto di quanto spesso il ratio viene clippato\n","    for epoch in range( p.num_update_epochs ):\n","        np.random.shuffle( b_indices )\n","        \n","        #ora itero sulla batch una minibatch alla volta\n","        for start in range(0, p.batch_size, p.minibatch_size):\n","            end = start + p.minibatch_size\n","            mb_indices = b_indices[ start : end ]\n","            \n","            #faccio forward pass sulle osservazioni della minibatch\n","            _ , new_logprob, entropy, new_values = agent.get_action_and_value( x = b_obs[ mb_indices ],\n","                                                                               action = b_actions[ mb_indices ] )\n","            \n","            log_ratio = new_logprob - b_logprobs[ mb_indices ]\n","            ratio = log_ratio.exp()\n","            \n","            \n","            #debug variables\n","            with torch.no_grad():\n","                approx_kl = ((ratio - 1) - log_ratio).mean()\n","\n","            \n","            #advantages normalization\n","            mb_advantages = b_advantages[ mb_indices ]\n","            if p.adv_normalization:\n","                mb_advantages = (mb_advantages - mb_advantages.mean())/( mb_advantages.std() + 1e-8)\n","            \n","            #policy loss\n","            surr_loss1 = - mb_advantages * ratio\n","            surr_loss2 = - mb_advantages * torch.clamp( ratio, 1 - p.clip_coef, 1 + p.clip_coef )\n","            # prendo il max siccome ho considero \"- advantages \"\n","            policy_loss = torch.max(surr_loss1, surr_loss2).mean()\n","            \n","            #value loss\n","            \n","            new_values = new_values.view(-1) # ( minibatch_size, 1) -> (minibatch_size)\n","            \n","            if p.clip_values:\n","                value_loss_unclipped = (new_values - b_returns[ mb_indices ]) ** 2\n","                #values clipped are minibatch values + or - the clipped difference \n","                #between minibatch new values and minibatch values itself  \n","                values_clipped = b_values[ mb_indices ] + torch.clamp(\n","                        new_values - b_values[ mb_indices ],\n","                        -p.clip_coef,\n","                        p.clip_coef,\n","                    )\n","                value_loss_clipped = (values_clipped - b_returns[ mb_indices ]) ** 2\n","                value_loss = torch.max(value_loss_unclipped, value_loss_clipped)\n","                value_loss = 0.5 * value_loss.mean()\n","            else: #value loss è MSE\n","                value_loss = .5 * ((new_values - b_returns[ mb_indices ] )**2).mean()\n","\n","            #entropy loss\n","            entropy_loss = entropy.mean()\n","\n","            # minus in entropy loss to increase exploration\n","            loss = policy_loss - p.ent_loss_coef * entropy_loss + p.v_loss_coef * value_loss\n","\n","            optim.zero_grad()\n","            loss.backward()\n","            nn.utils.clip_grad_norm_( agent.parameters(), p.max_grad_norm )\n","            optim.step()\n","        \n","        #kl stop at batch level. lo potrei usare anche a minibatch level\n","        if p.target_kl is not None:\n","            if approx_kl > p.target_kl:\n","                break\n","    \n","\n","\n","    writer.add_scalar(\"Charts/learning_rate\", optim.param_groups[0][\"lr\"], global_step)\n","    writer.add_scalar(\"Charts/clip_coefficient\", p.clip_coef, global_step)\n","    writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n","    writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n","    writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","    writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","\n","    #OK FIN QUA\n","    if update % 10 == 0:\n","        # validation \n","        single_env = vec_env.envs[0]\n","\n","        #data for plots\n","        portfolio_compositions_val = []\n","        actions_val = []\n","        values_val = []\n","        rewards_val = []\n","\n","        #closing returns is np.ndarray with closing returns for portfolio value\n","        next_obs_val, closing_returns_val, closing_prices, true_sequence, predicted_sequence = single_env.test_reset() \n","        next_obs_val =  from_numpy( next_obs_val ) \n","\n","        #register portfolio composition\n","        portfolio_compositions_val.append( to_numpy(next_obs_val[-single_env.num_assets:]) )\n","        actions_val.append( to_numpy(next_obs_val[-single_env.num_assets:]) )\n","        values_val.append(0)\n","        rewards_val.append(0)\n","\n","        done_val = False\n","\n","        while not done_val:\n","            \n","            with torch.no_grad():\n","                action_val, value_val = agent.actor( next_obs_val ), agent.critic( next_obs_val )\n","\n","            next_obs_val, reward_val, done_val = single_env.test_step( to_numpy( action_val ) )\n","            next_obs_val = from_numpy( next_obs_val )\n","\n","            #register portfolio composition, value, reward\n","            next_portfolio_composition_val = F.softmax(action_val, dim = -1)\n","            portfolio_compositions_val.append( to_numpy(next_portfolio_composition_val) )\n","            actions_val.append( to_numpy(action_val) )\n","            values_val.append( value_val.item() )\n","            rewards_val.append( reward_val.item() )\n","\n","\n","        portfolio_compositions_val = np.array( portfolio_compositions_val )\n","        actions_val = np.array( actions_val )\n","        values_val = np.array( values_val )\n","        rewards_val = np.array( rewards_val )\n","\n","        # as in https://github.com/openai/gym/blob/master/gym/wrappers/record_episode_statistics.py#L29\n","        writer.add_scalar(\"Charts/episode_return_val\", rewards_val.sum(), global_step)\n","\n","        \n","        #compute portfolio value\n","        element_returns_val = portfolio_compositions_val * closing_returns_val\n","        portfolio_returns_val = np.sum( element_returns_val, axis = -1 )\n","        #commission costs\n","        difference_portfolio_composition_val = np.absolute( portfolio_compositions_val[:-1,:] - portfolio_compositions_val[1:,:] )\n","        difference_portfolio_composition_val = np.concatenate( [np.zeros_like( difference_portfolio_composition_val[0:1,:]), difference_portfolio_composition_val ], axis = 0)\n","        difference_portfolio_composition_val = difference_portfolio_composition_val.sum( axis = 1 )\n","        difference_portfolio_composition_val.shape\n","        #returns and value\n","        portfolio_returns_val = portfolio_returns_val - p.composition_difference_coef * difference_portfolio_composition_val\n","        portfolio_values_val = np.cumprod( 1 + portfolio_returns_val )\n","\n","        #equi portfolio as baseline\n","        equi_portfolio_compositions_val = np.array( [portfolio_compositions_val[0] ]*portfolio_compositions_val.shape[0])\n","        equi_element_returns_val = equi_portfolio_compositions_val * closing_returns_val\n","        equi_portfolio_returns_val = np.sum( equi_element_returns_val, axis = -1 )\n","        equi_portfolio_values_val = np.cumprod( 1 + equi_portfolio_returns_val )\n","\n","        #plots\n","\n","        #agent portfolio value compared to equi portfolio value\n","        fig = figure( figsize = (13,8))\n","        title(\"Agent and equally-weighted portfolio value comparison on test set\")\n","        plot(portfolio_values_val, label = \"agent pv\")\n","        plot(equi_portfolio_values_val, label = \"e-v pv\")\n","        legend()\n","        xlabel(\"Day from start of set (#)\")\n","        ylabel(\"Portfolio value (#)\")\n","        writer.add_figure(\"media/agent_equi_portfolio_value_comparison\", fig, global_step)\n","        savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/pv/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/pv/{str(global_step)}.png\") \n","        close('all')\n","\n","        # agent portfolio composition\n","        fig, ax = subplots( 1,1, figsize = (13,8) )\n","        title(\"Agent portfolio composition\")\n","        ax = gca()\n","        ax.stackplot( np.arange( portfolio_compositions_val.shape[0] ), portfolio_compositions_val.transpose((1,0)), labels = [\"cash\"]+p.etfs, )\n","        legend()\n","        ylabel(\"Composition (%)\")\n","        xlabel(\"Day from start of test set (#)\")\n","        writer.add_figure(\"media/agent_portfolio_composition\", fig, global_step)\n","        savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/compositions/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/compositions/{str(global_step)}.png\")\n","        close('all')\n","\n","        # returns and allocation \n","        fig, axs = subplots( 1 + p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Returns and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets + 1):\n","            if i == 0:\n","                axs[i].set_title(\"CASH\")\n","            else:\n","                axs[i].set_title(f\"{p.etfs[i-1]}\")\n","            axs[i].plot( closing_returns_val[:,i], label = \"closing returns\")\n","            axs[i].hlines( 0, xmin = 0, xmax = closing_returns_val[:,i].shape[0], linestyles= \"dashed\", alpha = .3)\n","            axs[i].set_ylabel(\"returns (%)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i], color = \"tab:orange\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            if i!=11:\n","                axs[i].set_xticklabels([])\n","            if i==11:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/returns_and_allocation\", fig, global_step)\n","        savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/returns_allocations/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/returns_allocations/{str(global_step)}.png\")\n","        close('all')\n","\n","\n","        fig, axs = subplots( p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Returns, predictions and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets):\n","            axs[i].set_title(f\"{p.etfs[i]}\")\n","            axs[i].plot( true_sequence[:,i], label = \"True returns\" )\n","            axs[i].plot( predicted_sequence[:,i], label = \"Predicted returns\" )\n","            axs[i].set_ylabel(\"returns (%)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i+1], color = \"tab:green\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            if i!=p.num_assets - 1:\n","                axs[i].set_xticklabels([])\n","            if i==p.num_assets:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/prediction_allocations\", fig, global_step)\n","        savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/tpa/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/tpa/{str(global_step)}.png\")\n","        close('all')\n","\n","\n","        \n","        fig, axs = subplots( p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Closing prices and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets):\n","            axs[i].set_title(f\"{p.etfs[i]}\")\n","            axs[i].plot( closing_prices[:,i], label = \"Closing prices\" )\n","            axs[i].set_ylabel(\"closing prices ($)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i+1], color = \"tab:orange\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            if i!=p.num_assets - 1:\n","                axs[i].set_xticklabels([])\n","            if i==p.num_assets:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/prices_allocation\", fig, global_step)\n","        savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/prices_allocation/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/tpa/{str(global_step)}.png\")\n","        close('all')\n","\n","\n","vec_env.close()\n","writer.close()"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m4g0\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.14"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20220410_173451-ahav1mnb</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/4g0/DEF/runs/ahav1mnb\" target=\"_blank\">PortfolioPPO__1__1649612085</a></strong> to <a href=\"https://wandb.ai/4g0/DEF\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"output_type":"stream","name":"stdout","text":["global step:1592, episode return:-1.4082950763734194 \n","global step:3184, episode return:-0.7750684920780989 \n","global step:4776, episode return:-1.2210657499181805 \n","global step:6368, episode return:-0.838948065143584 \n","global step:7960, episode return:-0.8453089053170196 \n","global step:9552, episode return:-1.3198314807670146 \n","global step:11144, episode return:-1.2285062974994578 \n","global step:12736, episode return:-1.0525197999532858 \n","global step:14328, episode return:-1.4182040805680816 \n","global step:15920, episode return:-1.1734441819499577 \n","global step:17512, episode return:-0.8725665482166163 \n","global step:19104, episode return:-1.220817486856072 \n","global step:20696, episode return:-1.1186472958309772 \n","global step:22288, episode return:-1.343237228260347 \n","global step:23880, episode return:-1.0602886908787987 \n","global step:25472, episode return:-2.335046041537459 \n","global step:27064, episode return:-0.8756542120988864 \n","global step:28656, episode return:-0.9362761169320385 \n","global step:30248, episode return:-0.7987788126631213 \n","global step:31840, episode return:-1.5863712162681234 \n","global step:33432, episode return:-0.8967032266153409 \n","global step:35024, episode return:-0.867928573502785 \n","global step:36616, episode return:-0.8682101790250832 \n","global step:38208, episode return:-0.8867340981607092 \n","global step:39800, episode return:-0.9158053699342026 \n","global step:41392, episode return:-1.0523524606614272 \n","global step:42984, episode return:-1.2979159329956929 \n","global step:44576, episode return:-1.2503398415791727 \n","global step:46168, episode return:-0.7967092404373418 \n","global step:47760, episode return:-0.7593807224488838 \n","global step:49352, episode return:-1.1858941441130626 \n","global step:50944, episode return:-1.5449327729511226 \n","global step:52536, episode return:-0.820615922797034 \n","global step:54128, episode return:-0.7684264542045758 \n","global step:55720, episode return:-1.4581175115575602 \n","global step:57312, episode return:-0.7289513159096824 \n","global step:58904, episode return:-0.885221692208489 \n","global step:60496, episode return:-0.7880429056531665 \n","global step:62088, episode return:-0.7266957680425945 \n","global step:63680, episode return:-1.0726214161985352 \n","global step:65272, episode return:-0.7596549721123603 \n","global step:66864, episode return:-0.9642423726466748 \n","global step:68456, episode return:-0.7607223092521621 \n","global step:70048, episode return:-1.3718436789121424 \n","global step:71640, episode return:-0.8143458097889413 \n","global step:73232, episode return:-1.3307059623649944 \n","global step:74824, episode return:-1.3851235467018448 \n","global step:76416, episode return:-1.0833331077652038 \n","global step:78008, episode return:-1.4536073436561523 \n","global step:79600, episode return:-0.9769657389207878 \n","global step:81192, episode return:-0.9546152937651986 \n","global step:82784, episode return:-1.2653161384051486 \n","global step:84376, episode return:-0.7694620349031125 \n","global step:85968, episode return:-0.8443922595008614 \n","global step:87560, episode return:-0.8251563741318247 \n","global step:89152, episode return:-1.1227751940015323 \n","global step:90744, episode return:-0.7127565567937644 \n","global step:92336, episode return:-0.8401370342419746 \n","global step:93928, episode return:-0.8459449047775761 \n","global step:95520, episode return:-0.820016679277537 \n","global step:97112, episode return:-1.1554553369610912 \n","global step:98704, episode return:-0.7694739484218343 \n","global step:100296, episode return:-1.7574462093698078 \n","global step:101888, episode return:-0.7619702440358596 \n","global step:103480, episode return:-1.458559199144936 \n","global step:105072, episode return:-1.0205483794373695 \n","global step:106664, episode return:-1.1529470583279149 \n","global step:108256, episode return:-0.8115311165804276 \n","global step:109848, episode return:-2.224780303714981 \n","global step:111440, episode return:-1.0933273604009055 \n","global step:113032, episode return:-0.9813663355443814 \n","global step:114624, episode return:-1.1158570152518155 \n","global step:116216, episode return:-1.1082363761352776 \n","global step:117808, episode return:-0.8959533246616724 \n","global step:119400, episode return:-0.8601860212144504 \n","global step:120992, episode return:-1.2806740961667082 \n","global step:122584, episode return:-0.7561007426485884 \n","global step:124176, episode return:-0.6960240036342696 \n","global step:125768, episode return:-1.9874042458475005 \n","global step:127360, episode return:-0.8171860315094271 \n","global step:128952, episode return:-1.1461772356081268 \n","global step:130544, episode return:-0.9863652561636364 \n","global step:132136, episode return:-0.8380682885828692 \n","global step:133728, episode return:-0.8227124597267595 \n","global step:135320, episode return:-1.2780400559024998 \n","global step:136912, episode return:-0.8004881360025855 \n","global step:138504, episode return:-0.9483390395119458 \n","global step:140096, episode return:-1.137738595221104 \n","global step:141688, episode return:-1.1920961720821739 \n","global step:143280, episode return:-0.7948412548295261 \n","global step:144872, episode return:-1.0618212334509545 \n","global step:146464, episode return:-1.735086627380596 \n","global step:148056, episode return:-0.8197436352546049 \n","global step:149648, episode return:-0.9785348268022905 \n","global step:151240, episode return:-1.18865422984577 \n","global step:152832, episode return:-1.15770642277418 \n","global step:154424, episode return:-0.835157182815641 \n","global step:156016, episode return:-0.6787200771076597 \n","global step:157608, episode return:-0.8459824961727804 \n","global step:159200, episode return:-0.7658726429886699 \n","global step:160792, episode return:-0.8856090841777713 \n","global step:162384, episode return:-1.3516952242961948 \n","global step:163976, episode return:-0.8688222013047668 \n","global step:165568, episode return:-0.7558813665646935 \n","global step:167160, episode return:-0.8296217645102562 \n","global step:168752, episode return:-0.8441654691094 \n","global step:170344, episode return:-1.0517795863408257 \n","global step:171936, episode return:-0.9251123349939308 \n","global step:173528, episode return:-1.0086699151680527 \n","global step:175120, episode return:-0.7709144682665977 \n","global step:176712, episode return:-0.7553160554287145 \n","global step:178304, episode return:-0.8735083721574699 \n","global step:179896, episode return:-0.737796016945204 \n","global step:181488, episode return:-0.8646673284701654 \n","global step:183080, episode return:-1.5226890981573054 \n","global step:184672, episode return:-0.8318911178237051 \n","global step:186264, episode return:-1.2551179783639081 \n","global step:187856, episode return:-1.2960113930172927 \n","global step:189448, episode return:-0.949642131309179 \n","global step:191040, episode return:-1.04623359967946 \n","global step:192632, episode return:-0.7941820730897008 \n","global step:194224, episode return:-1.500938595047588 \n","global step:195816, episode return:-0.9250949676389634 \n","global step:197408, episode return:-0.8252599014952091 \n","global step:199000, episode return:-0.9547921027017835 \n","global step:200592, episode return:-0.8822687181301248 \n","global step:202184, episode return:-1.3137380215819132 \n","global step:203776, episode return:-1.2344070102443487 \n","global step:205368, episode return:-0.8369921730876336 \n","global step:206960, episode return:-0.7128003777512621 \n","global step:208552, episode return:-2.0738666524865716 \n","global step:210144, episode return:-0.853022509688228 \n","global step:211736, episode return:-0.8837080459612051 \n","global step:213328, episode return:-1.4654002831100013 \n","global step:214920, episode return:-1.1477716446306192 \n","global step:216512, episode return:-0.6781406412246037 \n","global step:218104, episode return:-0.8288545217014875 \n","global step:219696, episode return:-1.0607385333775148 \n","global step:221288, episode return:-0.7217567816480416 \n","global step:222880, episode return:-0.8017687348112159 \n","global step:224472, episode return:-0.7133919319550727 \n","global step:226064, episode return:-1.0883643441019786 \n","global step:227656, episode return:-0.6394183823926607 \n","global step:229248, episode return:-1.0272882473295553 \n","global step:230840, episode return:-0.8220551703893567 \n","global step:232432, episode return:-1.2763557622352084 \n","global step:234024, episode return:-1.2699808420476022 \n","global step:235616, episode return:-0.6901669109446326 \n","global step:237208, episode return:-1.3071050027426157 \n","global step:238800, episode return:-0.8151781521153961 \n","global step:240392, episode return:-0.8010265778272923 \n","global step:241984, episode return:-0.8099255408323804 \n","global step:243576, episode return:-0.9186763990702754 \n","global step:245168, episode return:-0.7584245393288498 \n","global step:246760, episode return:-1.8965145907158087 \n","global step:248352, episode return:-1.4778118194620338 \n","global step:249944, episode return:-0.7899609644077217 \n","global step:251536, episode return:-0.7960278927284312 \n","global step:253128, episode return:-0.8042364602833179 \n","global step:254720, episode return:-1.1234158156329166 \n","global step:256312, episode return:-0.7227292127866866 \n","global step:257904, episode return:-0.7540341508117087 \n","global step:259496, episode return:-0.8846269110597006 \n","global step:261088, episode return:-0.611886526047704 \n","global step:262680, episode return:-1.4640771950657738 \n","global step:264272, episode return:-0.7820258354735725 \n","global step:265864, episode return:-0.8988343075686624 \n","global step:267456, episode return:-1.1547928063882034 \n","global step:269048, episode return:-1.3060604987040616 \n","global step:270640, episode return:-0.7762380198817915 \n","global step:272232, episode return:-0.9470229833893425 \n","global step:273824, episode return:-0.7757486098997529 \n","global step:275416, episode return:-1.2711027167555744 \n","global step:277008, episode return:-0.7761304185458503 \n","global step:278600, episode return:-1.1995218312911482 \n","global step:280192, episode return:-0.8168029167852985 \n","global step:281784, episode return:-1.1624986593898843 \n","global step:283376, episode return:-0.9665373688856529 \n","global step:284968, episode return:-0.7653749496418757 \n","global step:286560, episode return:-0.9163336745543704 \n","global step:288152, episode return:-1.0027079593388113 \n","global step:289744, episode return:-0.9307215243411893 \n","global step:291336, episode return:-0.7646920033884371 \n","global step:292928, episode return:-0.8385702962299775 \n","global step:294520, episode return:-0.8621680049577796 \n","global step:296112, episode return:-0.9174279482359734 \n","global step:297704, episode return:-1.142989839532823 \n","global step:299296, episode return:-0.8808577168219115 \n","global step:300888, episode return:-0.7097478584394392 \n","global step:302480, episode return:-0.8903279519846334 \n","global step:304072, episode return:-0.9564582382843486 \n","global step:305664, episode return:-0.8255485116984832 \n","global step:307256, episode return:-1.319411447052865 \n","global step:308848, episode return:-0.9450567371612522 \n","global step:310440, episode return:-0.7299328938414996 \n","global step:312032, episode return:-0.8226778284329042 \n","global step:313624, episode return:-0.7928348908809344 \n","global step:315216, episode return:-1.4791112799160304 \n","global step:316808, episode return:-0.7823615765267297 \n","global step:318400, episode return:-1.287753024988939 \n","global step:319992, episode return:-0.7851059691588447 \n","global step:321584, episode return:-0.8480914106268371 \n","global step:323176, episode return:-0.9930693634605681 \n","global step:324768, episode return:-1.2960359586080898 \n","global step:326360, episode return:-0.7683171949117477 \n","global step:327952, episode return:-0.9529052313657354 \n","global step:329544, episode return:-0.6756133241753332 \n","global step:331136, episode return:-0.7881561345035721 \n","global step:332728, episode return:-0.8422393993850623 \n","global step:334320, episode return:-0.8180889021061115 \n","global step:335912, episode return:-0.7298788550258135 \n","global step:337504, episode return:-0.7907334472553964 \n","global step:339096, episode return:-0.7101183444255971 \n","global step:340688, episode return:-0.8733397484076388 \n","global step:342280, episode return:-1.2353138810388649 \n","global step:343872, episode return:-0.8132464759619894 \n","global step:345464, episode return:-0.8052474283715539 \n","global step:347056, episode return:-0.9477661641724224 \n","global step:348648, episode return:-1.243338187370554 \n","global step:350240, episode return:-0.7353181139255448 \n","global step:351832, episode return:-0.8258636332646178 \n","global step:353424, episode return:-0.9704696799862196 \n","global step:355016, episode return:-0.7754404796104727 \n","global step:356608, episode return:-0.6743701917878802 \n","global step:358200, episode return:-0.7295509313955212 \n","global step:359792, episode return:-0.9747090725703298 \n","global step:361384, episode return:-1.1065954293450653 \n","global step:362976, episode return:-1.0353480091261988 \n","global step:364568, episode return:-0.7470865650283253 \n","global step:366160, episode return:-1.236807787794825 \n","global step:367752, episode return:-0.7894576949235396 \n","global step:369344, episode return:-0.7542072753508052 \n","global step:370936, episode return:-2.0032334701655032 \n","global step:372528, episode return:-0.622616177257985 \n","global step:374120, episode return:-0.7039356891533268 \n","global step:375712, episode return:-1.1475326041568759 \n","global step:377304, episode return:-0.6840325079289203 \n","global step:378896, episode return:-0.7597751996593015 \n","global step:380488, episode return:-0.7369352107097359 \n","global step:382080, episode return:-0.7501784506169199 \n","global step:383672, episode return:-0.7653670945404559 \n","global step:385264, episode return:-1.0574735586622581 \n","global step:386856, episode return:-0.6884806879997605 \n","global step:388448, episode return:-1.1969765173216897 \n","global step:390040, episode return:-0.7447222706217973 \n","global step:391632, episode return:-0.6539313134140659 \n","global step:393224, episode return:-1.1480312583602559 \n","global step:394816, episode return:-1.272961646159958 \n","global step:396408, episode return:-0.7658826827933136 \n","global step:398000, episode return:-0.855089396539381 \n","global step:399592, episode return:-1.24438953420479 \n","global step:401184, episode return:-0.703036103860306 \n","global step:402776, episode return:-0.8740224657064957 \n","global step:404368, episode return:-0.8191186279329227 \n","global step:405960, episode return:-0.7206120556088469 \n","global step:407552, episode return:-0.6466870031753238 \n","global step:409144, episode return:-1.1176054610329875 \n","global step:410736, episode return:-0.659042463128604 \n","global step:412328, episode return:-0.7249397380798547 \n","global step:413920, episode return:-0.807286616045562 \n","global step:415512, episode return:-0.9132672479709255 \n","global step:417104, episode return:-0.775975685642748 \n","global step:418696, episode return:-0.85542634093667 \n","global step:420288, episode return:-0.7484322434408256 \n","global step:421880, episode return:-1.2922213654887942 \n","global step:423472, episode return:-1.2706752978985274 \n","global step:425064, episode return:-0.6589412172182766 \n","global step:426656, episode return:-0.8204776476594604 \n","global step:428248, episode return:-0.7350132923767065 \n","global step:429840, episode return:-0.7485615626950777 \n","global step:431432, episode return:-0.8477161629189165 \n","global step:433024, episode return:-0.8026481007766619 \n","global step:434616, episode return:-0.7511179110817712 \n","global step:436208, episode return:-0.7759434076176424 \n","global step:437800, episode return:-0.7392911716785905 \n","global step:439392, episode return:-0.7317948812867077 \n","global step:440984, episode return:-0.800169915313769 \n","global step:442576, episode return:-0.686953861581191 \n","global step:444168, episode return:-1.0045713399667549 \n","global step:445760, episode return:-0.7137188831720142 \n","global step:447352, episode return:-1.1340535945337442 \n","global step:448944, episode return:-0.7218244873989108 \n","global step:450536, episode return:-0.8372654591116637 \n","global step:452128, episode return:-0.6988381224813569 \n","global step:453720, episode return:-1.1295619423546457 \n","global step:455312, episode return:-1.9602265264075722 \n","global step:456904, episode return:-1.1538265634649656 \n","global step:458496, episode return:-0.7922816170256718 \n","global step:460088, episode return:-0.6646055623683448 \n","global step:461680, episode return:-0.8596477848134315 \n","global step:463272, episode return:-0.8478847531110038 \n","global step:464864, episode return:-0.8222143268627824 \n","global step:466456, episode return:-1.1502563921971956 \n","global step:468048, episode return:-0.7305006338871443 \n","global step:469640, episode return:-1.9732952920493874 \n","global step:471232, episode return:-1.266001110107593 \n","global step:472824, episode return:-0.7301194609329058 \n","global step:474416, episode return:-1.0100896620215856 \n","global step:476008, episode return:-0.6494869494399039 \n","global step:477600, episode return:-0.9920617969041702 \n","global step:479192, episode return:-0.7591764199031744 \n","global step:480784, episode return:-1.1650952354499502 \n","global step:482376, episode return:-0.6990446609909676 \n","global step:483968, episode return:-0.6902930174712475 \n","global step:485560, episode return:-0.7815766864691795 \n","global step:487152, episode return:-1.1092959087921395 \n","global step:488744, episode return:-1.188226799680272 \n","global step:490336, episode return:-0.7370888525580822 \n","global step:491928, episode return:-1.0608494572061193 \n","global step:493520, episode return:-0.8327629866088709 \n","global step:495112, episode return:-0.7395715220114956 \n","global step:496704, episode return:-0.645171341632303 \n","global step:498296, episode return:-1.065995397455892 \n","global step:499888, episode return:-0.9925737649625547 \n","global step:501480, episode return:-0.8216158147571864 \n","global step:503072, episode return:-0.7694048903329875 \n","global step:504664, episode return:-1.0594735032907903 \n","global step:506256, episode return:-0.7140346184728339 \n","global step:507848, episode return:-0.7583313856735316 \n","global step:509440, episode return:-1.0814839621660526 \n","global step:511032, episode return:-0.7500491211287686 \n","global step:512624, episode return:-0.8315062930434595 \n","global step:514216, episode return:-1.0404103406284697 \n","global step:515808, episode return:-0.8637633893723624 \n","global step:517400, episode return:-0.8836824546087197 \n","global step:518992, episode return:-1.0695689580485492 \n","global step:520584, episode return:-1.4782094980839828 \n","global step:522176, episode return:-0.8776449597362014 \n","global step:523768, episode return:-0.7060797278884412 \n","global step:525360, episode return:-0.8499454734760702 \n","global step:526952, episode return:-1.2990008379537008 \n","global step:528544, episode return:-0.6840285175953801 \n","global step:530136, episode return:-0.7160991497958585 \n","global step:531728, episode return:-0.861806575022826 \n","global step:533320, episode return:-0.6810325691981642 \n","global step:534912, episode return:-1.3934587010775201 \n","global step:536504, episode return:-0.9428272791530857 \n","global step:538096, episode return:-1.1258455494384196 \n","global step:539688, episode return:-0.7066943761598963 \n","global step:541280, episode return:-0.8095165610223742 \n","global step:542872, episode return:-1.0015978455875125 \n","global step:544464, episode return:-0.8017742222367229 \n","global step:546056, episode return:-0.756993258486029 \n","global step:547648, episode return:-0.7432893094753195 \n","global step:549240, episode return:-0.7019141266083335 \n","global step:550832, episode return:-0.7415045304770147 \n","global step:552424, episode return:-0.6238260960747299 \n","global step:554016, episode return:-0.7209692793937832 \n","global step:555608, episode return:-0.6569203936956307 \n","global step:557200, episode return:-1.6185075351352256 \n","global step:558792, episode return:-0.8130548145409462 \n","global step:560384, episode return:-0.9654572872634011 \n","global step:561976, episode return:-1.2132824737550783 \n","global step:563568, episode return:-0.9358036523577788 \n","global step:565160, episode return:-0.7169164858748028 \n","global step:566752, episode return:-0.7665516235684007 \n","global step:568344, episode return:-1.882895857438624 \n","global step:569936, episode return:-0.5880563446773828 \n","global step:571528, episode return:-0.6861716906918243 \n","global step:573120, episode return:-0.9395856224642397 \n","global step:574712, episode return:-1.9988819448931952 \n","global step:576304, episode return:-0.8463732082912446 \n","global step:577896, episode return:-0.9532079827068777 \n","global step:579488, episode return:-0.7220714884218502 \n","global step:581080, episode return:-0.7364801299499193 \n","global step:582672, episode return:-1.1880063638431433 \n","global step:584264, episode return:-0.800569875103501 \n","global step:585856, episode return:-0.9275186644549965 \n","global step:587448, episode return:-0.7224252579808359 \n","global step:589040, episode return:-1.4570286957353336 \n","global step:590632, episode return:-0.7225264322141657 \n","global step:592224, episode return:-0.8805014269819551 \n","global step:593816, episode return:-1.2088145991058703 \n","global step:595408, episode return:-0.9939952484793793 \n","global step:597000, episode return:-0.6803459444519129 \n","global step:598592, episode return:-0.9743602103813389 \n","global step:600184, episode return:-1.100305121224021 \n","global step:601776, episode return:-0.8993002470742594 \n","global step:603368, episode return:-1.004820209854624 \n","global step:604960, episode return:-0.6750679863391786 \n","global step:606552, episode return:-1.0131371504941453 \n","global step:608144, episode return:-0.6798865134298904 \n","global step:609736, episode return:-0.7295535589734482 \n","global step:611328, episode return:-0.6671018490252577 \n","global step:612920, episode return:-1.9349054233010599 \n","global step:614512, episode return:-0.9798203945969466 \n","global step:616104, episode return:-0.6457284336368353 \n","global step:617696, episode return:-0.977738008840214 \n","global step:619288, episode return:-0.8116633013302647 \n","global step:620880, episode return:-0.8216217067502873 \n","global step:622472, episode return:-1.9882712566165706 \n","global step:624064, episode return:-0.6221484523036724 \n","global step:625656, episode return:-0.6577836578029068 \n","global step:627248, episode return:-0.6380993817841072 \n","global step:628840, episode return:-1.1351499074928522 \n","global step:630432, episode return:-0.6815455359237658 \n","global step:632024, episode return:-0.8699916883973933 \n","global step:633616, episode return:-0.6972110997360244 \n","global step:635208, episode return:-0.7007950804735825 \n","global step:636800, episode return:-0.9288762184254198 \n","global step:638392, episode return:-0.9458247403616004 \n","global step:639984, episode return:-0.6149489394474758 \n","global step:641576, episode return:-1.1303874017034474 \n","global step:643168, episode return:-1.0150536907977674 \n","global step:644760, episode return:-1.0839831571614922 \n","global step:646352, episode return:-1.015405348221997 \n","global step:647944, episode return:-0.7437496944452207 \n","global step:649536, episode return:-0.8854266303035063 \n","global step:651128, episode return:-0.6569578510624233 \n","global step:652720, episode return:-1.0622107833454815 \n","global step:654312, episode return:-1.1371940390287683 \n","global step:655904, episode return:-1.0974543993201749 \n","global step:657496, episode return:-0.6525989970440275 \n","global step:659088, episode return:-0.6829183599333688 \n","global step:660680, episode return:-0.5896334666382639 \n","global step:662272, episode return:-0.6709829735492314 \n","global step:663864, episode return:-1.0729716683578494 \n","global step:665456, episode return:-1.0447546702331871 \n","global step:667048, episode return:-0.6515366853948239 \n","global step:668640, episode return:-1.1868485716638033 \n","global step:670232, episode return:-1.1101537639405048 \n","global step:671824, episode return:-1.1888874127771023 \n","global step:673416, episode return:-0.7302083758933856 \n","global step:675008, episode return:-0.9145405846518415 \n","global step:676600, episode return:-0.8107350652424689 \n","global step:678192, episode return:-0.6564305508072673 \n","global step:679784, episode return:-0.6214810937207284 \n","global step:681376, episode return:-0.9618357677239167 \n","global step:682968, episode return:-0.6937978288374362 \n","global step:684560, episode return:-0.7009694369530873 \n","global step:686152, episode return:-0.7672726874934731 \n","global step:687744, episode return:-1.1201967678411284 \n","global step:689336, episode return:-1.9401455146904991 \n","global step:690928, episode return:-1.37980620642606 \n","global step:692520, episode return:-1.122927865607315 \n","global step:694112, episode return:-0.5469443031547281 \n","global step:695704, episode return:-0.6789550448362845 \n","global step:697296, episode return:-0.835576974831019 \n","global step:698888, episode return:-0.6523279215277995 \n","global step:700480, episode return:-1.017148122670789 \n","global step:702072, episode return:-0.9184800188983213 \n","global step:703664, episode return:-1.4098202529756414 \n","global step:705256, episode return:-1.1419163272483688 \n","global step:706848, episode return:-0.8253856590983663 \n","global step:708440, episode return:-1.1742648802537567 \n","global step:710032, episode return:-0.6660697845764997 \n","global step:711624, episode return:-0.6477401852626413 \n","global step:713216, episode return:-1.978891424950272 \n","global step:714808, episode return:-0.9830169240319816 \n","global step:716400, episode return:-0.6741566872616327 \n","global step:717992, episode return:-0.6518969311674991 \n","global step:719584, episode return:-0.6406482700295051 \n","global step:721176, episode return:-0.7144376224035124 \n","global step:722768, episode return:-0.6512351700105675 \n","global step:724360, episode return:-0.6476396175033535 \n","global step:725952, episode return:-0.7692676429781554 \n","global step:727544, episode return:-0.685330306443497 \n","global step:729136, episode return:-1.0571689816538883 \n","global step:730728, episode return:-1.2763744028715451 \n","global step:732320, episode return:-0.5808782493040385 \n","global step:733912, episode return:-1.295755655013018 \n","global step:735504, episode return:-0.5742379024062411 \n","global step:737096, episode return:-0.6801934667251746 \n","global step:738688, episode return:-0.6696036153449201 \n","global step:740280, episode return:-1.0712242796592282 \n","global step:741872, episode return:-1.0524720803217353 \n","global step:743464, episode return:-0.5991955235129244 \n","global step:745056, episode return:-0.9855606122918944 \n","global step:746648, episode return:-0.6676064998971504 \n","global step:748240, episode return:-1.1804293298986055 \n","global step:749832, episode return:-0.7034638715422616 \n","global step:751424, episode return:-1.0898519366567072 \n","global step:753016, episode return:-0.7664150216807661 \n","global step:754608, episode return:-0.723467620903917 \n","global step:756200, episode return:-0.8211970225941492 \n","global step:757792, episode return:-0.7130897948227081 \n","global step:759384, episode return:-0.5947716658518619 \n","global step:760976, episode return:-0.8027662670564703 \n","global step:762568, episode return:-0.7600137075616874 \n","global step:764160, episode return:-0.6716289089257834 \n","global step:765752, episode return:-0.8041254262566252 \n","global step:767344, episode return:-1.2248879882948338 \n","global step:768936, episode return:-1.0413678443683205 \n","global step:770528, episode return:-0.6110304033707201 \n","global step:772120, episode return:-0.5830159970455089 \n","global step:773712, episode return:-0.6780240172136627 \n","global step:775304, episode return:-0.8441921076470955 \n","global step:776896, episode return:-1.989010874065837 \n","global step:778488, episode return:-0.5757589227158452 \n","global step:780080, episode return:-0.662604340701621 \n","global step:781672, episode return:-1.0927386494852083 \n","global step:783264, episode return:-1.499670913025926 \n","global step:784856, episode return:-1.4781985686595605 \n","global step:786448, episode return:-0.697497700445267 \n","global step:788040, episode return:-0.5931189063749227 \n","global step:789632, episode return:-0.6332827809481085 \n","global step:791224, episode return:-0.8073370892713057 \n","global step:792816, episode return:-1.0428432668330412 \n","global step:794408, episode return:-0.6422630349925117 \n","global step:796000, episode return:-1.0535425690467553 \n","global step:797592, episode return:-0.6511152536298175 \n","global step:799184, episode return:-0.8588722278478113 \n","global step:800776, episode return:-0.7473857914449398 \n","global step:802368, episode return:-1.0189677706449969 \n","global step:803960, episode return:-0.7057710320599043 \n","global step:805552, episode return:-1.0146492943204943 \n","global step:807144, episode return:-1.026019297244218 \n","global step:808736, episode return:-1.0552665727801072 \n","global step:810328, episode return:-0.6611118211119817 \n","global step:811920, episode return:-0.6210018833245519 \n","global step:813512, episode return:-0.8576185845996298 \n","global step:815104, episode return:-0.8006707235327434 \n","global step:816696, episode return:-0.7384486732314984 \n","global step:818288, episode return:-0.6460859333396755 \n","global step:819880, episode return:-0.7329643399537791 \n","global step:821472, episode return:-1.2871593173393432 \n","global step:823064, episode return:-1.0277935766873514 \n","global step:824656, episode return:-0.6247463665380358 \n","global step:826248, episode return:-0.6611074759258088 \n","global step:827840, episode return:-0.664196022259011 \n","global step:829432, episode return:-0.6288734104689919 \n","global step:831024, episode return:-0.9302769668788241 \n","global step:832616, episode return:-0.7719917833740754 \n","global step:834208, episode return:-0.6105599546187487 \n","global step:835800, episode return:-0.6153988898147873 \n","global step:837392, episode return:-0.9462323052221061 \n","global step:838984, episode return:-0.6342501957243876 \n","global step:840576, episode return:-1.3136712031844184 \n","global step:842168, episode return:-0.8899005688706396 \n","global step:843760, episode return:-0.7939596137274277 \n","global step:845352, episode return:-1.0838106839865997 \n","global step:846944, episode return:-0.7450105176612359 \n","global step:848536, episode return:-0.6306962912082035 \n","global step:850128, episode return:-0.6553890874119128 \n","global step:851720, episode return:-0.9375058188737425 \n","global step:853312, episode return:-0.9901702785273381 \n","global step:854904, episode return:-1.8274212312342417 \n","global step:856496, episode return:-0.7713550269506577 \n","global step:858088, episode return:-1.0380127042042253 \n","global step:859680, episode return:-0.5742832509135152 \n","global step:861272, episode return:-0.9993877108766628 \n","global step:862864, episode return:-1.3867830850397187 \n","global step:864456, episode return:-0.6510124388407765 \n","global step:866048, episode return:-0.6918471345764361 \n","global step:867640, episode return:-1.306753411423749 \n","global step:869232, episode return:-0.6881032529949531 \n","global step:870824, episode return:-1.1463072349075019 \n","global step:872416, episode return:-0.8546614747730861 \n","global step:874008, episode return:-0.6856299851076028 \n","global step:875600, episode return:-0.654305022781803 \n","global step:877192, episode return:-0.6613039228636705 \n","global step:878784, episode return:-0.6608618381819243 \n","global step:880376, episode return:-0.6079015524635079 \n","global step:881968, episode return:-1.1535650795544048 \n","global step:883560, episode return:-0.8178419164867768 \n","global step:885152, episode return:-0.5978989247286679 \n","global step:886744, episode return:-1.19459102691791 \n","global step:888336, episode return:-0.7207731038932823 \n","global step:889928, episode return:-0.6661841779249971 \n","global step:891520, episode return:-0.6141351195723439 \n","global step:893112, episode return:-1.465181229457872 \n","global step:894704, episode return:-0.632899969066579 \n","global step:896296, episode return:-0.5993839294550718 \n","global step:897888, episode return:-0.9877206914517018 \n","global step:899480, episode return:-0.8805446230501592 \n","global step:901072, episode return:-0.654469331363971 \n","global step:902664, episode return:-1.2442241371585487 \n","global step:904256, episode return:-0.5800447275349465 \n","global step:905848, episode return:-0.6902615865819678 \n","global step:907440, episode return:-0.6296270273562178 \n","global step:909032, episode return:-0.6058311585899976 \n","global step:910624, episode return:-1.465111221367793 \n","global step:912216, episode return:-1.0787289448093593 \n","global step:913808, episode return:-1.394153004459859 \n","global step:915400, episode return:-0.8506170670411283 \n","global step:916992, episode return:-0.7431817644390695 \n","global step:918584, episode return:-0.6309530928715452 \n","global step:920176, episode return:-0.6696976118800048 \n","global step:921768, episode return:-0.9071732450547682 \n","global step:923360, episode return:-0.9188829921650646 \n","global step:924952, episode return:-0.6278353781215439 \n","global step:926544, episode return:-0.6652057573795405 \n","global step:928136, episode return:-0.6723538083371795 \n","global step:929728, episode return:-1.1069622633074963 \n","global step:931320, episode return:-0.7179319133637334 \n","global step:932912, episode return:-0.7030299125308697 \n","global step:934504, episode return:-0.5710748561891047 \n","global step:936096, episode return:-0.6475635834122045 \n","global step:937688, episode return:-0.9397734358399845 \n","global step:939280, episode return:-0.7069335115130794 \n","global step:940872, episode return:-0.9356208506551985 \n","global step:942464, episode return:-0.7658419865859463 \n","global step:944056, episode return:-0.6622845061078134 \n","global step:945648, episode return:-0.8915358189744421 \n","global step:947240, episode return:-0.5719156873177336 \n","global step:948832, episode return:-0.6150598048671907 \n","global step:950424, episode return:-0.6412324204044683 \n","global step:952016, episode return:-0.9349219221774188 \n","global step:953608, episode return:-0.6837093258634946 \n","global step:955200, episode return:-0.7027611055562386 \n","global step:956792, episode return:-0.5618586017445344 \n","global step:958384, episode return:-0.9126129828845851 \n","global step:959976, episode return:-0.6222392476086189 \n","global step:961568, episode return:-0.6662593407571261 \n","global step:963160, episode return:-0.6274782365237084 \n","global step:964752, episode return:-0.6962757788889019 \n","global step:966344, episode return:-0.8692831561731303 \n","global step:967936, episode return:-0.7027988448520369 \n","global step:969528, episode return:-0.7715177664259132 \n","global step:971120, episode return:-0.5268533336737753 \n","global step:972712, episode return:-0.7905618230402386 \n","global step:974304, episode return:-0.7130839435033174 \n","global step:975896, episode return:-0.7338045129664132 \n","global step:977488, episode return:-0.9892993374954475 \n","global step:979080, episode return:-0.5920363897918238 \n","global step:980672, episode return:-0.9791449973567281 \n","global step:982264, episode return:-0.8409226212227511 \n","global step:983856, episode return:-1.6967694536074909 \n","global step:985448, episode return:-1.0817906155448942 \n","global step:987040, episode return:-0.6741430687878858 \n","global step:988632, episode return:-0.8720516670549207 \n","global step:990224, episode return:-0.9144879057972379 \n","global step:991816, episode return:-0.9365603910091286 \n","global step:993408, episode return:-0.7728738212981471 \n","global step:995000, episode return:-0.6139386855563009 \n","global step:996592, episode return:-0.7447458087325495 \n","global step:998184, episode return:-0.9852172842928554 \n","global step:999776, episode return:-0.5895426983969868 \n","global step:1001368, episode return:-0.9276193318044845 \n","global step:1002960, episode return:-0.9343251565793336 \n","global step:1004552, episode return:-0.6207000989313771 \n","global step:1006144, episode return:-1.4653622383113487 \n","global step:1007736, episode return:-0.908103494996539 \n","global step:1009328, episode return:-0.7314578546852972 \n","global step:1010920, episode return:-0.6499850451888345 \n","global step:1012512, episode return:-0.5997092787149165 \n","global step:1014104, episode return:-0.5813892479684352 \n","global step:1015696, episode return:-0.8748814782555879 \n","global step:1017288, episode return:-0.9926869953855872 \n","global step:1018880, episode return:-0.6454707374039623 \n","global step:1020472, episode return:-0.6963064721656986 \n","global step:1022064, episode return:-0.6839384817828522 \n","global step:1023656, episode return:-0.6206631463035108 \n","global step:1025248, episode return:-0.6371611749997618 \n","global step:1026840, episode return:-0.9221185687554239 \n","global step:1028432, episode return:-0.6910541458841736 \n","global step:1030024, episode return:-0.5597987811573369 \n","global step:1031616, episode return:-0.695221449503997 \n","global step:1033208, episode return:-0.5947008890331101 \n","global step:1034800, episode return:-0.5527530088727126 \n","global step:1036392, episode return:-0.5750706234385851 \n","global step:1037984, episode return:-0.5337129462893778 \n","global step:1039576, episode return:-0.9025640276310272 \n","global step:1041168, episode return:-0.6158986084341997 \n","global step:1042760, episode return:-0.8412638696115009 \n","global step:1044352, episode return:-0.9676638143424634 \n","global step:1045944, episode return:-1.3714999826020224 \n","global step:1047536, episode return:-0.741409767055083 \n","global step:1049128, episode return:-1.2548762495113124 \n","global step:1050720, episode return:-0.9813554507454358 \n","global step:1052312, episode return:-0.617099957180315 \n","global step:1053904, episode return:-0.6541274569144377 \n","global step:1055496, episode return:-0.5864096185561873 \n","global step:1057088, episode return:-0.721303854453336 \n","global step:1058680, episode return:-0.6012423432723486 \n","global step:1060272, episode return:-0.5934503903889182 \n","global step:1061864, episode return:-0.5918951626932575 \n","global step:1063456, episode return:-0.8165708904567398 \n","global step:1065048, episode return:-0.6368881707667772 \n","global step:1066640, episode return:-0.5546039944679176 \n","global step:1068232, episode return:-1.6248802657783523 \n","global step:1069824, episode return:-0.7621335662431457 \n","global step:1071416, episode return:-0.6168727984525852 \n","global step:1073008, episode return:-0.5698019331691271 \n","global step:1074600, episode return:-0.907104219077511 \n","global step:1076192, episode return:-0.573715778417582 \n","global step:1077784, episode return:-0.8010258603060424 \n","global step:1079376, episode return:-0.8097091317486876 \n","global step:1080968, episode return:-0.6325314585133832 \n","global step:1082560, episode return:-0.5263629217601176 \n","global step:1084152, episode return:-0.5456449994014619 \n","global step:1085744, episode return:-0.8412800621964842 \n","global step:1087336, episode return:-0.6550336055529068 \n","global step:1088928, episode return:-0.6733630653233057 \n","global step:1090520, episode return:-0.5847984578730877 \n","global step:1092112, episode return:-0.5723320431518142 \n","global step:1093704, episode return:-0.6134047650169449 \n","global step:1095296, episode return:-0.5561341021331629 \n","global step:1096888, episode return:-0.8822878063310953 \n","global step:1098480, episode return:-0.5324188504875583 \n","global step:1100072, episode return:-0.5645240556891438 \n","global step:1101664, episode return:-0.7882415837054504 \n","global step:1103256, episode return:-0.6614213275002863 \n","global step:1104848, episode return:-0.540228540019051 \n","global step:1106440, episode return:-0.5488334840661695 \n","global step:1108032, episode return:-0.9563910222037282 \n","global step:1109624, episode return:-0.5532074512407485 \n","global step:1111216, episode return:-0.5657952938328726 \n","global step:1112808, episode return:-0.5585050595998466 \n","global step:1114400, episode return:-0.9316938763331956 \n","global step:1115992, episode return:-0.6061736176385975 \n","global step:1117584, episode return:-0.6206304656837248 \n","global step:1119176, episode return:-0.6049647470662974 \n","global step:1120768, episode return:-0.7029903778082919 \n","global step:1122360, episode return:-0.9344913904001964 \n","global step:1123952, episode return:-0.636756742450952 \n","global step:1125544, episode return:-0.6608287789803646 \n","global step:1127136, episode return:-0.5300681854998155 \n","global step:1128728, episode return:-0.517331031229401 \n","global step:1130320, episode return:-0.7194547589904046 \n","global step:1131912, episode return:-0.5360765426986809 \n","global step:1133504, episode return:-1.4708654378015313 \n","global step:1135096, episode return:-0.5576936754350484 \n","global step:1136688, episode return:-0.5348544390995719 \n","global step:1138280, episode return:-0.8330335051931891 \n","global step:1139872, episode return:-0.5836941036451808 \n","global step:1141464, episode return:-0.5658705185866643 \n","global step:1143056, episode return:-0.5882484306176442 \n","global step:1144648, episode return:-0.9214847109838792 \n","global step:1146240, episode return:-0.9175912528490854 \n","global step:1147832, episode return:-0.666322933013191 \n","global step:1149424, episode return:-0.7985428428829973 \n","global step:1151016, episode return:-1.588709571453033 \n","global step:1152608, episode return:-0.5178035449394695 \n","global step:1154200, episode return:-0.526876353821889 \n","global step:1155792, episode return:-0.8757937261259857 \n","global step:1157384, episode return:-0.8472938950078722 \n","global step:1158976, episode return:-1.2927897088154945 \n","global step:1160568, episode return:-0.8933972108815343 \n","global step:1162160, episode return:-0.5359208311876705 \n","global step:1163752, episode return:-0.5802656288807058 \n","global step:1165344, episode return:-0.6131346425851225 \n","global step:1166936, episode return:-0.7510901190812906 \n","global step:1168528, episode return:-0.5696900873560266 \n","global step:1170120, episode return:-0.8183590788477605 \n","global step:1171712, episode return:-0.5526707032193294 \n","global step:1173304, episode return:-0.525602856121861 \n","global step:1174896, episode return:-0.7711322274813885 \n","global step:1176488, episode return:-0.6586708284566414 \n","global step:1178080, episode return:-0.5455727421407985 \n","global step:1179672, episode return:-0.8881438225036633 \n","global step:1181264, episode return:-0.6021783858478921 \n","global step:1182856, episode return:-0.557914787830768 \n","global step:1184448, episode return:-0.5907816851995167 \n","global step:1186040, episode return:-0.9897863036132101 \n","global step:1187632, episode return:-1.5051232465184086 \n","global step:1189224, episode return:-0.507217862484492 \n","global step:1190816, episode return:-0.7135859621598111 \n","global step:1192408, episode return:-0.7670295448176225 \n","global step:1194000, episode return:-0.5364323992574858 \n","global step:1195592, episode return:-0.5214943875210679 \n","global step:1197184, episode return:-0.5532362741683664 \n","global step:1198776, episode return:-0.5866480958712026 \n","global step:1200368, episode return:-0.6546152521835844 \n","global step:1201960, episode return:-1.203979114086406 \n","global step:1203552, episode return:-0.5482814237782468 \n","global step:1205144, episode return:-0.6421406108532905 \n","global step:1206736, episode return:-0.755257472767131 \n","global step:1208328, episode return:-0.7784318990158028 \n","global step:1209920, episode return:-0.6187422063692493 \n","global step:1211512, episode return:-0.5248306984527241 \n","global step:1213104, episode return:-0.9177562620530193 \n","global step:1214696, episode return:-1.131566056701711 \n","global step:1216288, episode return:-0.5362866415705484 \n","global step:1217880, episode return:-0.5411670029388735 \n","global step:1219472, episode return:-0.7532163230890799 \n","global step:1221064, episode return:-0.47581508432683883 \n","global step:1222656, episode return:-0.8140804323086857 \n","global step:1224248, episode return:-0.4947601379702214 \n","global step:1225840, episode return:-0.6574324481158098 \n","global step:1227432, episode return:-0.7517632334666167 \n","global step:1229024, episode return:-0.6610145469687896 \n","global step:1230616, episode return:-0.8062582694978023 \n","global step:1232208, episode return:-0.6164444580147876 \n","global step:1233800, episode return:-0.5077793119690484 \n","global step:1235392, episode return:-0.5310460141296561 \n","global step:1236984, episode return:-0.6345573555615011 \n","global step:1238576, episode return:-0.44805068893991234 \n","global step:1240168, episode return:-0.6937319019403244 \n","global step:1241760, episode return:-1.1570952563970833 \n","global step:1243352, episode return:-0.4820308931869138 \n","global step:1244944, episode return:-0.8608157171457738 \n","global step:1246536, episode return:-0.600753523564014 \n","global step:1248128, episode return:-0.4591521128037219 \n","global step:1249720, episode return:-0.471233767374773 \n","global step:1251312, episode return:-0.5116950337944112 \n","global step:1252904, episode return:-0.5560168139816047 \n","global step:1254496, episode return:-0.5940632504089371 \n","global step:1256088, episode return:-0.6244847735004231 \n","global step:1257680, episode return:-0.4227776183643347 \n","global step:1259272, episode return:-0.5063196824172406 \n","global step:1260864, episode return:-0.8160218214565508 \n","global step:1262456, episode return:-0.780931044729865 \n","global step:1264048, episode return:-0.5373993929087654 \n","global step:1265640, episode return:-0.5839926148035026 \n","global step:1267232, episode return:-0.5008155459912785 \n","global step:1268824, episode return:-0.48704809685206435 \n","global step:1270416, episode return:-0.8614872367128878 \n","global step:1272008, episode return:-1.2108050654197962 \n","global step:1273600, episode return:-0.5513556301244672 \n","global step:1275192, episode return:-0.462227099925713 \n","global step:1276784, episode return:-0.7874442258780298 \n","global step:1278376, episode return:-0.7726741463842793 \n","global step:1279968, episode return:-0.928121186059537 \n","global step:1281560, episode return:-0.48227877277688025 \n","global step:1283152, episode return:-0.5134817593210758 \n","global step:1284744, episode return:-0.8030100633903943 \n","global step:1286336, episode return:-0.490600055879973 \n","global step:1287928, episode return:-0.5357464055799637 \n","global step:1289520, episode return:-0.5590040318197954 \n","global step:1291112, episode return:-0.6210120297077184 \n","global step:1292704, episode return:-0.55730140392706 \n","global step:1294296, episode return:-0.9181953363003663 \n","global step:1295888, episode return:-0.5549836418898118 \n","global step:1297480, episode return:-0.5815458991188682 \n","global step:1299072, episode return:-0.4517271125000737 \n","global step:1300664, episode return:-0.5756497336438553 \n","global step:1302256, episode return:-1.0922390399601305 \n","global step:1303848, episode return:-0.47700915149192197 \n","global step:1305440, episode return:-1.1477470942687875 \n","global step:1307032, episode return:-0.7605979656203435 \n","global step:1308624, episode return:-0.48093546616446514 \n","global step:1310216, episode return:-0.5090287059952874 \n","global step:1311808, episode return:-0.6473979659091048 \n","global step:1313400, episode return:-0.44482149799172055 \n","global step:1314992, episode return:-0.4212942101257815 \n","global step:1316584, episode return:-0.5135150653956999 \n","global step:1318176, episode return:-0.4619760696247914 \n","global step:1319768, episode return:-0.468164683686686 \n","global step:1321360, episode return:-0.5505372229482488 \n","global step:1322952, episode return:-0.5611358212401012 \n","global step:1324544, episode return:-0.5082562141401685 \n","global step:1326136, episode return:-0.37376354001153017 \n","global step:1327728, episode return:-0.44401320141039663 \n","global step:1329320, episode return:-0.5599025985282288 \n","global step:1330912, episode return:-0.5129354488823995 \n","global step:1332504, episode return:-0.4963616933182597 \n","global step:1334096, episode return:-0.5904262648305072 \n","global step:1335688, episode return:-0.5189559660911915 \n","global step:1337280, episode return:-0.8049368022520555 \n","global step:1338872, episode return:-0.48058098130313476 \n","global step:1340464, episode return:-0.5603776785822793 \n","global step:1342056, episode return:-0.8538500954122855 \n","global step:1343648, episode return:-0.7849590227472323 \n","global step:1345240, episode return:-1.1244151010679793 \n","global step:1346832, episode return:-0.6883923796024096 \n","global step:1348424, episode return:-0.4706666157759213 \n","global step:1350016, episode return:-0.5676929756000231 \n","global step:1351608, episode return:-1.1918981795818342 \n","global step:1353200, episode return:-0.4793190844209502 \n","global step:1354792, episode return:-1.2106484803640352 \n","global step:1356384, episode return:-0.5277008039384494 \n","global step:1357976, episode return:-0.5768309676281862 \n","global step:1359568, episode return:-0.7043391228864969 \n","global step:1361160, episode return:-0.8201280520821286 \n","global step:1362752, episode return:-0.4888040776712345 \n","global step:1364344, episode return:-0.4314062516561334 \n","global step:1365936, episode return:-1.0842417830736921 \n","global step:1367528, episode return:-0.5086800369524591 \n","global step:1369120, episode return:-0.3726879547603341 \n","global step:1370712, episode return:-0.45061963651790515 \n","global step:1372304, episode return:-0.5775329262125671 \n","global step:1373896, episode return:-0.713997821914128 \n","global step:1375488, episode return:-0.47585412062102783 \n","global step:1377080, episode return:-0.530027037971446 \n","global step:1378672, episode return:-1.043873463152199 \n","global step:1380264, episode return:-0.5139497558816098 \n","global step:1381856, episode return:-0.6285627676386392 \n","global step:1383448, episode return:-0.42305666806385345 \n","global step:1385040, episode return:-0.5076583006820999 \n","global step:1386632, episode return:-0.6216350395826837 \n","global step:1388224, episode return:-0.39777418522610253 \n","global step:1389816, episode return:-0.6731339623073492 \n","global step:1391408, episode return:-0.44010293668326983 \n","global step:1393000, episode return:-0.505768900648748 \n","global step:1394592, episode return:-0.5608331835302037 \n","global step:1396184, episode return:-0.43092237972711095 \n","global step:1397776, episode return:-0.7507437728863444 \n","global step:1399368, episode return:-0.4612343526733894 \n","global step:1400960, episode return:-0.8058565506796918 \n","global step:1402552, episode return:-0.3631281214991998 \n","global step:1404144, episode return:-0.46422827770556974 \n","global step:1405736, episode return:-0.4593277407561141 \n","global step:1407328, episode return:-0.6105507224974783 \n","global step:1408920, episode return:-0.3919011267163692 \n","global step:1410512, episode return:-0.4789417687654363 \n","global step:1412104, episode return:-1.001073843840076 \n","global step:1413696, episode return:-0.6291048745725982 \n","global step:1415288, episode return:-0.8710057345545797 \n","global step:1416880, episode return:-0.4714452680293172 \n","global step:1418472, episode return:-0.5983054894223592 \n","global step:1420064, episode return:-0.40355909630411485 \n","global step:1421656, episode return:-0.5455764971753991 \n","global step:1423248, episode return:-0.5187163531771313 \n","global step:1424840, episode return:-0.8029290407760115 \n","global step:1426432, episode return:-0.6144459945595125 \n","global step:1428024, episode return:-0.7055616865469232 \n","global step:1429616, episode return:-0.5403889085240724 \n","global step:1431208, episode return:-0.7522751286919032 \n","global step:1432800, episode return:-0.43887552322198264 \n","global step:1434392, episode return:-0.6143655365065032 \n","global step:1435984, episode return:-0.5987169196446985 \n","global step:1437576, episode return:-0.7425102096621963 \n","global step:1439168, episode return:-0.4292860221168546 \n","global step:1440760, episode return:-0.5057609106643346 \n","global step:1442352, episode return:-0.5270568766110256 \n","global step:1443944, episode return:-0.3705864031969378 \n","global step:1445536, episode return:-0.5829188854812714 \n","global step:1447128, episode return:-0.4323009232950816 \n","global step:1448720, episode return:-0.724548032766424 \n","global step:1450312, episode return:-0.45195903787454267 \n","global step:1451904, episode return:-0.4622763193969572 \n","global step:1453496, episode return:-0.7527446023829041 \n","global step:1455088, episode return:-0.6105338068394393 \n","global step:1456680, episode return:-0.43303965451767223 \n","global step:1458272, episode return:-0.4252003065929378 \n","global step:1459864, episode return:-0.5815103096072219 \n","global step:1461456, episode return:-0.3898460439497757 \n","global step:1463048, episode return:-0.4345089312260684 \n","global step:1464640, episode return:-0.3514634859250076 \n","global step:1466232, episode return:-0.3347710349583718 \n","global step:1467824, episode return:-0.831341460767178 \n","global step:1469416, episode return:-0.3527213711636843 \n","global step:1471008, episode return:-0.39797132407945357 \n","global step:1472600, episode return:-0.3862351905720405 \n","global step:1474192, episode return:-0.5130589037200906 \n","global step:1475784, episode return:-0.8696094655957984 \n","global step:1477376, episode return:-0.8028787081347503 \n","global step:1478968, episode return:-0.5150937902633758 \n","global step:1480560, episode return:-0.7197305086229524 \n","global step:1482152, episode return:-0.4545097759446589 \n","global step:1483744, episode return:-0.3480168953760327 \n","global step:1485336, episode return:-0.4950624710286605 \n","global step:1486928, episode return:-0.8945169316981234 \n","global step:1488520, episode return:-0.5756212682444745 \n","global step:1490112, episode return:-0.3636515584709393 \n","global step:1491704, episode return:-0.6877920232358499 \n","global step:1493296, episode return:-0.38788723304041994 \n","global step:1494888, episode return:-0.48592553646935094 \n","global step:1496480, episode return:-0.40255629890274935 \n","global step:1498072, episode return:-0.4190576886290998 \n","global step:1499664, episode return:-0.4856390140424461 \n","global step:1501256, episode return:-0.3965308526764263 \n","global step:1502848, episode return:-0.4016872437110929 \n","global step:1504440, episode return:-0.3261341240316025 \n","global step:1506032, episode return:-0.37321162540568076 \n","global step:1507624, episode return:-0.7699367497549335 \n","global step:1509216, episode return:-0.4931592251492243 \n","global step:1510808, episode return:-0.722964866470997 \n","global step:1512400, episode return:-0.3774591432232525 \n","global step:1513992, episode return:-0.5617741541130575 \n","global step:1515584, episode return:-0.40848392719454246 \n","global step:1517176, episode return:-0.7739999788204239 \n","global step:1518768, episode return:-0.42576850594948934 \n","global step:1520360, episode return:-0.3611305606200923 \n","global step:1521952, episode return:-0.6427185278818317 \n","global step:1523544, episode return:-0.34929193387829177 \n","global step:1525136, episode return:-0.41240718179867164 \n","global step:1526728, episode return:-0.38080825838765175 \n","global step:1528320, episode return:-0.3739140876669876 \n","global step:1529912, episode return:-0.5479442142882538 \n","global step:1531504, episode return:-0.6553521921043727 \n","global step:1533096, episode return:-0.4486969430703058 \n","global step:1534688, episode return:-0.5796027208674831 \n","global step:1536280, episode return:-0.7494716245055207 \n","global step:1537872, episode return:-0.7088553374176838 \n","global step:1539464, episode return:-0.5840555232030276 \n","global step:1541056, episode return:-0.41661776738949857 \n","global step:1542648, episode return:-0.8288493488775474 \n","global step:1544240, episode return:-0.5272642697112314 \n","global step:1545832, episode return:-0.34697557682541635 \n","global step:1547424, episode return:-0.3209338221947299 \n","global step:1549016, episode return:-0.3289947852084868 \n","global step:1550608, episode return:-0.38278519203990685 \n","global step:1552200, episode return:-0.41128517038748724 \n","global step:1553792, episode return:-0.39812039050798315 \n","global step:1555384, episode return:-0.3764395439091902 \n","global step:1556976, episode return:-0.4556621661187517 \n","global step:1558568, episode return:-0.38025459049719307 \n","global step:1560160, episode return:-0.4499283077851671 \n","global step:1561752, episode return:-0.5159554172040501 \n","global step:1563344, episode return:-0.5751567957329637 \n","global step:1564936, episode return:-0.3796329810199141 \n","global step:1566528, episode return:-0.7341161867031124 \n","global step:1568120, episode return:-0.3103657247896522 \n","global step:1569712, episode return:-0.8009407525408703 \n","global step:1571304, episode return:-0.2995540844102531 \n","global step:1572896, episode return:-0.3961469574600162 \n","global step:1574488, episode return:-0.7206838597778268 \n","global step:1576080, episode return:-0.5425357226959013 \n","global step:1577672, episode return:-0.8331020460173413 \n","global step:1579264, episode return:-0.512605316634368 \n","global step:1580856, episode return:-0.28394742855920946 \n","global step:1582448, episode return:-0.3031065561487776 \n","global step:1584040, episode return:-0.7800362658638841 \n","global step:1585632, episode return:-0.4058247379828806 \n","global step:1587224, episode return:-0.4448759614527584 \n","global step:1588816, episode return:-0.7228935025185944 \n","global step:1590408, episode return:-0.30822130045769724 \n","global step:1592000, episode return:-0.4533645608215865 \n","global step:1593592, episode return:-0.2801222999361859 \n","global step:1595184, episode return:-0.36723805305548596 \n","global step:1596776, episode return:-0.42366726591011517 \n","global step:1598368, episode return:-0.34129733796594747 \n","global step:1599960, episode return:-0.6522225294957621 \n","global step:1601552, episode return:-0.7464457106929392 \n","global step:1603144, episode return:-0.3315434538826108 \n","global step:1604736, episode return:-0.3935641488507403 \n","global step:1606328, episode return:-0.3573141755881505 \n","global step:1607920, episode return:-0.7076620253822666 \n","global step:1609512, episode return:-0.47392799618254455 \n","global step:1611104, episode return:-0.2595410435672169 \n","global step:1612696, episode return:-0.4774792043606803 \n","global step:1614288, episode return:-0.25361934656941293 \n","global step:1615880, episode return:-0.3696225979509193 \n","global step:1617472, episode return:-0.43292761305298033 \n","global step:1619064, episode return:-0.3490734253961697 \n","global step:1620656, episode return:-0.6134566091320309 \n","global step:1622248, episode return:-0.46504440841448186 \n","global step:1623840, episode return:-0.6165058981047203 \n","global step:1625432, episode return:-0.3391853696762356 \n","global step:1627024, episode return:-0.5072327096553217 \n","global step:1628616, episode return:-0.34873019622678036 \n","global step:1630208, episode return:-0.45037395396805 \n","global step:1631800, episode return:-0.5192937788475307 \n","global step:1633392, episode return:-0.28727932308118015 \n","global step:1634984, episode return:-0.3657917283141881 \n","global step:1636576, episode return:-0.5753304599446434 \n","global step:1638168, episode return:-0.3425564923611207 \n","global step:1639760, episode return:-0.2758983151003921 \n","global step:1641352, episode return:-0.29447996006787003 \n","global step:1642944, episode return:-0.5211064523396887 \n","global step:1644536, episode return:-0.2155177047051322 \n","global step:1646128, episode return:-0.4680346049063003 \n","global step:1647720, episode return:-0.679431260173496 \n","global step:1649312, episode return:-0.27713954245421 \n","global step:1650904, episode return:-0.40385522740451335 \n","global step:1652496, episode return:-0.27718474767887746 \n","global step:1654088, episode return:-0.1969203610241456 \n","global step:1655680, episode return:-0.3073445466846013 \n","global step:1657272, episode return:-0.3840389142599161 \n","global step:1658864, episode return:-0.5340974649123913 \n","global step:1660456, episode return:-0.2852928795604739 \n","global step:1662048, episode return:-0.2959039119620785 \n","global step:1663640, episode return:-0.29731087488725416 \n","global step:1665232, episode return:-0.22954742746277954 \n","global step:1666824, episode return:-0.27487214765580587 \n","global step:1668416, episode return:-0.5462624216579804 \n","global step:1670008, episode return:-0.4619922526637573 \n","global step:1671600, episode return:-0.46752867533077147 \n","global step:1673192, episode return:-0.3390565749658247 \n","global step:1674784, episode return:-0.22670897611465676 \n","global step:1676376, episode return:-0.3502060613549601 \n","global step:1677968, episode return:-0.21091452178922152 \n","global step:1679560, episode return:-0.36438830612572765 \n","global step:1681152, episode return:-0.55936868466812 \n","global step:1682744, episode return:-0.5099164850070795 \n","global step:1684336, episode return:-0.2385985976544891 \n","global step:1685928, episode return:-0.4858873822031086 \n","global step:1687520, episode return:-0.2420581417922476 \n","global step:1689112, episode return:-0.4057502905685232 \n","global step:1690704, episode return:-0.28524161905629764 \n","global step:1692296, episode return:-0.32597993446615364 \n","global step:1693888, episode return:-0.3826791126503242 \n","global step:1695480, episode return:-0.1988292880975265 \n","global step:1697072, episode return:-0.23638466628302301 \n","global step:1698664, episode return:-0.24445318856019158 \n","global step:1700256, episode return:-0.21725160275051192 \n","global step:1701848, episode return:-0.3835517088042236 \n","global step:1703440, episode return:-0.1763267311650713 \n","global step:1705032, episode return:-0.3098393701447405 \n","global step:1706624, episode return:-0.38011062402434337 \n","global step:1708216, episode return:-0.20543698797795887 \n","global step:1709808, episode return:-0.5921291861898438 \n","global step:1711400, episode return:-0.26235962007102087 \n","global step:1712992, episode return:-0.4095576612888483 \n","global step:1714584, episode return:-0.23017257643403552 \n","global step:1716176, episode return:-0.16062085763492823 \n","global step:1717768, episode return:-0.34179709061080116 \n","global step:1719360, episode return:-0.1867732792177888 \n","global step:1720952, episode return:-0.24937667094682925 \n","global step:1722544, episode return:-0.215565426816926 \n","global step:1724136, episode return:-0.3182085776505565 \n","global step:1725728, episode return:-0.22242766205608197 \n","global step:1727320, episode return:-0.5190345921841606 \n","global step:1728912, episode return:-0.24672022426340792 \n","global step:1730504, episode return:-0.2936166655869119 \n","global step:1732096, episode return:-0.5442186157318246 \n","global step:1733688, episode return:-0.43942676103491796 \n","global step:1735280, episode return:-0.23321873466528453 \n","global step:1736872, episode return:-0.3374308337955711 \n","global step:1738464, episode return:-0.2694609510825407 \n","global step:1740056, episode return:-0.17423364443878547 \n","global step:1741648, episode return:-0.464449097011869 \n","global step:1743240, episode return:-0.20081933700682458 \n","global step:1744832, episode return:-0.1949204473201238 \n","global step:1746424, episode return:-0.22504726914515133 \n","global step:1748016, episode return:-0.3790122480761852 \n","global step:1749608, episode return:-0.48570349393466344 \n","global step:1751200, episode return:-0.4201915935085469 \n","global step:1752792, episode return:-0.17418458632409908 \n","global step:1754384, episode return:-0.23085345915418093 \n","global step:1755976, episode return:-0.35001674113418635 \n","global step:1757568, episode return:-0.5224397012564519 \n","global step:1759160, episode return:-0.20260212722549226 \n","global step:1760752, episode return:-0.1451590219182738 \n","global step:1762344, episode return:-0.31535545294083916 \n","global step:1763936, episode return:-0.2647858255336626 \n","global step:1765528, episode return:-0.22002936119021405 \n","global step:1767120, episode return:-0.49626429472319267 \n","global step:1768712, episode return:-0.3069412044223203 \n","global step:1770304, episode return:-0.14366531076081235 \n","global step:1771896, episode return:-0.26745641431033434 \n","global step:1773488, episode return:-0.20037752597982547 \n","global step:1775080, episode return:-0.5524343119534261 \n","global step:1776672, episode return:-0.2545650031855716 \n","global step:1778264, episode return:-0.4758975840138892 \n","global step:1779856, episode return:-0.18512365956561916 \n","global step:1781448, episode return:-0.354847682712609 \n","global step:1783040, episode return:-0.2790162216188021 \n","global step:1784632, episode return:-0.36732051890661854 \n","global step:1786224, episode return:-0.2102479579277667 \n","global step:1787816, episode return:-0.1815399384387013 \n","global step:1789408, episode return:-0.2394688759736625 \n","global step:1791000, episode return:-0.22189756586543982 \n","global step:1792592, episode return:-0.178544618661611 \n","global step:1794184, episode return:-0.20959637956384367 \n","global step:1795776, episode return:-0.38759765775741645 \n","global step:1797368, episode return:-0.1739590236209283 \n","global step:1798960, episode return:-0.1856733462859881 \n","global step:1800552, episode return:-0.28377417416897904 \n","global step:1802144, episode return:-0.1562211806198754 \n","global step:1803736, episode return:-0.18340669641393945 \n","global step:1805328, episode return:-0.1467689018957649 \n","global step:1806920, episode return:-0.1921279921456146 \n","global step:1808512, episode return:-0.34893981938245894 \n","global step:1810104, episode return:-0.21186874703348477 \n","global step:1811696, episode return:-0.20219554482627905 \n","global step:1813288, episode return:-0.18086700884635093 \n","global step:1814880, episode return:-0.19398385194228174 \n","global step:1816472, episode return:-0.2866435367166798 \n","global step:1818064, episode return:-0.11835079598227534 \n","global step:1819656, episode return:-0.11429029714496397 \n","global step:1821248, episode return:-0.3024187020907607 \n","global step:1822840, episode return:-0.24844466078112704 \n","global step:1824432, episode return:-0.1760116384394586 \n","global step:1826024, episode return:-0.3872398411222741 \n","global step:1827616, episode return:-0.42877408373192444 \n","global step:1829208, episode return:-0.37948724514370014 \n","global step:1830800, episode return:-0.1559014319577074 \n","global step:1832392, episode return:-0.14920016353944163 \n","global step:1833984, episode return:-0.16656036999754414 \n","global step:1835576, episode return:-0.12562887927337751 \n","global step:1837168, episode return:-0.28115459905961315 \n","global step:1838760, episode return:-0.26274553731529765 \n","global step:1840352, episode return:-0.1939361311152438 \n","global step:1841944, episode return:-0.10478947818498645 \n","global step:1843536, episode return:-0.16503841526505514 \n","global step:1845128, episode return:-0.11987000312788945 \n","global step:1846720, episode return:-0.34752035055810576 \n","global step:1848312, episode return:-0.2230649861283794 \n","global step:1849904, episode return:-0.2202526646752262 \n","global step:1851496, episode return:-0.18354590402667909 \n","global step:1853088, episode return:-0.4500021536787783 \n","global step:1854680, episode return:-0.18567380287922022 \n","global step:1856272, episode return:-0.48909345315399316 \n","global step:1857864, episode return:-0.1479110396949448 \n","global step:1859456, episode return:-0.39337387218470166 \n","global step:1861048, episode return:-0.270341984373323 \n","global step:1862640, episode return:-0.09475913386347608 \n","global step:1864232, episode return:-0.10472955365062965 \n","global step:1865824, episode return:-0.35650791937574094 \n","global step:1867416, episode return:-0.1441831658736416 \n","global step:1869008, episode return:-0.2268927828437718 \n","global step:1870600, episode return:-0.19634161078320084 \n"]}]},{"cell_type":"code","metadata":{"id":"BCaOBdUjcTRQ"},"source":[""],"execution_count":null,"outputs":[]}]}