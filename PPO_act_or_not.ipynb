{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"PPO_act_or_not.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true,"mount_file_id":"1wGMTUDRWGm1pn9UOTK9t2lpbtcfq0Duj","authorship_tag":"ABX9TyPHA1bK+kslviI7pN1j/cOl"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"THYBlH_niIxB"},"source":["# Note versione"]},{"cell_type":"markdown","metadata":{"id":"Yc_bqk0viIuc"},"source":["In questa versione l'agente ha nello stato un parametro contenente il valore del portafoglio,e deve decidere se investire oppure no.\n"]},{"cell_type":"markdown","metadata":{"id":"Q4mywZkniImU"},"source":["#Imports"]},{"cell_type":"code","metadata":{"id":"1YWMjC7S4mFT"},"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","from torch.optim import Adam\n","from torch.utils.tensorboard import SummaryWriter\n","from torch.distributions import Normal, Categorical\n","from sklearn.preprocessing import StandardScaler\n","\n","import math\n","import copy\n","import random\n","import pickle\n","import pandas as pd\n","import numpy as np\n","import matplotlib\n","from matplotlib.pyplot import *\n","\n","try:\n","    import wandb\n","except:\n","    !pip install wandb -qqq\n","    import wandb\n","\n","import time\n","from datetime import datetime\n","\n","import gym\n","from gym import spaces\n","from gym.utils import seeding\n","\n","import os\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fztc6UBPiWJi"},"source":["# Useful functions and classes"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8YEK_F-g5CpT","executionInfo":{"status":"ok","timestamp":1648669265183,"user_tz":-120,"elapsed":9,"user":{"displayName":"Gabriele Agostino","userId":"09755571968699249879"}},"outputId":"9b6bbaf5-3198-4788-c0a7-748ed559a802"},"source":["DEVICE = torch.device( \"cuda\" )\n","print(\"DEVICE: \", torch.cuda.get_device_name(DEVICE))\n","\n","#matplotlib.rcParams['figure.dpi'] = 200\n","\n","\n","def from_numpy( x ):\n","    return torch.from_numpy( x ).type( torch.float ).to( DEVICE )\n","\n","def to_numpy( x ):\n","    return x.detach().cpu().numpy()"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["DEVICE:  Tesla P100-PCIE-16GB\n"]}]},{"cell_type":"code","metadata":{"id":"VyMPerLJ5Cmw"},"source":["class SingleInstanceMetaClass(type):\n","    def __init__(self, name, bases, dic):\n","        self.__single_instance = None\n","        super().__init__(name, bases, dic)\n"," \n","    def __call__(cls, *args, **kwargs):\n","        if cls.__single_instance:\n","            return cls.__single_instance\n","        single_obj = cls.__new__(cls)\n","        single_obj.__init__(*args, **kwargs)\n","        cls.__single_instance = single_obj\n","        return single_obj"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c1PLkD1kcXo4"},"source":["def _ulcer_index(series):\n","    dd = 1. - series/series.cummax()\n","    ssdd = np.sum(dd**2)\n","    return np.sqrt(np.divide(ssdd, series.shape[0] - 1))\n","    if isinstance(series, pd.DataFrame):\n","        return series.apply(_ulcer_index)\n","    else:\n","        return _ulcer_index(series)\n","\n","\n","def get_martin_ratio(self):\n","    \"\"\"\n","    Returns the martin ratio for all the input time series.\n","    \"\"\"\n","    serie=self.dropna()\n","    rendimento=(serie.iloc[-1]-serie.iloc[0])/serie.iloc[0]\n","    ulcer_index = _ulcer_index(serie)\n","    martin_ratio = rendimento/ulcer_index\n","    return martin_ratio"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YyK-fsWjiqEK"},"source":["# Regression net"]},{"cell_type":"code","metadata":{"id":"I-G3L1r744v-"},"source":["class NormalizationLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, epsilon = 1e-6 ):\n","        super( NormalizationLayer, self ).__init__()\n","        self.epsilon = epsilon\n","        self.w = nn.Parameter( torch.ones( d_model ) )\n","        self.b = nn.Parameter( torch.zeros( d_model ) )\n","        \n","    def forward( self, x ):\n","        mean = x.mean( dim = -1, keepdim = True )\n","        std = x.std( dim = -1, keepdim = True )\n","        return self.w * ( x - mean ) / ( std + self.epsilon ) + self.b\n","\n","# as in https://timeseriestransformer.readthedocs.io/en/latest/README.html#installation the embedding layer is replaced by a generic linear layer\n","class EmbeddingLayer( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( EmbeddingLayer, self ).__init__()\n","        self.embedding = nn.Linear(in_features, out_features)\n","        \n","    def forward( self, x ):\n","        return self.embedding(x)\n","\n","\n","class Time2Vec( nn.Module ):\n","    \"\"\"\n","    Time2Vec implementation\n","    \n","    parameters\n","    in_features: int\n","        number of features of the data\n","    out_features: int\n","        number of out features (k in the original paper)\n","    activation_function: function or function like\n","        the activation function. If none, sin is used\n","    \"\"\"\n","    \n","    def __init__( self, in_features, out_features, activation_function = None ):\n","        super(Time2Vec, self).__init__()\n","        \n","        #i = 0\n","        self.linear_transformation = nn.Linear( in_features, 1, bias = True )\n","        \n","        #1 <= i <= k\n","        self.periodic_transformation = nn.Linear( in_features, out_features - 1, bias = True)\n","        \n","        if activation_function == None: \n","            self.activation_function = torch.sin\n","        \n","    def forward( self, x ):\n","        # x has shape (sequence_length, in_features)\n","        \n","        # linear_x has shape (sequence_length, 1)\n","        linear_x = self.linear_transformation( x )\n","        \n","        # periodic_x has shape (sequence_length, out_features - 1)\n","        periodic_x = self.activation_function( self.periodic_transformation(x) )\n","        \n","        # periodic_x has shape (sequence_length, out_features )\n","        out = torch.cat( [linear_x, periodic_x], dim = -1 )\n","        \n","        return out\n","\n","\n","class Query( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Query, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","        \n","           \n","            \n","class Key( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Key, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","    \n","    \n","            \n","class Value( nn.Module ):\n","    \n","    def __init__( self, in_features, out_features ):\n","        super( Value, self ).__init__()\n","        self.linear_layer = nn.Linear(in_features, out_features)\n","    \n","    def forward( self, x ):\n","        x = self.linear_layer( x )\n","        return x\n","    \n","\n","class MultiHeadAttention( nn.Module ):\n","    \n","    def __init__( self, in_features, d_model, num_heads ):\n","        super( MultiHeadAttention, self ).__init__()\n","        \n","        assert d_model % num_heads == 0\n","        \n","        self.d_model = d_model\n","        self.num_heads = num_heads\n","        self.depth = d_model // num_heads\n","        \n","        self.query = Query( in_features, d_model ) \n","        self.key = Key( in_features, d_model ) \n","        self.value = Value( in_features, d_model )\n","        \n","    def attention( self, query, key, value ):\n","        matmul_qk = torch.matmul( query, key.transpose(-2, -1) )  \n","        scaled_attention_logits = matmul_qk / math.sqrt( self.depth )\n","        attention_weights = F.softmax( scaled_attention_logits, dim = -1 )\n","        output = torch.matmul( attention_weights, value )\n","        return output, attention_weights\n","        \n","    def forward( self, query, key, value ):\n","        \n","        companies = query.size(0)\n","\n","        #linear transformation [ assets, sequence_length, d_model]\n","        query = self.query( query )\n","        key = self.key( key )\n","        value = self.value( value )\n"," \n","        # splitting in num_heads -> [ assets, sequence_length, num_heads, depth]\n","        query = query.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","        key = key.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","        value = value.contiguous().view( companies, -1 , self.num_heads, self.depth )\n","\n","        # [ assets, sequence_length, num_heads, depth] \n","        # -> [ assets, num_heads, sequence_length, depth]\n","        query = query.transpose( 2, 1 )\n","        key = key.transpose( 2, 1 )\n","        value = value.transpose( 2, 1 )\n","\n","        # applying attention\n","        # output [ assets, num_heads, sequence_length, depth]\n","        # attention_weights [ assets, num_heads, sequence_length_q, sequence_length_k]\n","        output, attention_weights = self.attention( query, key, value )\n","        \n","        # [ assets, num_heads, sequence_length, depth]\n","        # -> [ assets, sequence_length, num_heads, depth]\n","        output = output.transpose( 2, 1 )\n","        \n","        # [ assets, seq_len, d_model ]\n","        return output.contiguous().view( companies, -1 , self.d_model)\n","\n","\n","class FeedForward( nn.Module ):\n","    \n","    def __init__( self, in_features, n_layers, d_layers, out_features, dropout ):\n","        super( FeedForward, self ).__init__()\n","        \n","        layers = nn.ModuleList([])\n","        \n","        if n_layers > 1:\n","            layers.append( nn.Linear( in_features, d_layers ) )\n","            layers.append( nn.LeakyReLU() )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( nn.Linear( d_layers, d_layers))\n","                layers.append( nn.LeakyReLU() )\n","                layers.append( nn.Dropout( dropout ) )\n","            layers.append( nn.Linear( d_layers, out_features ) )\n","        else:\n","            layers.append( nn.Linear( in_features, out_features ))\n","\n","        self.net = nn.Sequential( *layers )\n","        \n","    def forward( self, x ):\n","        x = self.net(x)\n","        return x\n","\n","\n","class EncoderLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, in_features, n_layers_ff, d_layers_ff, num_heads, dropout ):\n","        super( EncoderLayer, self ).__init__()\n","        self.norm_layer1 = NormalizationLayer( d_model )\n","        self.norm_layer2 = NormalizationLayer( d_model )\n","        self.dropout_layer1 = nn.Dropout( dropout )\n","        self.dropout_layer2 = nn.Dropout( dropout )\n","        self.mha = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.ffnn = FeedForward( in_features = d_model,\n","                                 n_layers = n_layers_ff,\n","                                 d_layers = d_layers_ff,\n","                                 out_features = d_model, \n","                                 dropout = dropout )\n","        \n","    def forward( self, x ):\n","        x2 = self.mha( x, x, x )\n","        x = self.norm_layer1( x + self.dropout_layer1(x2) )\n","        x2 = self.ffnn( x )\n","        return self.norm_layer2( x + self.dropout_layer2(x2) )\n","\n","\n","class DecoderLayer( nn.Module ):\n","    \n","    def __init__( self, d_model, in_features, n_layers_ff, d_layers_ff, num_heads, dropout  ):\n","        super( DecoderLayer, self ).__init__()\n","        self.norm_layer1 = NormalizationLayer( d_model )\n","        self.norm_layer2 = NormalizationLayer( d_model )\n","        self.norm_layer3 = NormalizationLayer( d_model )\n","        self.dropout_layer1 = nn.Dropout( dropout )\n","        self.dropout_layer2 = nn.Dropout( dropout )\n","        self.dropout_layer3 = nn.Dropout( dropout )\n","        self.mha1 = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.mha2 = MultiHeadAttention( in_features = in_features,\n","                                       d_model = d_model,\n","                                       num_heads = num_heads )\n","        self.ffnn = FeedForward( in_features = d_model,\n","                                 n_layers = n_layers_ff,\n","                                 d_layers = d_layers_ff,\n","                                 out_features = d_model, \n","                                 dropout = dropout )\n","        \n","    def forward( self, x, encoder_output ):\n","        x2 = self.mha1( x, x, x )\n","        x = self.norm_layer1( x + self.dropout_layer1(x2) )\n","        x2 = self.mha2( query = x, key = encoder_output, value = encoder_output )\n","        x = self.norm_layer2( x + self.dropout_layer2(x2) )\n","        x2 = self.ffnn( x )\n","\n","        return self.norm_layer3( x + self.dropout_layer3(x2) )\n","\n","\n","\n","class Encoder( nn.Module ):\n","    \n","    def __init__( self, \n","                 d_model,\n","                 num_layers,\n","                 num_heads,\n","                 t2v_units,\n","                 sequence_length,\n","                 num_features,\n","                 num_ff_layers,\n","                 dim_ff_layers,\n","                 dropout\n","                ):\n","        super( Encoder, self ).__init__()\n","        \n","        self.t2v_layer = Time2Vec( in_features = num_features,\n","                                   out_features =  t2v_units )\n","        self.embedding_layer = EmbeddingLayer( num_features + t2v_units, d_model )\n","        self.encoder_layers = self.get_layers( num_layers = num_layers,\n","                                               d_model = d_model, \n","                                               num_ff_layers = num_ff_layers, \n","                                               dim_ff_layers = dim_ff_layers, \n","                                               num_heads = num_heads,\n","                                               dropout = dropout )\n","        \n","    def get_layers( self, num_layers, d_model, num_ff_layers, dim_ff_layers, num_heads, dropout ):\n","        return nn.ModuleList( [EncoderLayer( d_model = d_model, \\\n","                                             in_features = d_model, \\\n","                                             n_layers_ff = num_ff_layers, \\\n","                                             d_layers_ff = dim_ff_layers, \\\n","                                             num_heads = num_heads,\n","                                             dropout = dropout ) \\\n","                                for _ in range(num_layers)] )\n","        \n","    def forward( self, x ):\n","        #input is [companies, sequence_length, features]\n","\n","        #t2v output is [companies, sequence_length, t2v_units]\n","        x2 = self.t2v_layer(x)\n","\n","        #x is [companies, sequence_length, features + t2v_units]\n","        x = torch.cat( [ x, x2 ], dim = -1)\n","\n","        #x is [companies, sequence_length, d_model]\n","        x = self.embedding_layer( x )\n","\n","        for encoder_layer in self.encoder_layers:\n","            x = encoder_layer(x)\n","\n","        return x\n","\n","\n","class Decoder( nn.Module ):\n","    \n","    def __init__( self,\n","                  d_model,\n","                  num_layers, \n","                  num_heads, \n","                  t2v_units, \n","                  sequence_length,\n","                  num_features,\n","                  num_ff_layers,\n","                  dim_ff_layers, \n","                  dropout ):\n","        super( Decoder, self ).__init__()\n","        self.t2v_layer = Time2Vec( in_features = num_features,\n","                                   out_features =  t2v_units )\n","        self.embedding_layer = EmbeddingLayer( num_features + t2v_units, d_model )\n","        self.decoder_layers = self.get_layers( num_layers = num_layers,\n","                                               d_model = d_model, \n","                                               num_ff_layers = num_ff_layers, \n","                                               dim_ff_layers = dim_ff_layers, \n","                                               num_heads = num_heads, \n","                                               dropout = dropout )\n","        \n","    def get_layers( self, num_layers, d_model, num_ff_layers, dim_ff_layers, num_heads, dropout ):\n","        return nn.ModuleList( [DecoderLayer( d_model = d_model, \\\n","                                             in_features = d_model, \\\n","                                             n_layers_ff = num_ff_layers, \\\n","                                             d_layers_ff = dim_ff_layers, \\\n","                                             num_heads = num_heads,\n","                                             dropout = dropout ) \\\n","                                for _ in range(num_layers)] )\n","        \n","    def forward( self, x, encoder_output ):\n","        #input x is [companies, sequence_length, features]\n","        #encoder output is [ companies, sequence_length, encoder dimension]\n","\n","        #t2v output is [batch size, companies, sequence_length, t2v_units]\n","        x2 = self.t2v_layer(x)\n","\n","        #x is [batch size, companies, sequence_length, features + t2v_units]\n","        x = torch.cat( [ x, x2 ], dim = -1)\n","\n","        #x is [batch size, companies, sequence_length, d_model]\n","        x = self.embedding_layer( x )\n","\n","        for decoder_layer in self.decoder_layers:\n","            x = decoder_layer(x, encoder_output)\n","\n","        return x\n","\n","\n","\n","class Transformer( nn.Module ):\n","\n","    def __init__( self, \n","                 dim_transformer, \n","                 encoder_sequence_length, \n","                 decoder_sequence_length,\n","                 num_layers, \n","                 num_heads,\n","                 t2v_units,\n","                 num_features, \n","                 num_ff_layers, \n","                 dim_ff_layers,\n","                 dropout\n","                ):\n","        super ( Transformer, self ).__init__()\n","        self.encoder = Encoder( d_model = dim_transformer, \n","                                num_layers = num_layers,\n","                                num_heads = num_heads,\n","                                t2v_units = t2v_units,\n","                                sequence_length = encoder_sequence_length,\n","                                num_features = num_features,\n","                                num_ff_layers = num_ff_layers,\n","                                dim_ff_layers = dim_ff_layers,\n","                                dropout = dropout\n","                               )\n","        self.decoder = Decoder( d_model = dim_transformer, \n","                                num_layers = num_layers,\n","                                num_heads = num_heads,\n","                                t2v_units = t2v_units,\n","                                sequence_length = decoder_sequence_length,\n","                                num_features = num_features,\n","                                num_ff_layers = num_ff_layers,\n","                                dim_ff_layers = dim_ff_layers,\n","                                dropout = dropout\n","                               ) \n","        \n","        self.decoder_sequence_ffnn = FeedForward( in_features = decoder_sequence_length, n_layers = num_ff_layers, d_layers = dim_ff_layers, out_features = 1, dropout = dropout )\n","\n","\n","        self.decoder_sequence_length = decoder_sequence_length\n","        \n","    def forward( self, x ):\n","        \n","        #[  encoder_sequence_length, companies, features] -> [ companies, encoder_sequence_length, features]\n","        x = x.transpose(1,0) \n","\n","        #input x is for encoder [ companies, encoder_sequence_length, features]\n","        xe = x\n","        \n","        #input xd is for decoder [ companies, decoder_sequence_length, features]\n","        xd = x[:,-self.decoder_sequence_length:]\n","\n","        #[ companies, encoder_sequence_length, d_model_encoder]\n","        encoder_output = self.encoder(xe)\n","\n","        #[ companies, decoder_sequence_length, d_model_decoder]\n","        decoder_output = self.decoder(xd, encoder_output)\n","\n","        #[ companies, decoder_sequence_length, d_model_decoder] -> [ companies, d_model_decoder]\n","        output = self.decoder_sequence_ffnn( decoder_output.transpose(-2,-1) ).squeeze()\n","\n","        return output\n","\n","\n","class RegressionTransformer( nn.Module ):\n","\n","    def __init__( self, params ):\n","        super( RegressionTransformer, self ).__init__()\n","\n","        self.transformer = Transformer( params.dim_transformer, \n","                                        params.encoder_sequence_length, \n","                                        params.decoder_sequence_length,\n","                                        params.num_layers, \n","                                        params.num_heads,\n","                                        params.t2v_units,\n","                                        params.num_features, \n","                                        params.num_ff_layers, \n","                                        params.dim_ff_layers,\n","                                        params.dropout\n","                                        )\n","        \n","        self.ffnn = FeedForward( params.dim_transformer, params.regression_ff_layers, params.dim_regression_ff_layers, 1, params.dropout)\n","\n","    def forward( self, x):\n","        x = self.transformer(x)\n","        x = self.ffnn(x)\n","        return x"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"C52Dak9VitpQ"},"source":["# Dataloader and Dataset"]},{"cell_type":"code","metadata":{"id":"-J6gJ5ru44te"},"source":["class Dataloader( metaclass=SingleInstanceMetaClass ):\n","    \"\"\"\n","    Loads return and prices from the previously constructed dataset, a DataFrame, saved in pickle format\n","    \"\"\"\n","\n","    def __init__( self, file_path, moving_average ):\n","        \"\"\"\n","        file path: \n","            the path of the Dataframe in pickle format\n","        moving average:\n","            moving average window size applied to data\n","        \"\"\"\n","        #dataframe is loaded\n","        self.data_df = self.load_df( file_path, moving_average )\n","\n","        #number of assets\n","        self.assets = self.data_df.columns.get_level_values(0).unique()\n","        \n","        #number of features \n","        self.features = self.data_df.columns.get_level_values(1).unique()\n","\n","    def load_df( self, file_path: str, moving_average: int ) -> pd.DataFrame :\n","        \"\"\"\n","        file path: \n","            the path of the Dataframe in pickle format\n","        moving average:\n","            moving average window size applied to data\n","\n","        returns:\n","            dataframe from file_path\n","        \"\"\"\n","        data_df = pd.read_pickle( file_path )\n","        data_df =  data_df.rolling( moving_average ).mean().dropna()\n","        return data_df\n","\n","    def load_prices( self, ) -> np.ndarray:\n","        \"\"\"\n","        returns numpy array of shape (number of days, number of assets, number of features)\n","        containing the OHCLV prices \n","        \"\"\"\n","        prices = []\n","        for asset in self.assets:\n","            to_append = self.data_df[asset][self.features].values \n","            prices.append( to_append )\n","\n","        #prices is [ days, assets, features ]\n","        prices = np.stack( prices, axis = 1)\n","        return prices\n","\n","    def load_returns( self, ):\n","        \"\"\"\n","        returns numpy array of shape (number of days, number of assets, number of features)\n","        \"\"\"\n","        self.returns_df = self.data_df.pct_change().fillna(0)\n","        returns = []\n","        for asset in self.assets:\n","            to_append = self.returns_df[asset][self.features].values\n","            returns.append( to_append )\n","        \n","        #returns is [ days, assets, features ]\n","        returns = np.stack( returns, axis = 1)\n","        return returns\n","\n","class Dataset( metaclass=SingleInstanceMetaClass ):\n","\n","    def __init__( self, params ):\n","        # dataloader instance \n","        self.loader = Dataloader( params.file_path, params.moving_average )\n","\n","        # we store the data for env accessibility\n","        self.returns = self.loader.load_returns() # returns are clippend and standardized\n","        self.true_returns = self.loader.load_returns() # returns are pct variations \n","        self.prices = self.loader.load_prices()\n","\n","        # split the indices for training, validation and testing\n","        self.split_indices( params.test_portion, params.val_portion )\n","\n","        # clipping an normalizing the data\n","        self.scale_data( params.feature_clip, params.vol_clip)\n","\n","        # other useful parameters\n","        #self.episode_length = params.b_size\n","        self.episode_length = params.episode_length\n","        self.encoder_sequence_length = params.encoder_sequence_length\n","\n","    def split_indices( self, test_portion, val_portion ):\n","        \"\"\"\n","        test_portion: float\n","            test portion of the dataset\n","        val_portion: float\n","            validation portion of the dataset\n","\n","        split dataset indices in self.train_indices, self.val_indices, self.test_indices : np.ndarray \n","        in accord with the portions. validation portion and test portion are at the end of dataset \n","        i.e. closer to present. \n","        \"\"\"\n","\n","        num_periods = self.returns.shape[0]\n","        start_train_set_index = 0\n","        start_val_set_index = int( num_periods *( 1 - (test_portion + val_portion) ) )\n","        start_test_set_index = int( num_periods * ( 1 - val_portion ) )\n","\n","        self.train_indices = np.arange(start_train_set_index, start_val_set_index)\n","        self.val_indices = np.arange(start_val_set_index, start_test_set_index)\n","        self.train_val_indices = np.arange(start_train_set_index, start_test_set_index)\n","        self.test_indices = np.arange(start_test_set_index, num_periods)\n","    \n","\n","    def scale_data( self, feature_clip = .02, vol_clip = .8):\n","        \"\"\"\n","        feature_clip: float\n","            clipping value for the OHCL features\n","        vol_clip: float\n","            clipping value for Volume\n","\n","        clip OHCLV data and for each asset, a StardardScaler scales OHCL data and another StandardScaler scales Volumes data\n","        Standardized data is stored self.returns\n","        Non standardized data is in sefl.true_returns  \n","        \"\"\"\n","\n","        feature_returns = self.returns[:,:,:-1]\n","        volumes_returns = self.returns[:,:,-1:]\n","\n","        clipped_features = np.clip( feature_returns, - feature_clip , feature_clip )\n","        clipped_volumes = np.clip( volumes_returns, - vol_clip, vol_clip )\n","\n","\n","        features_std = clipped_features.copy()\n","        volumes_std = clipped_volumes.copy()\n","\n","        feature_scalers = {}\n","        volume_scalers = {}\n","\n","        #scale training data\n","        for i in range(features_std.shape[1]):\n","            feature_scalers[i] = StandardScaler()\n","            volume_scalers[i] = StandardScaler()\n","            features_std[self.train_val_indices, i, :] = feature_scalers[i].fit_transform(features_std[self.train_val_indices, i, :]) \n","            volumes_std[self.train_val_indices,i,:] = volume_scalers[i].fit_transform( volumes_std[self.train_val_indices,i,:])\n","\n","            #scale validation data\n","            #features_std[self.val_indices, i, :] = feature_scalers[i].transform(features_std[self.val_indices, i, :]) \n","            #volumes_std[self.val_indices,i,:] = volume_scalers[i].transform( volumes_std[self.val_indices,i,:])\n","\n","            #scale test data\n","            features_std[self.test_indices, i, :] = feature_scalers[i].transform(features_std[self.test_indices, i, :]) \n","            volumes_std[self.test_indices,i,:] = volume_scalers[i].transform( volumes_std[self.test_indices,i,:])\n","\n","        self.returns = np.concatenate([features_std, volumes_std], axis = -1)\n","\n","\n","    def load_sequence_indices( self, ):\n","        \"\"\"\n","        return sequence_indices_encoder, reward_returns_indices\n","        sequence_indices_encoder are used for selecting transformer input from self.returns or self.true_returns in the training phase\n","        reward_returns_indices are used for reward or target \n","        \"\"\"\n","        #questo metodo viene usato nella parte di RL, in modo da poter eventualmente modificare load_sequence in caso si voglia fare multi step forecasting o altre modifiche alla \n","        #regressione con il transformer\n","        indices = self.train_val_indices[ self.encoder_sequence_length : - self.episode_length ]\n","\n","        starting_index = np.random.choice( indices )\n","        sequence_indices = np.arange( starting_index, starting_index + self.episode_length )\n","        sequence_indices_encoder = []\n","        for i in range( self.episode_length ):\n","            sequence_indices_encoder.append( np.arange( sequence_indices[i] - self.encoder_sequence_length, sequence_indices[i] ) )\n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_encoder = 1 + np.array( sequence_indices_encoder )\n","\n","        reward_returns_indices = sequence_indices + 1\n","\n","        return sequence_indices_encoder, reward_returns_indices\n","\n","    def load_test_indices( self, ):\n","        \"\"\"\n","        return sequence_indices_encoder, reward_returns_indices\n","        sequence_indices_encoder are used for selecting transformer input from self.returns or self.true_returns in the training phase\n","        reward_returns_indices are used for reward or target \n","        \"\"\"\n","        #questo metodo viene usato nella parte di RL, in modo da poter eventualmente modificare load_sequence in caso si voglia fare multi step forecasting o altre modifiche alla \n","        #regressione con il transformer\n","        indices = self.test_indices[ self.encoder_sequence_length : ]\n","\n","        sequence_indices_transformer = []\n","        for index in indices:\n","            sequence_indices_transformer.append( np.arange( index - self.encoder_sequence_length, index ) )\n","            \n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","        sequence_indices_transformer = sequence_indices_transformer[:-1]\n","\n","        reward_returns_indices = indices + 1\n","        reward_returns_indices = reward_returns_indices[:-1]\n","\n","        return sequence_indices_transformer, reward_returns_indices, indices\n","\n","    \n","    def load_sequence_and_targets( self, ):\n","        \"\"\"\n","        return transformer_input_sequence, regression_target_sequence\n","        sequence_indices_encoder sequence of standardized data to be used as transformer input\n","        regression_target_sequence are closing returns, target for regression\n","        \"\"\"\n","        indices = self.train_val_indices[ self.encoder_sequence_length : - self.episode_length ]\n","\n","        starting_index = np.random.choice( indices )\n","        sequence_indices = np.arange( starting_index, starting_index + self.episode_length )\n","        sequence_indices_transformer = []\n","        for i in range( self.episode_length ):\n","            sequence_indices_transformer.append( np.arange( sequence_indices[i] - self.encoder_sequence_length, sequence_indices[i] ) )\n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","\n","        target_returns_indices = sequence_indices + 1\n","\n","        transformer_input_sequence = self.returns[ sequence_indices_transformer ]\n","\n","        #since the prediction is passed as input for actor and critic, standardized returns are target\n","        regression_target_sequence = self.returns[ target_returns_indices, :, -2 ]\n","\n","        return transformer_input_sequence, regression_target_sequence\n","\n","    def load_test_sequence_and_targets( self, ):\n","        \"\"\"\n","        return transformer_input_sequence, regression_target_sequence\n","        sequence_indices_encoder sequence of standardized data to be used as transformer input\n","        regression_target_sequence are closing returns, target for regression\n","        \"\"\"\n","        indices = self.test_indices[ self.encoder_sequence_length : ]\n","\n","        sequence_indices_transformer = []\n","        for index in indices:\n","            sequence_indices_transformer.append( np.arange( index - self.encoder_sequence_length, index ) )\n","            \n","        #to be consistent with sequence indices selected above, having used arange we have to add one\n","        sequence_indices_transformer = 1 + np.array( sequence_indices_transformer )\n","        sequence_indices_transformer = sequence_indices_transformer[:-1]\n","\n","        target_returns_indices = indices + 1\n","        target_returns_indices = target_returns_indices[:-1]\n","\n","        transformer_input_sequence = self.returns[ sequence_indices_transformer ]\n","\n","        regression_target_sequence = self.returns[ target_returns_indices, :, -2 ]\n","\n","        return transformer_input_sequence, regression_target_sequence"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsQ9PbHSiyHZ"},"source":["# Environment"]},{"cell_type":"code","metadata":{"id":"arSlHXUs44qv"},"source":["class Sequence():\n","\n","    def __init__(self, num_assets, sequence_len, composition_difference_coef, risk_coef):\n","\n","        self.num_assets = num_assets\n","        self.sequence_len = sequence_len\n","        self.composition_difference_coef = composition_difference_coef\n","        self.risk_coef = risk_coef\n","\n","\n","    def encode( self, ):\n","        #next observation is concatenation of transformer output and portfolio composition\n","        next_obs = np.concatenate( [self.predicted_sequence[ self._idx ], self.portfolio_composition, self.portfolio_value] )\n","        return next_obs\n","\n","    def reset_portfolio_composition( self, ):\n","        portfolio_composition = np.ones((self.num_assets,)) / (self.num_assets)\n","        return portfolio_composition\n","\n","\n","    def reset_portfolio_value( self, ):\n","        initial_portfolio_value = np.ones(1,)\n","        return initial_portfolio_value\n","\n","\n","    def reset( self, predicted_sequence, closing_reward_returns ):\n","        #current index in the sequence\n","        self._idx = 0\n","\n","        #output of the transformer, i.e. prediction of closing returns \n","        self.predicted_sequence = predicted_sequence\n","        \n","        # true returns for the day i.e. input of transformer\n","        self.closing_reward_returns = closing_reward_returns\n","\n","        #current portfolio composition\n","        self.portfolio_composition = self.reset_portfolio_composition()\n","        self.portfolio_value = self.reset_portfolio_value()\n","\n","        self.done = False\n","\n","        obs = self.encode()\n","\n","        return obs\n","    \n","    def step( self, action ):\n","\n","        #action in this version is to_act * action\n","\n","        #calculating reward\n","        next_portfolio_composition, reward, portfolio_returns, pc_difference_sum, portfolio_std = self.reward( action )\n","\n","        #updating the portfolio composition\n","\n","        #TODO: HERE MODIFY THE PORTFOLIO COMPOSITION SHIT\n","        self._idx += 1\n","\n","        self.portfolio_composition = next_portfolio_composition\n","\n","        self.portfolio_value = self.portfolio_value * (1+portfolio_returns) - pc_difference_sum\n","\n","        #checking if done\n","        self.done = self._idx + 1 == self.sequence_len \n","        #getting next_observation\n","        obs = self.encode()\n","\n","        info = {}\n","        info[\"portfolio returns\"] = portfolio_returns\n","        info[\"portfolio difference sum\"] = pc_difference_sum\n","        info[\"portfolio returns std\"] = portfolio_std\n","        \n","        return obs, reward, self.done, info\n","\n","    def reward( self, action ):\n","\n","        #if action is to keep portfolio composition then we do nothing\n","        #in this case action is zeros\n","\n","        if action.sum() == 0:\n","            #this is the do nothing case\n","            next_portfolio_composition = self.portfolio_composition\n","\n","        if action.sum() != 0:\n","            #this is the perform action case\n","            next_portfolio_composition = to_numpy( F.softmax(from_numpy(action), dim = -1) )\n","\n","        returns = self.closing_reward_returns[self._idx]\n","        \n","        portfolio_elements_returns = (next_portfolio_composition * returns)\n","\n","        portfolio_returns = portfolio_elements_returns.sum()\n","\n","        portfolio_std = portfolio_elements_returns.std()\n","\n","        #if the action is not performed this is zero\n","        pc_difference =  np.absolute(next_portfolio_composition - self.portfolio_composition)\n","        pc_difference_sum = pc_difference.sum()\n","\n","        \n","        reward = portfolio_returns - self.composition_difference_coef * pc_difference_sum - self.risk_coef * portfolio_std\n","\n","        return next_portfolio_composition, reward, portfolio_returns, pc_difference_sum, portfolio_std\n","\n","\n","\n","class CustomEnv( gym.Env ):\n","    #required for gym.Env compatibility\n","    metadata = {'render.modes': ['human']}\n","\n","    \n","    def __init__(self,):\n","        super(CustomEnv, self).__init__()\n","\n","        # internal value of parameters\n","        self.params = Parameters()\n","\n","        #parameters to be used in env\n","        self.num_assets = self.params.num_assets + 1\n","        self.prediction_shape = self.params.num_assets \n","\n","        # for memory reasons (vec env), dataset is passed as argument\n","        self.dataset = Dataset( self.params )\n","\n","        #for reward\n","        self._true_returns = self.dataset.true_returns\n","\n","        #for observations\n","        self._returns = self.dataset.returns\n","\n","        # closing prices\n","        self._closing_prices = self.dataset.prices[:,:,-2]\n","\n","        #a sequence object that produces observations, compute rewards and keeps track of portfolio composition \n","        self._sequence = Sequence( sequence_len = self.params.episode_length, \n","                                   num_assets = self.num_assets, \n","                                   composition_difference_coef = self.params.composition_difference_coef, \n","                                   risk_coef = self.params.risk_coef) \n","\n","        self.regression_net = self.load_regression_net()\n","\n","\n","        self.action_space = spaces.Box( low = 0, \n","                                        high = 1., \n","                                        shape = (self.num_assets,), \n","                                        dtype = np.float32)\n","        \n","        self.observation_space = spaces.Box( low = -np.inf,\n","                                             high = np.inf,\n","                                             shape= (self.prediction_shape + self.num_assets + 1,), \n","                                             dtype= np.float32)\n","        \n","    def load_regression_net( self, trained_regression_model_path = \"/content/drive/MyDrive/0_Codice tesi/RUN_DEF/pesi/regression_weights.pt\" ):\n","        regression_net = torch.load( trained_regression_model_path )\n","        return regression_net\n","\n","    def step( self, action ):\n","\n","        next_obs, reward, done, info = self._sequence.step( action )\n","        #info is a dictionary \n","\n","        return next_obs, reward, done, info\n","    \n","\n","    def reset( self, ):\n","\n","        sequence_indices_transformer, reward_returns_indices = self.dataset.load_sequence_indices()\n","        transformer_in_sequence = from_numpy(self._returns[ sequence_indices_transformer ])\n","\n","        predicted_sequence = []\n","        for transformer_in in transformer_in_sequence:\n","            with torch.no_grad():\n","                pred = self.regression_net( transformer_in ).flatten() # after flatten dim: (num_assets,)\n","            predicted_sequence.append( to_numpy(pred) )\n","        predicted_sequence = np.array( predicted_sequence ) # dim: (sequence_len, num_assets,)\n","\n","        closing_reward_returns = np.concatenate( [np.zeros((self.params.episode_length,1) ), self._true_returns[ reward_returns_indices, :, -2 ]], axis = -1)\n","\n","        obs = self._sequence.reset( predicted_sequence, closing_reward_returns )\n","\n","        return obs\n","\n","    def render( self, ):\n","        pass\n","\n","    def close( self, ):\n","        pass\n","\n","    def test_reset( self, ):\n","        # a run on validation set\n","        sequence_indices_transformer, reward_returns_indices, indices_for_closing_prices = self.dataset.load_test_indices()\n","        transformer_in_sequence = from_numpy(self._returns[ sequence_indices_transformer ])\n","\n","        predicted_sequence = []\n","        for transformer_in in transformer_in_sequence:\n","            with torch.no_grad():\n","                pred = self.regression_net( transformer_in ).flatten() # after flatten dim: (num_assets,)\n","            predicted_sequence.append( to_numpy(pred) )\n","        predicted_sequence = np.array( predicted_sequence ) # dim: (sequence_len, num_assets,)\n","\n","        #true sequence to confront with predicted in plot\n","        true_sequence = self._returns[ reward_returns_indices, :, -2 ]\n","\n","        #selecting the closing returns in validation set for reward\n","        closing_reward_returns = np.concatenate( [np.zeros((predicted_sequence.shape[0],1) ), self._true_returns[ reward_returns_indices, :, -2 ]], axis = -1) \n","\n","        #a validation sequence with different sequence length from the training one is created\n","        self._test_sequence = Sequence( sequence_len = predicted_sequence.shape[0], \n","                                       num_assets = self.num_assets, \n","                                       composition_difference_coef = self.params.composition_difference_coef, \n","                                       risk_coef = self.params.risk_coef) \n","        \n","        obs = self._test_sequence.reset( predicted_sequence, closing_reward_returns )\n","        \n","        #closing price portfolios for martin ratio\n","        closing_prices = self._closing_prices[ indices_for_closing_prices ]\n","\n","        return obs, closing_reward_returns, closing_prices, true_sequence, predicted_sequence\n","\n","    def test_step( self, action ):\n","        next_obs, reward, done, info = self._test_sequence.step( action )\n","        return next_obs, reward, done, info\n","\n","    def test( self, ):\n","        #a run on test set\n","        pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TFDphPgi-RRl"},"source":["def make_env():\n","\n","    def child( ):\n","        env = CustomEnv()\n","        env = gym.wrappers.RecordEpisodeStatistics( env )\n","        return env\n","\n","    return child"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pGUORHTFi2DD"},"source":["# Actor Critic"]},{"cell_type":"code","metadata":{"id":"M-SSbjp9FtDw"},"source":["def layer_init( layer, std = np.sqrt(2), bias_const = 0.0 ):\n","    \"\"\"\n","    PPO specific layer initialization\n","    \n","    parameters\n","    \n","    std: float or float-like\n","        default np.sqrt(2)\n","        in actor last layer set to 0.01\n","        in critic last layer set to 1.\n","        \n","    bias: float \n","        default 0\n","        do not change\n","    \"\"\"\n","    \n","    if isinstance( layer, nn.Linear ):\n","        torch.nn.init.orthogonal_( layer.weight, std )\n","        torch.nn.init.constant_( layer.bias, bias_const )\n","    return layer\n","\n","class Critic( nn.Module ):\n","    \"\"\"\n","    Critic architecture for Actor-Critic\n","    \n","    parameters\n","    input_shape: int\n","        the single observation shape of the vector environment \n","        can be obtained as np.array(vec_env.single_observation_space.shape).prod()\n","    \"\"\"\n","    \n","    def __init__( self, input_shape, n_layers, d_layers ):\n","        super( Critic, self ).__init__()\n","\n","        layers = nn.ModuleList([])\n","\n","        if n_layers > 1:\n","            layers.append( layer_init( nn.Linear( input_shape, d_layers ) ) ) #std is sqrt(2)\n","            layers.append( nn.Tanh(), )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( layer_init( nn.Linear( d_layers, d_layers ) ) )  #std is sqrt(2)\n","                layers.append( nn.Tanh() )\n","            layers.append( layer_init( nn.Linear( d_layers,1 ), std = 1.) ) #std is 1.\n","        else:\n","            layers.append( layer_init( nn.Linear( input_shape, 1 ), std = 1. ) ) #std is 1.\n","\n","        self.net = nn.Sequential( *layers )\n","        \n","    def forward( self, x):\n","        x = self.net( x )\n","        return x\n","\n","\n","\n","\n","class Actor( nn.Module ):\n","    \"\"\"\n","    Actor architecture for Actor-Critic\n","    \n","    parameters\n","    input_shape: int\n","        the single observation shape of the vector environment \n","        can be obtained as np.array(vec_env.single_observation_space.shape).prod()\n","        \n","    action_number: int\n","        the number of action in the single environment of the vector environment \n","        can be obtained as np.prod(vec_env.single_action_space.shape)\n","    \"\"\"\n","    \n","    def __init__( self, input_shape, action_number, n_layers, d_layers ):\n","        super( Actor, self ).__init__()\n","\n","        layers = nn.ModuleList([])\n","    \n","        if n_layers > 1:\n","            layers.append( layer_init( nn.Linear( input_shape, d_layers ) ) ) #std is sqrt(2)\n","            layers.append( nn.Tanh(), )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( layer_init( nn.Linear( d_layers, d_layers ) ) )  #std is sqrt(2)\n","                layers.append( nn.Tanh() )\n","            layers.append( layer_init( nn.Linear( d_layers, action_number ), std = .01) ) #std is .01 \n","        else:\n","            layers.append( layer_init( nn.Linear( input_shape, action_number ), std = .01) ) #std is .01 \n","\n","        self.pc_net = nn.Sequential( *layers )\n","\n","        layers = nn.ModuleList([])\n","    \n","        if n_layers > 1:\n","            layers.append( layer_init( nn.Linear( input_shape, d_layers ) ) ) #std is sqrt(2)\n","            layers.append( nn.Tanh(), )\n","            for layer_index in range( n_layers - 1 ):\n","                layers.append( layer_init( nn.Linear( d_layers, d_layers ) ) )  #std is sqrt(2)\n","                layers.append( nn.Tanh() )\n","            layers.append( layer_init( nn.Linear( d_layers, 2 ), std = .01) ) #std is .01 \n","        else:\n","            layers.append( layer_init( nn.Linear( input_shape, 2 ), std = .01) ) #std is .01 \n","        \n","        self.act_net = nn.Sequential( *layers )\n","\n","    def forward( self, x):\n","        pc = self.pc_net( x )\n","        to_act = self.act_net( x )\n","        return pc, to_act\n","\n","\n","\n","class Agent( nn.Module ):\n","    \n","    \"\"\"\n","    the PPO agent\n","    \n","    parameters\n","    vec_env: gym.vector.SyncVectorEnv\n","        the vectorized environment in use\n","    \"\"\"\n","    \n","    def __init__( self, vec_env, n_layers, d_layers ):\n","        super( Agent, self).__init__()\n","        self.critic = Critic( np.array(vec_env.single_observation_space.shape).prod(), \n","                              n_layers,\n","                              d_layers )\n","        self.actor = Actor( np.array(vec_env.single_observation_space.shape).prod(), \n","                            np.array(vec_env.single_action_space.shape).prod(), \n","                            n_layers,\n","                            d_layers )\n","        self.actor_logstd = nn.Parameter( torch.zeros(1, np.prod(vec_env.single_action_space.shape)) )\n","        \n","    #inference for critic\n","    def get_value( self, x ):\n","        # x is observation (number of envs, features in single vec observation)\n","        #returns tensor (num_envs, 1)\n","        return self.critic( x )\n","    \n","    def get_action_and_value( self, x, action = None, to_act = None ):\n","        \n","        #first i get un normalized action probabilities\n","        action_mean, to_act_logits = self.actor( x )\n","        action_logstd = self.actor_logstd.expand_as( action_mean )\n","        action_std = torch.exp( action_logstd )\n","        \n","        probs = Normal( action_mean, action_std )\n","        to_act_probs = Categorical( logits = to_act_logits)\n","        \n","        #in the rollout phase, we sample actions\n","        if action is None and to_act is None:\n","            action = probs.sample()\n","            to_act = to_act_probs.sample()\n","           \n","        #log probabilities of the action\n","        logprobs = probs.log_prob( action ).sum(1)\n","\n","        to_act_logprobs = to_act_probs.log_prob( to_act )\n","        \n","        #entropy of the distribution\n","        entropy = probs.entropy().sum(1)\n","        to_act_entropy = to_act_probs.entropy()\n","        \n","        value = self.critic( x )\n","        \n","        return action, to_act, logprobs, to_act_logprobs, entropy, to_act_entropy, value"],"execution_count":null,"outputs":[]},{"cell_type":"code","source":["##\n","\n","#x , x2\n","\n","#-> rete_on_off( x, x2) #add this try and select adequate layer\n","\n","\n","#[10,90] -> [.1,.9]"],"metadata":{"id":"z2In0uAdBrOA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Coy6urmoi5Ec"},"source":["# Parameters"]},{"cell_type":"code","metadata":{"id":"0gsdC1dF-vGZ"},"source":["class Parameters(metaclass=SingleInstanceMetaClass):\n","\n","    def __init__(self,):\n","\n","        self.file_path = \n","        self.etfs = ['XLB', 'XLC', 'XLE', 'XLF', 'XLI', 'XLK', 'XLP', 'XLRE', 'XLU', 'XLV', 'XLY']\n","        self.num_assets = len(self.etfs) #number of assets\n","        self.moving_average = 10 # moving average smoothing to be applied to data\n","        self.val_portion = .1 #validation portion in dataset\n","        self.test_portion = .1 #test portion in dataset\n","\n","        self.feature_clip = 0.02\n","        self.vol_clip = 0.8\n","\n","        self.encoder_sequence_length = 60 # sequence length for encoder input\n","        self.decoder_sequence_length = 20 # sequence length for decoder input\n","        self.dim_transformer = 64  #transformer model dimension\n","\n","        self.num_features = 5 #OHCLV\n","\n","        #PPO\n","\n","        self.seed = 1 # seed for reproducibility\n","\n","        self.num_envs = 8 #number of parallel environments\n","        self.episode_length = 200 #number of steps in each environment per policy rollout\n","\n","        self.num_ac_layers = 3 # number of feedforward layers in actor and critic\n","        self.dim_ac_layers = 128 #dimension of of feedforward layers in actor and critic\n","\n","        self.composition_difference_coef = 0.0025 #(0.25%) #coefficient for reward calculation\n","        self.risk_coef = 1. #coefficient for reward calculation\n","\n","        self.total_timesteps =  5000000 #the envvironment steps\n","        self.num_steps = self.episode_length #steps in rollout\n","\n","        self.batch_size = self.num_envs * self.num_steps\n","        self.num_updates = self.total_timesteps // self.batch_size #number of updates\n","\n","        self.anneal_lr = True\n","\n","        self.use_single_lr = True\n","        self.learning_rate = 1e-4\n","        self.beta1 = .9\n","        self.beta2 = .9\n","        \n","        self.actor_learning_rate = 1e-4\n","        self.actor_beta1 = .9 # beta1 parameter in adam optimizer\n","        self.actor_beta2 = .99 # beta2 parameter in adam optimizer\n","        self.critic_learning_rate = 5e-4\n","        self.critic_beta1 = .9 # beta1 parameter in adam optimizer\n","        self.critic_beta2 = .99 # beta2 parameter in adam optimizer\n","       \n","        self.adam_eps = 1e-05\n","\n","        self.gae = True #use GAE for advantage calculations\n","        self.gamma = .99\n","        self.gae_lambda = .95\n","\n","        self.num_minibatches = 16\n","        self.minibatch_size = self.batch_size // self.num_minibatches\n","\n","        self.num_update_epochs = 10 #number of times the policy is updated\n","\n","        self.adv_normalization = True # normalize advantages\n","\n","        self.clip_coef = .2 #log probabilities ratio clip value\n","        self.anneal_clip_coef = False #linear decrease of clip coef from init value to 0, but decreases performances\n","        self.clip_values = True # clip value function loss\n","\n","        self.ent_loss_coef = 1e-6\n","        self.v_loss_coef = 0.5\n","\n","        self.max_grad_norm = .5 #global maximum gradient clipping\n","\n","        self.target_kl = None # 0.015 default in openai spinning. Altrimenti, None\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1fTHorQ-i7NB"},"source":["# Main"]},{"cell_type":"code","metadata":{"id":"VY4sKYTXF1b0","colab":{"base_uri":"https://localhost:8080/","height":1000},"outputId":"2f139088-f010-4cc1-b7ab-1994ef300dee"},"source":["p = Parameters()\n","d = Dataset( p )\n","\n","run_name = f\"PortfolioPPO_act_or_not__{p.seed}__{int(time.time())}\"\n","\n","wandb.init( \n","    project = ,\n","    entity = ,\n","    sync_tensorboard=True,\n","    config = vars(p),\n","    name = run_name,\n","    monitor_gym = True,\n","    save_code = True\n",")\n","\n","writer = SummaryWriter()\n","writer.add_text(\n","        \"hyperparameters\",\n","        \"|param|value|\\n|-|-|\\n%s\" % (\"\\n\".join([f\"|{key}|{value}|\" for key, value in vars(p).items()])),\n","    )\n","\n","\n","#setting the seed\n","random.seed( p.seed )\n","np.random.seed( p.seed )\n","torch.manual_seed( p.seed )\n","torch.backends.cudnn.deterministic = True\n","\n","vec_env = gym.vector.SyncVectorEnv( [make_env() for i in range( p.num_envs ) ] )\n","\n","agent = Agent( vec_env, p.num_ac_layers, p.dim_ac_layers ).to( DEVICE )\n","\n","if p.use_single_lr:\n","    optim = Adam( agent.parameters(), lr = p.learning_rate, betas = (p.beta1, p.beta2), eps = p.adam_eps)\n","else:\n","    actor_optim = Adam( agent.actor.parameters(), lr=p.actor_learning_rate, betas=(p.actor_beta1, p.actor_beta2), eps=p.adam_eps)\n","    critic_optim = Adam( agent.critic.parameters(), lr=p.critic_learning_rate, betas=(p.critic_beta1, p.critic_beta2), eps=p.adam_eps)\n","\n","\n","#Per i rollouts, non uso una classe ma questi tensori\n","#obs is ( number of steps, number of envs, features in single vec observation )\n","obs = torch.zeros((p.num_steps, p.num_envs) + vec_env.single_observation_space.shape).to( DEVICE )\n","\n","#actions is ( number of steps, number of envs, features in single vec action )\n","#features in single vec action is in this case 1\n","actions = torch.zeros((p.num_steps, p.num_envs) + vec_env.single_action_space.shape).to( DEVICE )\n","to_act_actions = torch.zeros((p.num_steps, p.num_envs) + tuple([1])).to( DEVICE )\n","\n","#these are ( number of steps, number of envs )\n","logprobs = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","to_act_logprobs = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","rewards = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","dones = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","values = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","\n","#for tracking reward components\n","#portfolio_returns = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","#portfolio_differences = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","#portfolio_returns_std = torch.zeros((p.num_steps, p.num_envs)).to( DEVICE )\n","\n","\n","# a global step counter\n","global_step = 0\n","\n","# for time tracking\n","start_time = time.time()\n","\n","# to store initial observation\n","next_obs = from_numpy( vec_env.reset() )\n","\n","# to store initial termination conditions\n","next_done = torch.zeros( p.num_envs ).to( DEVICE )\n","\n","for update in range(1, p.num_updates + 1):\n","    \n","    #un update  una iterazione del training loop cos composto\n","    #0.update lr\n","    #1.policy rollouts\n","    #2.process rollouts data\n","    #3.policy training\n","    \n","    #0.update lr\n","    if p.anneal_lr:\n","        # decrescita lineare dal valore iniziale fino a 0\n","        # decremento ogni training loop\n","        frac = 1. - (update - 1)/p.num_updates\n","        if p.use_single_lr:\n","            learning_rate_now = frac * p.learning_rate\n","            optim.param_groups[0]['lr'] = learning_rate_now\n","        else:\n","            actor_learning_rate_now = frac * p.actor_learning_rate\n","            actor_optim.param_groups[0]['lr'] = actor_learning_rate_now\n","            critic_learning_rate_now = frac * p.critic_learning_rate\n","            critic_optim.param_groups[0]['lr'] = critic_learning_rate_now\n","        \n","    if p.anneal_clip_coef:\n","        frac = 1. - (update - 1)/p.num_updates\n","        clip_coef_now = frac * p.clip_coef\n","        p.clip_coef = clip_coef_now\n","        \n","        \n","    #1.policy_rollouts\n","    for step in range(0, p.num_steps ):\n","        global_step += 1 * p.num_envs\n","        \n","        obs[step] = next_obs\n","        dones[step] = next_done\n","        \n","        with torch.no_grad():\n","            action, to_act, logprob, to_act_logprob, _, _, value = agent.get_action_and_value( next_obs )\n","            values[step] = value.flatten()\n","\n","        actions[step] = action\n","        to_act_actions[step] = to_act.unsqueeze(-1)\n","        logprobs[step] = logprob \n","        to_act_logprobs[step] = to_act_logprob\n","        \n","        #step dell'environment\n","        next_obs, reward, done, info = vec_env.step( to_numpy( to_act.unsqueeze(-1) * action ) )\n","\n","        rewards[step] = from_numpy( reward )\n","        \n","        #for tracking reward components\n","        #portfolio_returns[step] = from_numpy( info[\"portfolio returns\"] )\n","        #portfolio_differences[step] = from_numpy( info[\"portfolio difference sum\"] )\n","        #portfolio_returns_std[step] = from_numpy( info[\"portfolio returns std\"] )\n","        \n","        next_obs = from_numpy( next_obs )\n","        next_done = from_numpy( done )\n","        \n","        #a print\n","        for item in info:\n","            if \"episode\" in item.keys():\n","                print(f\"global step:{global_step}, episode return:{item['episode']['r']} \")\n","                writer.add_scalar(\"Charts/episode_return\", item[\"episode\"][\"r\"], global_step ) \n","            break\n","                \n","    #2.process data\n","    \n","    #calcolo \n","    with torch.no_grad():\n","        next_value = agent.get_value(next_obs) #(num_envs, 1)\n","        next_value = next_value.reshape(1, -1) #(1,num_envs)\n","        if p.gae: #da implementazione originale\n","            advantages = torch.zeros_like(rewards).to( DEVICE )\n","            lastgaelam = 0\n","            for t in reversed(range(p.num_steps)):\n","                if t == p.num_steps - 1:\n","                    nextnonterminal = 1.0 - next_done\n","                    nextvalues = next_value\n","                else:\n","                    nextnonterminal = 1.0 - dones[t + 1]\n","                    nextvalues = values[t + 1]\n","                delta = rewards[t] + p.gamma * nextvalues * nextnonterminal - values[t]\n","                advantages[t] = lastgaelam = delta + p.gamma * p.gae_lambda * nextnonterminal * lastgaelam\n","            returns = advantages + values\n","        else: #modo tipico di calcolare advantages\n","            returns = torch.zeros_like(rewards).to( DEVICE )\n","            for t in reversed(range(p.num_steps)):\n","                if t == p.num_steps - 1:\n","                    nextnonterminal = 1.0 - next_done\n","                    next_return = next_value\n","                else:\n","                    nextnonterminal = 1.0 - dones[t + 1]\n","                    next_return = returns[t + 1]\n","                returns[t] = rewards[t] + p.gamma * nextnonterminal * next_return\n","            advantages = returns - values\n","        # i returns sono differenti nei due modi (lo posso vedere stampando returns.sum())\n","    \n","    #per usare minibatches, appiattisco i dati calcolati in modo da avere la batch per esteso\n","    # in generale (num_steps, num_envs, single environment shape) -> (batch_size, single environment shape)\n","\n","    b_obs = obs.reshape((-1,) + vec_env.single_observation_space.shape )\n","    b_actions = actions.reshape((-1,) + vec_env.single_action_space.shape )\n","    b_to_act_actions = to_act_actions.reshape( -1 )\n","    b_logprobs = logprobs.reshape(-1)\n","    b_to_act_logprobs = to_act_logprobs.reshape(-1)\n","    b_advantages = advantages.reshape(-1)\n","    b_returns = returns.reshape(-1)\n","    b_values = values.reshape(-1)\n","    \n","    \n","    #3.policy training\n","    b_indices = np.arange(p.batch_size)\n","    #clipped_fractions = [] # tengo conto di quanto spesso il ratio viene clippato\n","    for epoch in range( p.num_update_epochs ):\n","        np.random.shuffle( b_indices )\n","        \n","        #ora itero sulla batch una minibatch alla volta\n","        for start in range(0, p.batch_size, p.minibatch_size):\n","            end = start + p.minibatch_size\n","            mb_indices = b_indices[ start : end ]\n","            \n","            #faccio forward pass sulle osservazioni della minibatch\n","            _ , _, new_logprob, new_to_act_logprob, entropy, to_act_entropy, new_values = agent.get_action_and_value( x = b_obs[ mb_indices ],\n","                                                                                                                      action = b_actions[ mb_indices ],\n","                                                                                                                      to_act = b_to_act_actions[mb_indices]  )\n","            \n","            #azione con cui calcolo il reward: to_act * pc            \n","\n","            #advantages normalization\n","            mb_advantages = b_advantages[ mb_indices ]\n","            if p.adv_normalization:\n","                mb_advantages = (mb_advantages - mb_advantages.mean())/( mb_advantages.std() + 1e-8)\n","\n","            #this time I try to compute different losses for the two networks of the actor (that have separate weights)\n","            log_ratio = new_logprob - b_logprobs[ mb_indices ]\n","            ratio = log_ratio.exp()\n","\n","            #debug variables\n","            with torch.no_grad():\n","                approx_kl = ((ratio - 1) - log_ratio).mean()\n","\n","            #policy loss\n","            surr_loss1 = - mb_advantages * ratio\n","            surr_loss2 = - mb_advantages * torch.clamp( ratio, 1 - p.clip_coef, 1 + p.clip_coef )\n","            # prendo il max siccome ho considero \"- advantages \"\n","            #policy_loss_1 = torch.max(surr_loss1, surr_loss2).mean()\n","            policy_loss_1 = ( b_to_act_actions[mb_indices] *  torch.max(surr_loss1, surr_loss2) ).mean()\n","\n","            #now for the second net\n","            to_act_log_ratio = new_to_act_logprob - b_to_act_logprobs[ mb_indices ]\n","            to_act_ratio = to_act_log_ratio.exp()\n","            to_act_surr_loss1 = - mb_advantages * to_act_ratio\n","            to_act_surr_loss2 = - mb_advantages * torch.clamp( to_act_ratio, 1 - p.clip_coef, 1 + p.clip_coef )\n","            # prendo il max siccome ho considero \"- advantages \"\n","            policy_loss_2 = torch.max(to_act_surr_loss1, to_act_surr_loss2).mean()\n","\n","            policy_loss = policy_loss_1 + policy_loss_2\n","            \n","            #value loss\n","            new_values = new_values.view(-1) # ( minibatch_size, 1) -> (minibatch_size)\n","            \n","            if p.clip_values:\n","                value_loss_unclipped = (new_values - b_returns[ mb_indices ]) ** 2\n","                #values clipped are minibatch values + or - the clipped difference \n","                #between minibatch new values and minibatch values itself  \n","                values_clipped = b_values[ mb_indices ] + torch.clamp(\n","                        new_values - b_values[ mb_indices ],\n","                        -p.clip_coef,\n","                        p.clip_coef,\n","                    )\n","                value_loss_clipped = (values_clipped - b_returns[ mb_indices ]) ** 2\n","                value_loss = torch.max(value_loss_unclipped, value_loss_clipped)\n","                value_loss = 0.5 * value_loss.mean()\n","            else: #value loss  MSE\n","                value_loss = .5 * ((new_values - b_returns[ mb_indices ] )**2).mean()\n","\n","            #entropy loss\n","            entropy_loss = entropy.mean()\n","\n","            loss = policy_loss - p.ent_loss_coef * entropy_loss + p.v_loss_coef * value_loss\n","\n","            if p.use_single_lr:\n","                optim.zero_grad()\n","            else:\n","                actor_optim.zero_grad()\n","                critic_optim.zero_grad()\n","\n","            loss.backward()\n","            nn.utils.clip_grad_norm_( agent.parameters(), p.max_grad_norm )\n","\n","            if p.use_single_lr:\n","                optim.step()\n","            else:\n","                actor_optim.step()\n","                critic_optim.step()\n","        \n","        #kl stop at batch level. lo potrei usare anche a minibatch level\n","        if p.target_kl is not None:\n","            if approx_kl > p.target_kl:\n","                break\n","    \n","  \n","    #if p.use_single_lr:\n","    #    writer.add_scalar(\"Charts/learning_rate\", optim.param_groups[0][\"lr\"], global_step)\n","    #else:\n","    #    writer.add_scalar(\"Charts/actor_learning_rate\", actor_optim.param_groups[0][\"lr\"], global_step)\n","    #    writer.add_scalar(\"Charts/critic_learning_rate\", critic_optim.param_groups[0][\"lr\"], global_step)\n","    #writer.add_scalar(\"Charts/clip_coefficient\", p.clip_coef, global_step)\n","    #writer.add_scalar(\"losses/value_loss\", value_loss.item(), global_step)\n","    #writer.add_scalar(\"losses/policy_loss\", policy_loss.item(), global_step)\n","    #writer.add_scalar(\"losses/entropy\", entropy_loss.item(), global_step)\n","    #writer.add_scalar(\"losses/approx_kl\", approx_kl.item(), global_step)\n","    \n","    if update % 25 == 0:\n","\n","        #Critic network output and its target\n","        _ , _, _, _, _, _, new_values = agent.get_action_and_value( x = b_obs, action = b_actions, to_act = b_to_act_actions )\n","        new_values = new_values.view(-1)\n","\n","        fig = figure(figsize = (13,8))\n","        title(\"Values and returns in batch\")\n","        plot( to_numpy(b_returns[:p.episode_length]), label = \"Returns\")\n","        plot( to_numpy(new_values[:p.episode_length]), label = \"Values\")\n","        xlabel( \"Batch element (#)\")\n","        ylabel( \"Values, Returns (#)\")\n","        legend( loc = \"upper right\")\n","        writer.add_figure(\"media/critic_prediction_and_target\", fig, global_step)\n","        close('all')\n","\n","\n","        # validation \n","        single_env = vec_env.envs[0]\n","\n","        #data for plots\n","        portfolio_compositions_val = []\n","        actions_val = []\n","        to_act_actions_val = []\n","        values_val = []\n","        rewards_val = []\n","        #for tracking reward components\n","        portfolio_returns_val = []\n","        portfolio_differences_val = []\n","        portfolio_returns_std_val = []\n","\n","        #closing returns is np.ndarray with closing returns for portfolio value\n","        next_obs_val, closing_returns_val, closing_prices, true_sequence, predicted_sequence = single_env.test_reset() \n","        next_obs_val =  from_numpy( next_obs_val ) \n","\n","        #register portfolio composition\n","        portfolio_compositions_val.append( to_numpy(next_obs_val[-single_env.num_assets-1:-1]) )\n","        #actions_val.append( to_numpy(next_obs_val[-single_env.num_assets:]) )\n","        values_val.append(0)\n","        rewards_val.append(0)\n","\n","\n","        done_val = False\n","\n","        while not done_val:\n","            \n","            with torch.no_grad():\n","                \n","                #the output of the agent is \n","                #action, to_act, logprob, to_act_logprob, _, _, value = agent( next_obs )\n","                \n","                (action_val, to_act_val), value_val = agent.actor( next_obs_val ), agent.critic( next_obs_val )\n","            \n","            to_act_val = torch.argmax( to_act_val )\n","            next_obs_val, reward_val, done_val, info_val = single_env.test_step( to_numpy( to_act_val * action_val ) )\n","            next_obs_val = from_numpy( next_obs_val )\n","\n","\n","            #register portfolio composition, value, reward\n","            next_portfolio_composition_val = next_obs_val[ p.num_assets : -1 ]\n","\n","            portfolio_compositions_val.append( to_numpy(next_portfolio_composition_val) )\n","            actions_val.append( to_numpy(action_val) )\n","            to_act_actions_val.append( to_numpy(to_act_val) )\n","            values_val.append( value_val.item() )\n","            rewards_val.append( reward_val.item() )\n","\n","            portfolio_returns_val.append( info_val[\"portfolio returns\"]  )\n","            portfolio_differences_val.append( info_val[\"portfolio difference sum\"] )\n","            portfolio_returns_std_val.append( info_val[\"portfolio returns std\"] ) \n","\n","\n","        portfolio_compositions_val = np.array( portfolio_compositions_val )\n","        actions_val = np.array( actions_val )\n","        to_act_actions_val = np.array( to_act_actions_val )\n","        values_val = np.array( values_val )\n","        rewards_val = np.array( rewards_val )\n","        portfolio_returns_val = np.array( portfolio_returns_val )\n","        portfolio_differences_val = np.array( portfolio_differences_val )\n","        portfolio_returns_std_val = np.array( portfolio_returns_std_val )\n","\n","        # as in https://github.com/openai/gym/blob/master/gym/wrappers/record_episode_statistics.py#L29\n","        writer.add_scalar(\"Charts/episode_return_val\", rewards_val.sum(), global_step)\n","        \n","        #compute portfolio value\n","        element_returns_val = portfolio_compositions_val * closing_returns_val\n","        portfolio_returns_val = np.sum( element_returns_val, axis = -1 )\n","        #commission costs\n","        difference_portfolio_composition_val = np.absolute( portfolio_compositions_val[:-1,:] - portfolio_compositions_val[1:,:] )\n","        difference_portfolio_composition_val = np.concatenate( [np.zeros_like( difference_portfolio_composition_val[0:1,:]), difference_portfolio_composition_val ], axis = 0)\n","        difference_portfolio_composition_val = difference_portfolio_composition_val.sum( axis = 1 )\n","        difference_portfolio_composition_val.shape\n","        #returns and value\n","        portfolio_returns_val = portfolio_returns_val - p.composition_difference_coef * difference_portfolio_composition_val\n","        portfolio_values_val = np.cumprod( 1 + portfolio_returns_val )\n","\n","        #equi portfolio as baseline\n","        equi_portfolio_compositions_val = np.array( [portfolio_compositions_val[0] ]*portfolio_compositions_val.shape[0])\n","        equi_element_returns_val = equi_portfolio_compositions_val * closing_returns_val\n","        equi_portfolio_returns_val = np.sum( equi_element_returns_val, axis = -1 )\n","        equi_portfolio_values_val = np.cumprod( 1 + equi_portfolio_returns_val )\n","\n","        #compute martin ratio\n","\n","        \n","        #equi_portfolio_close = np.sum( equi_portfolio_compositions_val[:,1:] * closing_prices, axis = 1)\n","        equi_martin_ratio = get_martin_ratio( pd.Series(equi_portfolio_values_val) )\n","\n","        #portfolio_close = np.sum( portfolio_compositions_val[:,1:] * closing_prices, axis = 1)\n","        martin_ratio = get_martin_ratio( pd.Series(portfolio_values_val) )\n","\n","        #upi for agent portfolio\n","        portfolio_values_val = pd.Series( portfolio_values_val )\n","        drawdown = 1. - portfolio_values_val/portfolio_values_val.cummax()\n","        #drawdown *= 100\n","        drawdown = np.cumsum( drawdown**2 )\n","        indices = np.arange(drawdown.shape[0])\n","        div = np.divide( drawdown, indices )\n","        div[div == np.inf] = 0\n","        ulcer_index = np.sqrt( div )\n","        returns = portfolio_values_val.apply( lambda x: (x - portfolio_values_val.iloc[0]) / portfolio_values_val.iloc[0] )\n","        upis = returns / ulcer_index\n","\n","        #upi for equi portfolio\n","        equi_portfolio_values_val = pd.Series( equi_portfolio_values_val )\n","        equi_drawdown = 1. - equi_portfolio_values_val/equi_portfolio_values_val.cummax()\n","        #equi_drawdown *= 100\n","        equi_drawdown = np.cumsum( equi_drawdown**2 )\n","        equi_indices = np.arange(equi_drawdown.shape[0])\n","        equi_div = np.divide( equi_drawdown, equi_indices )\n","        equi_div[equi_div == np.inf] = 0\n","        equi_ulcer_index = np.sqrt( equi_div )\n","        equi_returns = equi_portfolio_values_val.apply( lambda x: (x - equi_portfolio_values_val.iloc[0]) / equi_portfolio_values_val.iloc[0] )\n","        equi_upis = equi_returns / equi_ulcer_index\n","\n","        assert np.round(upis.iloc[-1], 3 ) == np.round(martin_ratio, 3 ), \"Differs from Dario's function\"\n","        assert np.round(equi_upis.iloc[-1], 3 ) == np.round(equi_martin_ratio, 3 ), \"Differs from Dario's function\"\n","       \n","\n","        #register in writer\n","        writer.add_scalar(\"Charts/agent_martin_ratio_val\", martin_ratio, global_step)\n","        writer.add_scalar(\"Charts/equi_martin_ratio_val\", equi_martin_ratio, global_step)\n","\n","\n","        #### PLOTS ####\n","\n","        ### PORTFOLIO VALUE COMPARISON PLOT ###\n","        #agent portfolio value compared to equi portfolio value\n","        fig, ax = subplots( 1, 1, figsize = (13,8))\n","        title(\"Agent and equally-weighted portfolio value comparison on test set\")\n","        ax.plot(portfolio_values_val, label = \"agent pv\")\n","        ax.plot(equi_portfolio_values_val, label = \"equi pv\")\n","        ax.legend()\n","        ax.grid(axis='both', which='both')\n","        ax.set_xlabel(\"Day from start of set (#)\")\n","        ax.set_ylabel(\"Portfolio value (#)\")\n","        writer.add_figure(\"media/agent_equi_portfolio_value_comparison\", fig, global_step)\n","        close('all')\n","        \n","\n","        ### AGENT PORTFOLIO COMPOSITION PLOT ###\n","        #plot with bars\n","        xs = np.arange( portfolio_compositions_val.shape[0] )\n","        all_assets = [\"cash\"]+p.etfs\n","        width = 1\n","\n","        fig, ax = subplots( 1,1, figsize = (13,8) )\n","        title(\"Agent portfolio composition\")\n","        for i in range( len(all_assets) ):\n","            if i == 0:\n","                ax.bar(xs, portfolio_compositions_val[:,i], width, label  = all_assets[i] )\n","            else:\n","                ax.bar(xs, portfolio_compositions_val[:,i], width, bottom=np.sum( portfolio_compositions_val[:,:i], axis = 1 ), label  = all_assets[i] )\n","        legend()\n","        ylabel(\"Composition (%)\")\n","        xlabel(\"Day from start of test set (#)\")\n","        writer.add_figure(\"media/agent_portfolio_composition\", fig, global_step)\n","        close('all')\n","        \n","\n","        \n","\n","        ### PLOT WITH TRANSFORMER PREDICTION ###\n","        fig, axs = subplots( p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Returns, predictions and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets):\n","            axs[i].set_title(f\"{p.etfs[i]}\")\n","            axs[i].plot( true_sequence[:,i], label = \"True returns\" )\n","            axs[i].plot( predicted_sequence[:,i], label = \"Predicted returns\" )\n","            axs[i].set_ylabel(\"returns (%)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i+1], color = \"tab:green\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            axs[i].grid(axis='x', which='both')\n","            if i!=p.num_assets - 1:\n","                axs[i].set_xticklabels([])\n","            if i==p.num_assets:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/prediction_allocations\", fig, global_step)\n","        close('all')\n","\n","        ### PLOTS WITH RETURNS ###\n","        fig, axs = subplots( 1 + p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Returns and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets + 1):\n","            if i == 0:\n","                axs[i].set_title(\"CASH\")\n","            else:\n","                axs[i].set_title(f\"{p.etfs[i-1]}\")\n","            axs[i].plot( closing_returns_val[:,i], label = \"closing returns\")\n","            axs[i].hlines( 0, xmin = 0, xmax = closing_returns_val[:,i].shape[0], linestyles= \"dashed\", alpha = .3)\n","            axs[i].set_ylabel(\"returns (%)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i], color = \"tab:orange\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            axs[i].grid(axis='x', which='both')\n","            if i!=11:\n","                axs[i].set_xticklabels([])\n","            if i==11:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/returns_and_allocation\", fig, global_step)\n","        close('all')\n","\n","        fig, axs = subplots( 1 + p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Returns and to act choice in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets + 1):\n","            if i == 0:\n","                axs[i].set_title(\"CASH\")\n","            else:\n","                axs[i].set_title(f\"{p.etfs[i-1]}\")\n","            axs[i].plot( closing_returns_val[:,i], label = \"closing returns\")\n","            axs[i].hlines( 0, xmin = 0, xmax = closing_returns_val[:,i].shape[0], linestyles= \"dashed\", alpha = .3)\n","            axs[i].set_ylabel(\"returns (%)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( to_act_actions_val, color = \"tab:orange\",label = \"to perform action\")\n","            axs2.set_ylabel( \"to perform action (bool) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            axs[i].grid(axis='x', which='both')\n","            if i!=11:\n","                axs[i].set_xticklabels([])\n","            if i==11:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/returns_and_to_act\", fig, global_step)\n","        close('all')\n","\n","\n","        ### PLOTS WITH CLOSING PRICES ####\n","        #Closing prices and allocation\n","        fig, axs = subplots( p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Closing prices and allocation in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets):\n","            axs[i].set_title(f\"{p.etfs[i]}\")\n","            axs[i].plot( closing_prices[:,i], label = \"Closing prices\" )\n","            axs[i].set_ylabel(\"closing prices ($)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( portfolio_compositions_val[:,i+1], color = \"tab:orange\",label = \"agent allocation\")\n","            axs2.set_ylabel( \"Allocation(%) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            axs[i].grid(axis='x', which='both')\n","            if i!=p.num_assets - 1:\n","                axs[i].set_xticklabels([])\n","            if i==p.num_assets:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/prices_allocation\", fig, global_step)\n","        #savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/prices_allocation/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/tpa/{str(global_step)}.png\")\n","        #show()\n","        close('all')\n","\n","        #Closing prices and to act\n","        fig, axs = subplots( p.num_assets , 1, figsize = (13,30))\n","        fig.suptitle(\"Closing prices and to act choice in test set\", fontsize = 21)\n","        fig.subplots_adjust(top= .95)\n","        for i in range( p.num_assets):\n","            axs[i].set_title(f\"{p.etfs[i]}\")\n","            axs[i].plot( closing_prices[:,i], label = \"Closing prices\" )\n","            axs[i].set_ylabel(\"closing prices ($)\")\n","            axs2 = axs[i].twinx()\n","            axs2.plot( to_act_actions_val, color = \"tab:orange\",label = \"to perform action\")\n","            axs2.set_ylabel( \"to perform action (bool) \" )\n","            axs2.legend( loc = \"lower right\")\n","            axs[i].legend( loc = \"upper right\" )\n","            axs[i].grid(axis='x', which='both')\n","            if i!=p.num_assets - 1:\n","                axs[i].set_xticklabels([])\n","            if i==p.num_assets:\n","                axs[i].set_xlabel(\"Day from start of set(#)\")\n","        writer.add_figure(\"Media/prices_to_act\", fig, global_step)\n","        #savefig( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/prices_allocation/{str(global_step)}.png\")\n","        #files.download( f\"/content/drive/MyDrive/0_Codice tesi/PPO_logs/{run_name}/tpa/{str(global_step)}.png\")\n","        #show()\n","        close('all')\n","\n","\n","        ##### ULCER PERFORMANCE INDEX PLOT ####\n","        fig, ax = subplots(2,2, figsize = (13,8), sharex = True)\n","        fig.suptitle(f\"Agent UPI {upis.iloc[-1]:.2f}\")\n","\n","        ax[0,0].set_title(\"Agent portfolio value\")\n","        ax[0,0].plot(portfolio_values_val.values, color = \"black\")\n","        ax[0,0].set_ylabel( \"Price ($)\" )\n","        ax[0,0].set_xlabel( \"Day from start of set (#)\" )\n","        ax[0,0].grid(axis='both', which='both')\n","\n","        ax[1,0].set_title(\"Agent portfolio UPI\")\n","        ax[1,0].plot(upis.replace([np.inf, -np.inf], np.nan ).fillna(0).values, color = \"red\")\n","        ax[1,0].set_ylabel( \"UPI (#)\" )\n","        ax[1,0].set_xlabel( \"Day from start of set (#)\" )\n","        ax[1,0].grid(axis='both', which='both')\n","\n","        ax[0,1].set_title(\"Equi portfolio value\")\n","        ax[0,1].plot(equi_portfolio_values_val.values, color = \"black\")\n","        ax[0,1].set_ylabel( \"Price ($)\" )\n","        ax[0,1].set_xlabel( \"Day from start of set (#)\" )\n","        ax[0,1].grid(axis='both', which='both')\n","\n","        ax[1,1].set_title(\"Equi portfolio UPI\")\n","        ax[1,1].plot(equi_upis.replace([np.inf, -np.inf], np.nan ).fillna(0).values, color = \"red\")\n","        ax[1,1].set_ylabel( \"UPI (#)\" )\n","        ax[1,1].set_xlabel( \"Day from start of set (#)\" )\n","        ax[1,1].grid(axis='both', which='both')\n","        writer.add_figure(\"Media/UPI\", fig, global_step)\n","        #show()\n","        close('all')\n","\n","\n","        ### REWARD AND COMPONENTS PLOT ####\n","\n","        #fig, ax = subplots( 4, 1, figsize = (13,15), sharex = True)\n","        #fig.suptitle(f\"Reward with components\")\n","        #ax[0].set_title(\"Reward\")\n","        #ax[0].plot( rewards_val, label = \"reward\" )\n","        #ax[0].plot( portfolio_returns_val, label = \"closing returns\", alpha = .5 )\n","        #ax[0].plot( p.composition_difference_coef * portfolio_differences_val, label = \"commission\", alpha = .5 )\n","        #ax[0].plot( p.risk_coef * portfolio_returns_std_val, label = \"risk\", alpha = .5 )\n","        #ax[0].legend()\n","        #ax[0].set_ylabel( \"reward (#)\" )\n","#\n","        #ax[1].set_title(\"Portfolio returns\")\n","        #ax[1].plot( portfolio_returns_val )\n","        #ax[1].set_ylabel( \"reward (#)\" )\n","#\n","        #ax[2].set_title(\"Commission proxy term\")\n","        #ax[2].plot( portfolio_differences_val )\n","        #ax[2].set_ylabel( \"reward (#)\" )\n","#\n","        #ax[3].set_title(\"Portfolio returns std\")\n","        #ax[3].plot( portfolio_returns_std_val )\n","        #ax[3].set_ylabel( \"reward (#)\" )\n","        #ax[3].set_xlabel( \"Day from start of set (#)\" )\n","        #writer.add_figure(\"Media/Reward_and_components\", fig, global_step)\n","        #close('all')\n","\n","vec_env.close()\n","writer.close()"],"execution_count":null,"outputs":[{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33m4g0\u001b[0m (use `wandb login --relogin` to force relogin)\n"]},{"data":{"text/html":["Tracking run with wandb version 0.12.11"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Run data is saved locally in <code>/content/wandb/run-20220330_194116-24trb19q</code>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"data":{"text/html":["Syncing run <strong><a href=\"https://wandb.ai/4g0/DEF/runs/24trb19q\" target=\"_blank\">PortfolioPPO_act_or_not__1__1648669270</a></strong> to <a href=\"https://wandb.ai/4g0/DEF\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{},"output_type":"display_data"},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["global step:1592, episode return:-0.19757809439729612 \n","global step:3184, episode return:-0.08188520325555347 \n","global step:4776, episode return:-0.13872056209156525 \n","global step:6368, episode return:-0.040800812490922794 \n","global step:7960, episode return:-0.09691321786964112 \n","global step:9552, episode return:-0.3535958145838438 \n","global step:11144, episode return:-0.2910867770268129 \n","global step:12736, episode return:-0.22110343406406288 \n","global step:14328, episode return:-0.2507754849392531 \n","global step:15920, episode return:-0.2508657011137901 \n","global step:17512, episode return:-0.06459671067727529 \n","global step:19104, episode return:-0.20660196342805737 \n","global step:20696, episode return:-0.1632170386661128 \n","global step:22288, episode return:-0.18534275269265968 \n","global step:23880, episode return:0.004698081695613367 \n","global step:25472, episode return:-0.17799270613764048 \n","global step:27064, episode return:-0.01819579405829223 \n","global step:28656, episode return:-0.01705785401067935 \n","global step:30248, episode return:0.02129495262919479 \n","global step:31840, episode return:-0.3003908194191138 \n","global step:33432, episode return:-0.06244155770821157 \n","global step:35024, episode return:-0.04427783324372821 \n","global step:36616, episode return:0.00924513818643305 \n","global step:38208, episode return:-0.007413359300211798 \n","global step:39800, episode return:0.00887453792204676 \n","global step:41392, episode return:-0.09830358825367551 \n","global step:42984, episode return:-0.11060798848115065 \n","global step:44576, episode return:-0.18658400537935138 \n","global step:46168, episode return:0.06053649397414434 \n","global step:47760, episode return:0.10392290741380038 \n","global step:49352, episode return:-0.07937621249189905 \n","global step:50944, episode return:-0.26230922198734996 \n","global step:52536, episode return:0.09058945369785294 \n","global step:54128, episode return:0.1132466306125379 \n","global step:55720, episode return:0.013068962543269832 \n","global step:57312, episode return:0.10707445447031341 \n","global step:58904, episode return:0.142041412907801 \n","global step:60496, episode return:0.1251947471668869 \n","global step:62088, episode return:0.09710721965061272 \n","global step:63680, episode return:-0.06105420204147958 \n","global step:65272, episode return:0.05721696192828182 \n","global step:66864, episode return:0.06045661314772757 \n","global step:68456, episode return:0.04993756598525669 \n","global step:70048, episode return:-0.014570358007506123 \n","global step:71640, episode return:0.07411441618452308 \n","global step:73232, episode return:-0.1978741097596742 \n","global step:74824, episode return:-0.24240358464571896 \n","global step:76416, episode return:-0.12599698952366983 \n","global step:78008, episode return:-0.23408045482350204 \n","global step:79600, episode return:0.04981143197238968 \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_utils.py:22: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n","  canvas.draw()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["global step:81192, episode return:-0.013720819991337911 \n","global step:82784, episode return:0.06100357460513187 \n","global step:84376, episode return:0.1188404878453429 \n","global step:85968, episode return:0.060197645970549926 \n","global step:87560, episode return:0.0295771641437864 \n","global step:89152, episode return:-0.10739417897773958 \n","global step:90744, episode return:0.08533435271117569 \n","global step:92336, episode return:0.04141331084254153 \n","global step:93928, episode return:0.06587041579046111 \n","global step:95520, episode return:0.02754668961147485 \n","global step:97112, episode return:-0.15752834319008366 \n","global step:98704, episode return:0.15176228560752186 \n","global step:100296, episode return:-0.590493844050932 \n","global step:101888, episode return:0.03712685382372919 \n","global step:103480, episode return:-0.2326987528665804 \n","global step:105072, episode return:-0.06599941091502658 \n","global step:106664, episode return:-0.022934741577637915 \n","global step:108256, episode return:0.042198344415237864 \n","global step:109848, episode return:-0.7182054078287351 \n","global step:111440, episode return:-0.0820069146717443 \n","global step:113032, episode return:-0.08563251198768461 \n","global step:114624, episode return:-0.031096173389742656 \n","global step:116216, episode return:-0.2756060658789528 \n","global step:117808, episode return:0.28712220205827577 \n","global step:119400, episode return:-0.024154812098047214 \n","global step:120992, episode return:-0.2343325433403174 \n","global step:122584, episode return:0.11155485084223399 \n","global step:124176, episode return:0.042335908761264224 \n","global step:125768, episode return:-0.4661235099911317 \n","global step:127360, episode return:-0.010292633148589315 \n","global step:128952, episode return:-0.11959042255025382 \n","global step:130544, episode return:-0.000847741366309281 \n","global step:132136, episode return:-0.02393026995023007 \n","global step:133728, episode return:-0.005199543175611645 \n","global step:135320, episode return:-0.030070891511791813 \n","global step:136912, episode return:0.020654012918593274 \n","global step:138504, episode return:-0.08850821075044284 \n","global step:140096, episode return:-0.1043623254737428 \n","global step:141688, episode return:-0.037436638333767175 \n","global step:143280, episode return:0.0628745779727218 \n","global step:144872, episode return:-0.11104634238863197 \n","global step:146464, episode return:-0.4177153926277198 \n","global step:148056, episode return:0.11350528705214856 \n","global step:149648, episode return:0.02891339150980848 \n","global step:151240, episode return:-0.19868603146613767 \n","global step:152832, episode return:-0.033223100634674534 \n","global step:154424, episode return:-0.05475105639919207 \n","global step:156016, episode return:0.10428110024922817 \n","global step:157608, episode return:0.275487437134219 \n","global step:159200, episode return:0.08125543208322084 \n","global step:160792, episode return:-0.025046617678749373 \n","global step:162384, episode return:-0.12900355492582033 \n","global step:163976, episode return:0.0013747142110461736 \n","global step:165568, episode return:0.07892090689396238 \n","global step:167160, episode return:0.07707852007900501 \n","global step:168752, episode return:0.10250632921004807 \n","global step:170344, episode return:-0.05174895812055996 \n","global step:171936, episode return:0.09912179332397537 \n","global step:173528, episode return:-0.05640079670115934 \n","global step:175120, episode return:0.1477602214952045 \n","global step:176712, episode return:0.08768512588816037 \n","global step:178304, episode return:0.04987305436806465 \n","global step:179896, episode return:0.03139450603886562 \n","global step:181488, episode return:-0.013278910674193019 \n","global step:183080, episode return:-0.37942519508059275 \n","global step:184672, episode return:-0.019438655403608313 \n","global step:186264, episode return:-0.01844502438035423 \n","global step:187856, episode return:-0.08980730411736212 \n","global step:189448, episode return:-0.023536574031301737 \n","global step:191040, episode return:-0.029825461224870765 \n","global step:192632, episode return:0.127709881641766 \n","global step:194224, episode return:-0.16796619917734298 \n","global step:195816, episode return:-0.17133348202427506 \n","global step:197408, episode return:-0.14234326871912226 \n","global step:199000, episode return:0.09495214409069491 \n","global step:200592, episode return:-0.02531225720846156 \n","global step:202184, episode return:-0.12866550588395284 \n","global step:203776, episode return:-0.27144637056682536 \n","global step:205368, episode return:0.12103485189531984 \n","global step:206960, episode return:0.10594877368799119 \n","global step:208552, episode return:-0.455306441569741 \n","global step:210144, episode return:0.028097399768637288 \n","global step:211736, episode return:-0.020296081590564407 \n","global step:213328, episode return:-0.34782280652913244 \n","global step:214920, episode return:-0.09873231238111355 \n","global step:216512, episode return:0.11167721949297331 \n","global step:218104, episode return:0.13730725237245958 \n","global step:219696, episode return:-0.0792719225755591 \n","global step:221288, episode return:0.0462510633079452 \n","global step:222880, episode return:0.058373292828325096 \n","global step:224472, episode return:0.15907011408662322 \n","global step:226064, episode return:-0.11425828117050861 \n","global step:227656, episode return:0.05380464768740387 \n","global step:229248, episode return:-0.03656381798442028 \n","global step:230840, episode return:0.027533713225655735 \n","global step:232432, episode return:-0.0735223398878982 \n","global step:234024, episode return:-0.26287036332381064 \n","global step:235616, episode return:0.11026128844746827 \n","global step:237208, episode return:-0.24345419037972452 \n","global step:238800, episode return:0.05640419788156506 \n","global step:240392, episode return:0.12033238194530486 \n","global step:241984, episode return:0.06127181734328665 \n","global step:243576, episode return:-0.022829166098275018 \n","global step:245168, episode return:0.020167397478549547 \n","global step:246760, episode return:-0.42520305029133065 \n","global step:248352, episode return:-0.34067074739453423 \n","global step:249944, episode return:-0.02492474283420175 \n","global step:251536, episode return:0.007951344005705241 \n","global step:253128, episode return:0.055032728305645276 \n","global step:254720, episode return:-0.22894102182085907 \n","global step:256312, episode return:0.14338697521840083 \n","global step:257904, episode return:0.04964534341433248 \n","global step:259496, episode return:0.02671684464147423 \n","global step:261088, episode return:0.10131789129885856 \n","global step:262680, episode return:-0.3212311105049649 \n","global step:264272, episode return:0.008049217441161221 \n","global step:265864, episode return:-0.09264409571799721 \n","global step:267456, episode return:-0.3633161145517201 \n","global step:269048, episode return:0.02210745161948995 \n","global step:270640, episode return:-0.023916819561539376 \n","global step:272232, episode return:-0.033865752337714244 \n","global step:273824, episode return:0.014107423597875018 \n","global step:275416, episode return:-0.07995247275851519 \n","global step:277008, episode return:0.03976371826577278 \n","global step:278600, episode return:0.021615740965443435 \n","global step:280192, episode return:-0.01925711709438539 \n","global step:281784, episode return:-0.15490166799711422 \n","global step:283376, episode return:-0.13162594829414664 \n","global step:284968, episode return:0.026751919065509673 \n","global step:286560, episode return:-0.09636268323161906 \n","global step:288152, episode return:-0.15483026918724793 \n","global step:289744, episode return:0.03517332439255569 \n","global step:291336, episode return:-0.0060242061930473835 \n","global step:292928, episode return:0.048855659313332356 \n","global step:294520, episode return:0.03525118805020719 \n","global step:296112, episode return:0.03121126645261798 \n","global step:297704, episode return:-0.058870934142943795 \n","global step:299296, episode return:0.048381204063041694 \n","global step:300888, episode return:-0.034268035155488236 \n","global step:302480, episode return:0.04428878898864245 \n","global step:304072, episode return:0.008404369166894484 \n","global step:305664, episode return:0.043335711436598014 \n","global step:307256, episode return:0.02304892658204179 \n","global step:308848, episode return:-0.1196652409810074 \n","global step:310440, episode return:0.07706203530035738 \n","global step:312032, episode return:-0.03563910093726678 \n","global step:313624, episode return:0.016580718333847062 \n","global step:315216, episode return:-0.14881517556306204 \n","global step:316808, episode return:0.043352268273356136 \n","global step:318400, episode return:0.09558304184174513 \n","global step:319992, episode return:0.029767933350416408 \n","global step:321584, episode return:-0.05585578214855283 \n","global step:323176, episode return:-0.11263188238547685 \n","global step:324768, episode return:-0.17231165280184663 \n","global step:326360, episode return:0.025515978464801338 \n","global step:327952, episode return:0.00111610688994792 \n","global step:329544, episode return:0.12182230599535038 \n","global step:331136, episode return:0.10325563535926167 \n","global step:332728, episode return:0.014611652412717484 \n","global step:334320, episode return:-0.010016838028169512 \n","global step:335912, episode return:0.05003533939191279 \n","global step:337504, episode return:0.03674483838747208 \n","global step:339096, episode return:0.03697122749300548 \n","global step:340688, episode return:0.04895516301653333 \n","global step:342280, episode return:0.03878356928058105 \n","global step:343872, episode return:-0.06515735592458405 \n","global step:345464, episode return:0.12067797919075131 \n","global step:347056, episode return:-0.08925515888557813 \n","global step:348648, episode return:-0.02678475972131514 \n","global step:350240, episode return:0.04597331076650549 \n","global step:351832, episode return:0.06210666731940978 \n","global step:353424, episode return:-0.03887158553178513 \n","global step:355016, episode return:0.042326079060731985 \n","global step:356608, episode return:0.09136160768650782 \n","global step:358200, episode return:-0.0002797251080452847 \n","global step:359792, episode return:-0.04134948684631381 \n","global step:361384, episode return:-0.06862881665328578 \n","global step:362976, episode return:-0.2298335760401156 \n","global step:364568, episode return:0.10582859323738415 \n","global step:366160, episode return:-0.13343406518440398 \n","global step:367752, episode return:-0.045884101685445725 \n","global step:369344, episode return:0.04535274556643963 \n","global step:370936, episode return:-0.6451502986085523 \n","global step:372528, episode return:0.05756507562120425 \n","global step:374120, episode return:0.1586478956308646 \n","global step:375712, episode return:-0.14421325343655395 \n","global step:377304, episode return:0.08942033783748686 \n","global step:378896, episode return:0.04306691824156738 \n","global step:380488, episode return:0.024947942088025647 \n","global step:382080, episode return:0.10297000548320626 \n","global step:383672, episode return:0.00917067521465822 \n","global step:385264, episode return:-0.17672846749025264 \n","global step:386856, episode return:0.12496777263050268 \n","global step:388448, episode return:-0.07539779006750923 \n","global step:390040, episode return:-0.0006220884287366892 \n","global step:391632, episode return:0.08690353841521956 \n","global step:393224, episode return:-0.10941554945573309 \n","global step:394816, episode return:-0.20229085654158113 \n","global step:396408, episode return:-0.014392704760338316 \n","global step:398000, episode return:-0.03242924396452327 \n","global step:399592, episode return:-0.19379102784760094 \n","global step:401184, episode return:0.10608730277536285 \n","global step:402776, episode return:0.033525873506787665 \n","global step:404368, episode return:0.09117268801410287 \n","global step:405960, episode return:0.038449090334123706 \n","global step:407552, episode return:0.060900508626444264 \n","global step:409144, episode return:-0.17329455841195807 \n","global step:410736, episode return:0.045768764238208665 \n","global step:412328, episode return:0.027816756557714324 \n","global step:413920, episode return:0.0013168033389791885 \n","global step:415512, episode return:-0.06353120129261851 \n","global step:417104, episode return:0.033158529957986425 \n","global step:418696, episode return:-0.013851464486841571 \n","global step:420288, episode return:0.05332042311384479 \n","global step:421880, episode return:0.04751061512868441 \n","global step:423472, episode return:-0.06179628797441887 \n","global step:425064, episode return:0.11711321029923455 \n","global step:426656, episode return:0.017675602401586152 \n","global step:428248, episode return:0.0087687808909594 \n","global step:429840, episode return:-0.024450728915691368 \n","global step:431432, episode return:-0.05832902085318985 \n","global step:433024, episode return:0.007353643839736182 \n","global step:434616, episode return:0.06275069944502469 \n","global step:436208, episode return:0.07994098608144953 \n","global step:437800, episode return:0.07302378273161558 \n","global step:439392, episode return:-0.012424187319448273 \n","global step:440984, episode return:0.006124860219642379 \n","global step:442576, episode return:0.08684121164445777 \n","global step:444168, episode return:-0.06438226416685397 \n","global step:445760, episode return:0.05620549108408717 \n","global step:447352, episode return:-0.1420624329702825 \n","global step:448944, episode return:0.08678873380673138 \n","global step:450536, episode return:0.1336845114888355 \n","global step:452128, episode return:0.02868024146104964 \n","global step:453720, episode return:-0.08867117819740619 \n","global step:455312, episode return:-0.34251138230218003 \n","global step:456904, episode return:-0.19089965506374107 \n","global step:458496, episode return:0.05328808768356703 \n","global step:460088, episode return:0.08142507843731366 \n","global step:461680, episode return:0.05609225528928464 \n","global step:463272, episode return:-0.10277119162000406 \n","global step:464864, episode return:0.045130316910845965 \n","global step:466456, episode return:-0.07038831194704366 \n","global step:468048, episode return:0.050446034115957994 \n","global step:469640, episode return:-0.3029127021863975 \n","global step:471232, episode return:-0.008164404066403792 \n","global step:472824, episode return:-0.02887781759939322 \n","global step:474416, episode return:-0.08053137731214942 \n","global step:476008, episode return:0.11927043724596384 \n","global step:477600, episode return:0.003849367989416235 \n","global step:479192, episode return:-0.010727503156180096 \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_utils.py:22: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n","  canvas.draw()\n"]},{"metadata":{"tags":null},"name":"stdout","output_type":"stream","text":["global step:480784, episode return:-0.10813810460721383 \n","global step:482376, episode return:0.08576739610485398 \n","global step:483968, episode return:0.09679178778058099 \n","global step:485560, episode return:0.01186141730095822 \n","global step:487152, episode return:-0.17428027738956942 \n","global step:488744, episode return:-0.14793476962458985 \n","global step:490336, episode return:0.04496498482882078 \n","global step:491928, episode return:-0.041767656024648415 \n","global step:493520, episode return:0.0007650885516951957 \n","global step:495112, episode return:0.0419175458726382 \n","global step:496704, episode return:0.02107416701738505 \n","global step:498296, episode return:-0.07405541002757407 \n","global step:499888, episode return:-0.02189802997573779 \n","global step:501480, episode return:0.05880227699686701 \n","global step:503072, episode return:-0.03357509744986907 \n","global step:504664, episode return:0.05532287790995407 \n","global step:506256, episode return:0.038294696810425535 \n","global step:507848, episode return:0.04820978608148389 \n","global step:509440, episode return:-0.08543702235820363 \n","global step:511032, episode return:0.03554835526315585 \n","global step:512624, episode return:0.1293526120077478 \n","global step:514216, episode return:-0.028330106215276724 \n","global step:515808, episode return:-0.12067205881389337 \n","global step:517400, episode return:0.05087152142028994 \n","global step:518992, episode return:-0.027142161780577354 \n","global step:520584, episode return:-0.28475969166907317 \n","global step:522176, episode return:-0.10134772711925663 \n","global step:523768, episode return:0.1334064168359745 \n","global step:525360, episode return:-0.02319502020549707 \n","global step:526952, episode return:-5.0035528820734304e-05 \n","global step:528544, episode return:0.046064450152893496 \n","global step:530136, episode return:0.07466840221695413 \n","global step:531728, episode return:-0.010250790615221075 \n","global step:533320, episode return:0.047791195107019006 \n","global step:534912, episode return:-0.33662164650035525 \n","global step:536504, episode return:-0.04315032555217824 \n","global step:538096, episode return:-0.169041282676212 \n","global step:539688, episode return:0.0606340086551121 \n","global step:541280, episode return:0.020468054075570796 \n","global step:542872, episode return:-0.008788504642072068 \n","global step:544464, episode return:0.019080344815624317 \n","global step:546056, episode return:0.0451990764122164 \n","global step:547648, episode return:0.012055056469098591 \n","global step:549240, episode return:0.07428895703531664 \n","global step:550832, episode return:-0.015840511997912133 \n","global step:552424, episode return:0.06590546253041563 \n","global step:554016, episode return:0.02508320185159398 \n","global step:555608, episode return:-0.00898288814223033 \n","global step:557200, episode return:-0.3551708714296606 \n","global step:558792, episode return:0.006655875341749157 \n","global step:560384, episode return:-0.0471221391997655 \n","global step:561976, episode return:-0.020449700615760163 \n","global step:563568, episode return:-0.008251498559599876 \n","global step:565160, episode return:0.1132655562824419 \n","global step:566752, episode return:0.00520400804731882 \n","global step:568344, episode return:-0.21065152512064725 \n","global step:569936, episode return:-0.0011968087385821951 \n","global step:571528, episode return:-0.025559670610889373 \n","global step:573120, episode return:0.06942374990103925 \n","global step:574712, episode return:-0.17109714502763904 \n","global step:576304, episode return:-0.056957006960100226 \n","global step:577896, episode return:-0.09190637024483071 \n","global step:579488, episode return:-0.023518866198089183 \n","global step:581080, episode return:0.02414022232965574 \n","global step:582672, episode return:-0.13381656037814718 \n","global step:584264, episode return:-0.0322820666188025 \n","global step:585856, episode return:-0.048109349737916966 \n","global step:587448, episode return:0.04238917862473441 \n","global step:589040, episode return:-0.37154440846201664 \n","global step:590632, episode return:0.02463481211382682 \n","global step:592224, episode return:-0.02516189546912787 \n","global step:593816, episode return:-0.002220551982433433 \n","global step:595408, episode return:-0.0926818041052222 \n","global step:597000, episode return:0.043782971712266215 \n","global step:598592, episode return:0.09587863728700417 \n","global step:600184, episode return:-0.09831918963740202 \n","global step:601776, episode return:-0.018381242640337137 \n","global step:603368, episode return:-0.022982751250572662 \n","global step:604960, episode return:0.09979838169064012 \n","global step:606552, episode return:0.07717063673619204 \n","global step:608144, episode return:-0.01952229545996767 \n","global step:609736, episode return:-0.2673253875992725 \n","global step:611328, episode return:0.004783434802998657 \n","global step:612920, episode return:-0.007742780153290181 \n","global step:614512, episode return:-0.49672863903710424 \n","global step:616104, episode return:0.028773103103812887 \n","global step:617696, episode return:0.05231316912752931 \n","global step:619288, episode return:-0.005991386357211534 \n","global step:620880, episode return:-0.0010326512534726958 \n","global step:622472, episode return:-0.40886092328802964 \n","global step:624064, episode return:-0.20551979466866038 \n","global step:625656, episode return:0.06957585647485033 \n","global step:627248, episode return:0.03563925108383047 \n","global step:628840, episode return:0.06454328750325013 \n","global step:630432, episode return:-0.013633794090501031 \n","global step:632024, episode return:-0.02016116599415165 \n","global step:633616, episode return:-0.005827963171017537 \n","global step:635208, episode return:0.03017382897584811 \n","global step:636800, episode return:-0.13241486427318527 \n","global step:638392, episode return:-0.022039186667780996 \n","global step:639984, episode return:0.11817401860999716 \n","global step:641576, episode return:-0.035679785774193666 \n","global step:643168, episode return:0.036168996203241675 \n","global step:644760, episode return:-0.025375080970506676 \n","global step:646352, episode return:-0.005187397708713141 \n","global step:647944, episode return:-0.013633972675244127 \n","global step:649536, episode return:-0.03058619382060396 \n","global step:651128, episode return:0.017607898919120943 \n","global step:652720, episode return:-0.18566839881102745 \n","global step:654312, episode return:-0.10575024398528374 \n","global step:655904, episode return:0.027041744719708192 \n","global step:657496, episode return:0.06764647797632412 \n","global step:659088, episode return:0.09259698411536277 \n","global step:660680, episode return:0.054652740399051505 \n","global step:662272, episode return:0.021213770010232384 \n","global step:663864, episode return:-0.07234273669801698 \n","global step:665456, episode return:-0.046709719018300636 \n","global step:667048, episode return:0.02750078927080549 \n","global step:668640, episode return:-0.10475496122468887 \n","global step:670232, episode return:-0.14109930502291587 \n","global step:671824, episode return:-0.189064138960646 \n","global step:673416, episode return:0.020904234043987075 \n","global step:675008, episode return:-0.014271622893508177 \n","global step:676600, episode return:0.23362566185252398 \n","global step:678192, episode return:0.04605258860879913 \n","global step:679784, episode return:0.09776494157192907 \n","global step:681376, episode return:-0.13848422708640243 \n","global step:682968, episode return:0.010030507115854603 \n","global step:684560, episode return:0.012488465013434327 \n","global step:686152, episode return:-0.025323825579983777 \n","global step:687744, episode return:-0.02701202083187791 \n","global step:689336, episode return:-0.26682990807329815 \n","global step:690928, episode return:-0.3153794826493924 \n","global step:692520, episode return:-0.10172564774607663 \n","global step:694112, episode return:0.09570939030036829 \n","global step:695704, episode return:0.04230375205266454 \n","global step:697296, episode return:-0.015183723276330607 \n","global step:698888, episode return:0.03688662714588821 \n","global step:700480, episode return:-0.09515893557964103 \n","global step:702072, episode return:-0.0740677495463822 \n","global step:703664, episode return:-0.2781460057369145 \n","global step:705256, episode return:-0.1268001289013283 \n","global step:706848, episode return:0.07703743518767657 \n","global step:708440, episode return:-0.15428568421393335 \n","global step:710032, episode return:0.053424713157799474 \n","global step:711624, episode return:0.022632262332958446 \n","global step:713216, episode return:-0.1702881119028471 \n","global step:714808, episode return:0.07516802680204906 \n","global step:716400, episode return:0.06094740676155808 \n","global step:717992, episode return:0.13543404511558663 \n","global step:719584, episode return:-0.007916851732515398 \n","global step:721176, episode return:0.06013634512734827 \n","global step:722768, episode return:0.06522848324812369 \n","global step:724360, episode return:-0.017683117360663237 \n","global step:725952, episode return:0.1069057332125652 \n","global step:727544, episode return:-0.0778165280341611 \n","global step:729136, episode return:0.0056209521723100155 \n","global step:730728, episode return:0.0033747026182032003 \n","global step:732320, episode return:0.03138842123087605 \n","global step:733912, episode return:0.11926711616698098 \n","global step:735504, episode return:-0.1723279784223615 \n","global step:737096, episode return:0.15966732167254277 \n","global step:738688, episode return:0.08975166946959318 \n","global step:740280, episode return:0.05807330784092839 \n","global step:741872, episode return:-0.09758459050537607 \n","global step:743464, episode return:0.04832461756281065 \n","global step:745056, episode return:0.06837481990417894 \n","global step:746648, episode return:0.03141231463135678 \n","global step:748240, episode return:-0.0477591370571033 \n","global step:749832, episode return:0.04465492232107386 \n","global step:751424, episode return:0.05359638710538162 \n","global step:753016, episode return:0.032606373443482294 \n","global step:754608, episode return:0.000506144537166383 \n","global step:756200, episode return:0.05142593251185321 \n","global step:757792, episode return:0.017602062909587185 \n","global step:759384, episode return:0.03343909063835433 \n","global step:760976, episode return:-0.03408510104448504 \n","global step:762568, episode return:0.049049029524957655 \n","global step:764160, episode return:0.05157982883270647 \n","global step:765752, episode return:-0.06780975169732541 \n","global step:767344, episode return:-0.13021910811916707 \n","global step:768936, episode return:0.039034569851874666 \n","global step:770528, episode return:0.11918547579652593 \n","global step:772120, episode return:0.07909087020991727 \n","global step:773712, episode return:0.02857766379341986 \n","global step:775304, episode return:0.01676547401791389 \n","global step:776896, episode return:-0.38797308624525334 \n","global step:778488, episode return:0.08402929421980307 \n","global step:780080, episode return:-0.07552172922587759 \n","global step:781672, episode return:-0.11044137156899074 \n","global step:783264, episode return:-0.17193683315092484 \n","global step:784856, episode return:-0.03394925499497432 \n","global step:786448, episode return:0.012507907254355388 \n","global step:788040, episode return:0.07978942926864707 \n","global step:789632, episode return:0.02319509794773563 \n","global step:791224, episode return:-0.024835928988784013 \n","global step:792816, episode return:-0.17048302888835742 \n","global step:794408, episode return:0.03810878191339088 \n","global step:796000, episode return:0.26115523334490254 \n","global step:797592, episode return:0.027791293188157935 \n","global step:799184, episode return:-0.029596067558736512 \n","global step:800776, episode return:0.031154058560571622 \n","global step:802368, episode return:-0.07355012943902377 \n","global step:803960, episode return:-0.008773606926716854 \n","global step:805552, episode return:0.011621810460909801 \n","global step:807144, episode return:-0.03828880777231816 \n","global step:808736, episode return:0.0364685142874851 \n","global step:810328, episode return:0.04085057360703934 \n","global step:811920, episode return:0.0881946357249426 \n","global step:813512, episode return:-0.003590990022542413 \n","global step:815104, episode return:0.07012706319332537 \n","global step:816696, episode return:0.059017543361994454 \n","global step:818288, episode return:0.003430937290752748 \n","global step:819880, episode return:-0.017318774956589688 \n","global step:821472, episode return:-0.22490644399719767 \n","global step:823064, episode return:-0.005875295085190396 \n","global step:824656, episode return:-0.03072882455561075 \n","global step:826248, episode return:0.011904072941127003 \n","global step:827840, episode return:0.12066595939144399 \n","global step:829432, episode return:-0.04092735627043122 \n","global step:831024, episode return:-0.02736555144544868 \n","global step:832616, episode return:-0.002648460019733581 \n","global step:834208, episode return:0.061775222984547074 \n","global step:835800, episode return:0.05531366565101666 \n","global step:837392, episode return:-0.08338484671631732 \n","global step:838984, episode return:0.02386757737279744 \n"]},{"metadata":{"tags":null},"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.7/dist-packages/torch/utils/tensorboard/_utils.py:22: UserWarning: Creating legend with loc=\"best\" can be slow with large amounts of data.\n","  canvas.draw()\n"]},{"output_type":"stream","name":"stdout","text":["global step:840576, episode return:0.07367443898983514 \n","global step:842168, episode return:-0.026329668802076957 \n","global step:843760, episode return:0.027780466616643542 \n","global step:845352, episode return:-0.024597461337907887 \n","global step:846944, episode return:0.357491577287806 \n","global step:848536, episode return:0.07901240125632669 \n","global step:850128, episode return:0.05977735694729876 \n","global step:851720, episode return:0.03526775702163389 \n","global step:853312, episode return:0.12059211077120431 \n","global step:854904, episode return:-0.07089934335960145 \n","global step:856496, episode return:0.0072913588862760755 \n","global step:858088, episode return:-0.009719384984320373 \n","global step:859680, episode return:0.07255477144708689 \n","global step:861272, episode return:0.0669112842803711 \n","global step:862864, episode return:-0.1308897893101882 \n","global step:864456, episode return:0.08659756108664779 \n","global step:866048, episode return:0.002693717229197029 \n","global step:867640, episode return:-0.03524255802198899 \n","global step:869232, episode return:0.08059436711340175 \n","global step:870824, episode return:0.2935152537706046 \n","global step:872416, episode return:-0.009472068815904603 \n","global step:874008, episode return:0.04352326755918443 \n","global step:875600, episode return:-0.006501508178286016 \n","global step:877192, episode return:0.14269246791928258 \n","global step:878784, episode return:0.04674822982598578 \n","global step:880376, episode return:0.09332866250590631 \n","global step:881968, episode return:0.26988124269526004 \n","global step:883560, episode return:0.05445943007920212 \n","global step:885152, episode return:-0.0032720210234428574 \n","global step:886744, episode return:0.31099348647006325 \n","global step:888336, episode return:0.13072999207176406 \n","global step:889928, episode return:0.13947194514169123 \n","global step:891520, episode return:0.08004407467555483 \n","global step:893112, episode return:-0.2023330927899316 \n","global step:894704, episode return:0.019500167691471643 \n","global step:896296, episode return:0.06485476369655183 \n","global step:897888, episode return:0.06072537902778132 \n","global step:899480, episode return:-0.07206884350525433 \n","global step:901072, episode return:0.09321296245223624 \n","global step:902664, episode return:-0.1627273181274743 \n","global step:904256, episode return:0.11799693904277193 \n","global step:905848, episode return:-0.004076968789900031 \n","global step:907440, episode return:0.05492092460945841 \n","global step:909032, episode return:0.09523025027744082 \n","global step:910624, episode return:-0.06951833641907362 \n","global step:912216, episode return:-0.02388777146140869 \n","global step:913808, episode return:0.21252167609472689 \n","global step:915400, episode return:-0.06226578856627855 \n","global step:916992, episode return:0.07841541814944174 \n","global step:918584, episode return:0.04687259118137401 \n","global step:920176, episode return:0.09177867311243415 \n","global step:921768, episode return:0.09850503276491947 \n","global step:923360, episode return:-0.021809049799220524 \n","global step:924952, episode return:0.02747626825686227 \n","global step:926544, episode return:0.012461327051578389 \n","global step:928136, episode return:0.02481604925471309 \n","global step:929728, episode return:-0.06285917431251888 \n","global step:931320, episode return:0.18818370351302124 \n","global step:932912, episode return:0.03350733191611024 \n","global step:934504, episode return:0.10168336175144009 \n","global step:936096, episode return:0.059314886663405515 \n","global step:937688, episode return:0.06100008474625652 \n","global step:939280, episode return:0.20218582477060404 \n","global step:940872, episode return:0.025876097337507172 \n","global step:942464, episode return:0.05098546466785949 \n","global step:944056, episode return:0.10892050076709697 \n","global step:945648, episode return:-0.0384401778371903 \n","global step:947240, episode return:0.03119643024956809 \n","global step:948832, episode return:-0.010273284824111035 \n","global step:950424, episode return:0.04562103755257875 \n","global step:952016, episode return:0.12944312641933967 \n","global step:953608, episode return:0.03450915288075253 \n","global step:955200, episode return:0.018539903482231435 \n","global step:956792, episode return:0.05199580410208777 \n","global step:958384, episode return:-0.01421493537300994 \n","global step:959976, episode return:0.05701592481133628 \n","global step:961568, episode return:0.02002658255393376 \n","global step:963160, episode return:-0.0227930365574803 \n","global step:964752, episode return:0.045738780713210095 \n","global step:966344, episode return:0.04594357672524327 \n","global step:967936, episode return:0.10316461430610498 \n","global step:969528, episode return:0.049821106238769314 \n","global step:971120, episode return:0.012174488729548698 \n","global step:972712, episode return:-0.03718333970348893 \n","global step:974304, episode return:0.05661419298878411 \n","global step:975896, episode return:0.18332507746948676 \n","global step:977488, episode return:0.04624911855242592 \n","global step:979080, episode return:-0.009808588157882244 \n","global step:980672, episode return:0.029217956732098625 \n","global step:982264, episode return:-0.05018751969344269 \n","global step:983856, episode return:-0.09814762366150129 \n","global step:985448, episode return:0.3625735530391838 \n","global step:987040, episode return:0.05003255946993808 \n","global step:988632, episode return:0.04925183658242133 \n","global step:990224, episode return:0.07260334706944006 \n","global step:991816, episode return:0.0522993669735042 \n","global step:993408, episode return:-0.12657561287427999 \n","global step:995000, episode return:0.0691864404396539 \n","global step:996592, episode return:0.1876001997825375 \n","global step:998184, episode return:0.026235567360038913 \n","global step:999776, episode return:0.10050863164405394 \n","global step:1001368, episode return:0.00857828571877148 \n","global step:1002960, episode return:0.07742813279911942 \n","global step:1004552, episode return:0.053077661946852116 \n","global step:1006144, episode return:-0.023936426152533923 \n","global step:1007736, episode return:0.08143836759165565 \n","global step:1009328, episode return:0.18616738773194255 \n","global step:1010920, episode return:0.045091612636043665 \n","global step:1012512, episode return:0.04312529362853101 \n","global step:1014104, episode return:0.04376247830867599 \n","global step:1015696, episode return:0.02074336243129525 \n","global step:1017288, episode return:0.024047213425074494 \n","global step:1018880, episode return:0.017848375345462096 \n","global step:1020472, episode return:0.39197989262270627 \n","global step:1022064, episode return:0.23981164027174073 \n","global step:1023656, episode return:0.036093690173326747 \n","global step:1025248, episode return:0.03491042671239523 \n","global step:1026840, episode return:0.06558661083805256 \n","global step:1028432, episode return:0.07815677514117662 \n","global step:1030024, episode return:0.057895999191797165 \n","global step:1031616, episode return:0.0769178189515749 \n","global step:1033208, episode return:0.03834762378408932 \n","global step:1034800, episode return:0.06653889252850034 \n","global step:1036392, episode return:0.08782414788677184 \n","global step:1037984, episode return:0.03487000211936353 \n","global step:1039576, episode return:0.07966974520692013 \n","global step:1041168, episode return:0.08787840079237577 \n","global step:1042760, episode return:0.03729028138815039 \n","global step:1044352, episode return:-0.02630813513676361 \n","global step:1045944, episode return:0.0494179158198566 \n","global step:1047536, episode return:0.018330095840488978 \n","global step:1049128, episode return:0.32335825950384556 \n","global step:1050720, episode return:0.07557462047366781 \n","global step:1052312, episode return:0.05156013619248587 \n","global step:1053904, episode return:0.017244642642231904 \n","global step:1055496, episode return:0.017471794780206565 \n","global step:1057088, episode return:0.036037352543876584 \n","global step:1058680, episode return:0.02788977240618333 \n","global step:1060272, episode return:0.04445327125432223 \n","global step:1061864, episode return:0.02973947776436446 \n","global step:1063456, episode return:0.0542683203661106 \n","global step:1065048, episode return:0.09975261981261911 \n","global step:1066640, episode return:0.1471523472991154 \n","global step:1068232, episode return:0.2910162935424392 \n","global step:1069824, episode return:0.1185233911376834 \n","global step:1071416, episode return:0.07233221903639145 \n","global step:1073008, episode return:0.12186834530634168 \n","global step:1074600, episode return:0.013467291778419713 \n","global step:1076192, episode return:0.15656755004399536 \n","global step:1077784, episode return:0.03304370356362145 \n","global step:1079376, episode return:0.09541409151436608 \n","global step:1080968, episode return:0.13458972013669457 \n","global step:1082560, episode return:0.09435263731272732 \n","global step:1084152, episode return:0.09438134277084292 \n","global step:1085744, episode return:0.12879057351670084 \n","global step:1087336, episode return:0.04363894804345404 \n","global step:1088928, episode return:0.1611580206691499 \n","global step:1090520, episode return:0.03749339605082298 \n","global step:1092112, episode return:0.14703507130802046 \n","global step:1093704, episode return:0.13506460766263748 \n","global step:1095296, episode return:0.09254016678125351 \n","global step:1096888, episode return:0.09817822666341754 \n","global step:1098480, episode return:0.05933930424643225 \n","global step:1100072, episode return:0.1661196179361462 \n","global step:1101664, episode return:0.38975529165987266 \n","global step:1103256, episode return:0.08937557894218627 \n","global step:1104848, episode return:0.07479000457705247 \n","global step:1106440, episode return:0.10950480233460234 \n","global step:1108032, episode return:0.042582734322557214 \n","global step:1109624, episode return:0.13518739878481817 \n","global step:1111216, episode return:0.0668148209041323 \n","global step:1112808, episode return:0.10495459438943568 \n","global step:1114400, episode return:0.11084340923724995 \n","global step:1115992, episode return:0.07625016742126643 \n","global step:1117584, episode return:0.0546020505564323 \n","global step:1119176, episode return:0.048598911801228085 \n","global step:1120768, episode return:0.13430464008037438 \n","global step:1122360, episode return:0.08412043169413826 \n","global step:1123952, episode return:0.18063592549378918 \n","global step:1125544, episode return:0.33382645212050277 \n","global step:1127136, episode return:0.08181632360540367 \n","global step:1128728, episode return:0.1796404377215763 \n","global step:1130320, episode return:0.12773924226452632 \n","global step:1131912, episode return:0.08891807201502959 \n","global step:1133504, episode return:0.3077061653823955 \n","global step:1135096, episode return:0.0695555660553118 \n","global step:1136688, episode return:0.08129371602355594 \n","global step:1138280, episode return:0.12039618867607521 \n","global step:1139872, episode return:0.12059512426329194 \n","global step:1141464, episode return:0.13776845431944382 \n","global step:1143056, episode return:0.029812692217826585 \n","global step:1144648, episode return:0.09971242514386922 \n","global step:1146240, episode return:0.0380355420512632 \n","global step:1147832, episode return:0.16484414108277426 \n","global step:1149424, episode return:0.08561165046724258 \n","global step:1151016, episode return:0.3232271063682946 \n","global step:1152608, episode return:0.08428971009911262 \n","global step:1154200, episode return:0.03648155820472194 \n","global step:1155792, episode return:0.14471901295380082 \n","global step:1157384, episode return:0.08182558161553889 \n","global step:1158976, episode return:0.0860490776894957 \n","global step:1160568, episode return:-0.014086046494585369 \n","global step:1162160, episode return:0.08939581687269643 \n","global step:1163752, episode return:0.08110299416748713 \n","global step:1165344, episode return:0.0866148543848262 \n","global step:1166936, episode return:0.12208571487924452 \n","global step:1168528, episode return:-0.002262378728419815 \n","global step:1170120, episode return:0.11744443898419092 \n","global step:1171712, episode return:0.19263075548299946 \n","global step:1173304, episode return:0.0908794964641958 \n","global step:1174896, episode return:-0.007050646692838817 \n","global step:1176488, episode return:0.07394907496775263 \n","global step:1178080, episode return:0.02294131526061623 \n","global step:1179672, episode return:0.4694128578291586 \n","global step:1181264, episode return:0.09458569203606225 \n","global step:1182856, episode return:0.15917330672052876 \n","global step:1184448, episode return:0.025576459279825468 \n","global step:1186040, episode return:0.0998738323002524 \n","global step:1187632, episode return:0.3576992984810131 \n","global step:1189224, episode return:0.05504874051821563 \n","global step:1190816, episode return:0.12535406978733274 \n","global step:1192408, episode return:0.06949629873499684 \n","global step:1194000, episode return:0.08169697287359685 \n","global step:1195592, episode return:0.19207101927087675 \n","global step:1197184, episode return:0.10669791390447464 \n","global step:1198776, episode return:0.12850838664547765 \n","global step:1200368, episode return:0.10059602268026152 \n","global step:1201960, episode return:0.06351976701638273 \n","global step:1203552, episode return:0.05246718549104857 \n","global step:1205144, episode return:0.2170459001034476 \n","global step:1206736, episode return:0.13604472392598332 \n","global step:1208328, episode return:0.16334886957745134 \n","global step:1209920, episode return:0.07994357339795005 \n","global step:1211512, episode return:0.0882261465929247 \n","global step:1213104, episode return:0.05280545768891528 \n","global step:1214696, episode return:0.03551637608624673 \n","global step:1216288, episode return:0.09832144058850686 \n","global step:1217880, episode return:0.13602874191803893 \n","global step:1219472, episode return:0.13095844467742862 \n","global step:1221064, episode return:0.19988632386619154 \n","global step:1222656, episode return:0.03788904536657743 \n","global step:1224248, episode return:0.024483399931653053 \n","global step:1225840, episode return:0.5194149424604241 \n","global step:1227432, episode return:0.05810515076710588 \n","global step:1229024, episode return:0.11416226124451484 \n","global step:1230616, episode return:-0.0012924560503240739 \n","global step:1232208, episode return:0.08240519807852272 \n","global step:1233800, episode return:0.11372659872529622 \n","global step:1235392, episode return:0.2040178774119065 \n","global step:1236984, episode return:0.14208599673269784 \n","global step:1238576, episode return:0.05764915180621703 \n","global step:1240168, episode return:0.12316811318649916 \n","global step:1241760, episode return:0.1394342582937958 \n","global step:1243352, episode return:0.10744342096057381 \n","global step:1244944, episode return:0.08821593076675109 \n","global step:1246536, episode return:0.1358376843413018 \n","global step:1248128, episode return:0.10928418859754371 \n","global step:1249720, episode return:0.16505863339244675 \n","global step:1251312, episode return:0.07600859105634458 \n","global step:1252904, episode return:0.07669769371805242 \n","global step:1254496, episode return:0.07225007870164595 \n","global step:1256088, episode return:0.0831989578666959 \n","global step:1257680, episode return:0.0506187248188538 \n","global step:1259272, episode return:0.04438093365537971 \n","global step:1260864, episode return:0.06280795026535174 \n","global step:1262456, episode return:0.09489205214767427 \n","global step:1264048, episode return:0.059091940030966295 \n","global step:1265640, episode return:0.10180075259193093 \n","global step:1267232, episode return:0.1553496302590926 \n","global step:1268824, episode return:0.19098333031926937 \n","global step:1270416, episode return:0.0913809739241852 \n","global step:1272008, episode return:0.010919580190836593 \n","global step:1273600, episode return:0.11356990660666043 \n","global step:1275192, episode return:0.14375588865127623 \n","global step:1276784, episode return:0.13913021568492978 \n","global step:1278376, episode return:0.02286688939372262 \n","global step:1279968, episode return:0.08451808452472648 \n","global step:1281560, episode return:0.17612731341177204 \n","global step:1283152, episode return:0.49021551360822174 \n","global step:1284744, episode return:0.1919003868556722 \n","global step:1286336, episode return:0.18349047233964463 \n","global step:1287928, episode return:0.206046054503314 \n","global step:1289520, episode return:0.051963286474264445 \n","global step:1291112, episode return:0.04171776841111374 \n","global step:1292704, episode return:0.14966581350949412 \n","global step:1294296, episode return:0.14580149511854587 \n","global step:1295888, episode return:0.17536031511932043 \n","global step:1297480, episode return:-0.02485402757644775 \n","global step:1299072, episode return:-0.046947194068436615 \n","global step:1300664, episode return:0.06715525875917726 \n","global step:1302256, episode return:0.0744237528804338 \n","global step:1303848, episode return:0.17525477253185462 \n","global step:1305440, episode return:0.08245499667207266 \n","global step:1307032, episode return:0.20069812172152626 \n","global step:1308624, episode return:0.04438665225163603 \n","global step:1310216, episode return:0.03658411111932983 \n","global step:1311808, episode return:0.04544769531835682 \n","global step:1313400, episode return:0.22371364343028494 \n","global step:1314992, episode return:0.03300033815656742 \n","global step:1316584, episode return:0.13304926522528882 \n","global step:1318176, episode return:0.18421039478498458 \n","global step:1319768, episode return:0.21511141379133847 \n","global step:1321360, episode return:0.06811682985237974 \n","global step:1322952, episode return:0.07940352145578204 \n","global step:1324544, episode return:0.12461460099633012 \n","global step:1326136, episode return:0.06020523576168208 \n","global step:1327728, episode return:0.0776655716751658 \n","global step:1329320, episode return:0.10432927866785963 \n","global step:1330912, episode return:0.10995426186022224 \n","global step:1332504, episode return:0.15007139513480358 \n","global step:1334096, episode return:0.11938943856905518 \n","global step:1335688, episode return:0.06160197736197819 \n","global step:1337280, episode return:0.08413474168580806 \n","global step:1338872, episode return:0.05914748035946575 \n","global step:1340464, episode return:0.04527221678064727 \n","global step:1342056, episode return:0.10559790976404071 \n","global step:1343648, episode return:0.14136608993765418 \n","global step:1345240, episode return:0.164873803009588 \n","global step:1346832, episode return:0.45802632061195375 \n","global step:1348424, episode return:0.17217135834535263 \n","global step:1350016, episode return:0.06212387100141285 \n","global step:1351608, episode return:0.1413427567106893 \n","global step:1353200, episode return:0.07370485673240217 \n","global step:1354792, episode return:0.0826997522136671 \n","global step:1356384, episode return:0.1320741668201914 \n","global step:1357976, episode return:-0.027889396573238145 \n","global step:1359568, episode return:0.02928599042636261 \n","global step:1361160, episode return:0.0596717263658172 \n","global step:1362752, episode return:0.06129572311167006 \n","global step:1364344, episode return:0.10799445695269329 \n","global step:1365936, episode return:0.08211138985859655 \n","global step:1367528, episode return:0.16995332240118682 \n","global step:1369120, episode return:0.15704107012800955 \n","global step:1370712, episode return:0.20033000518395586 \n","global step:1372304, episode return:0.06411723977346838 \n","global step:1373896, episode return:0.07561032216485787 \n","global step:1375488, episode return:0.04966318121877146 \n","global step:1377080, episode return:0.0028068727616087476 \n","global step:1378672, episode return:0.17473741714077154 \n","global step:1380264, episode return:0.1104121676607758 \n","global step:1381856, episode return:0.0851953703163293 \n","global step:1383448, episode return:0.017545072851674098 \n","global step:1385040, episode return:0.13456806353959463 \n","global step:1386632, episode return:0.17308229800659994 \n","global step:1388224, episode return:0.10233690852814781 \n","global step:1389816, episode return:0.04145318232158359 \n","global step:1391408, episode return:0.06908148917511665 \n","global step:1393000, episode return:0.13185297056768164 \n","global step:1394592, episode return:0.03376266013534344 \n","global step:1396184, episode return:0.15434971199454406 \n","global step:1397776, episode return:0.1770912374965678 \n","global step:1399368, episode return:0.021061012753644474 \n","global step:1400960, episode return:0.08983766214700267 \n","global step:1402552, episode return:0.10644597742114603 \n","global step:1404144, episode return:0.10363069340425433 \n","global step:1405736, episode return:0.3720431539377273 \n","global step:1407328, episode return:0.10956001485608 \n","global step:1408920, episode return:0.15781069415319796 \n","global step:1410512, episode return:0.09681800244303113 \n","global step:1412104, episode return:0.03291272060587032 \n","global step:1413696, episode return:0.10849358213765962 \n","global step:1415288, episode return:0.14199928595436126 \n","global step:1416880, episode return:0.14402540873348904 \n","global step:1418472, episode return:0.0949887931416439 \n","global step:1420064, episode return:0.13769947693986764 \n","global step:1421656, episode return:0.09740011655742296 \n","global step:1423248, episode return:0.22513746418213657 \n","global step:1424840, episode return:0.16571543619665108 \n","global step:1426432, episode return:0.17034106029611001 \n","global step:1428024, episode return:0.12210662352640539 \n","global step:1429616, episode return:0.08665212086627462 \n","global step:1431208, episode return:0.0613496274823541 \n","global step:1432800, episode return:0.10526364099858879 \n","global step:1434392, episode return:0.06366406341817887 \n","global step:1435984, episode return:0.13628389311786218 \n","global step:1437576, episode return:0.16020200967273468 \n","global step:1439168, episode return:0.10393211212290847 \n","global step:1440760, episode return:0.11144305328954329 \n","global step:1442352, episode return:0.22542242597130474 \n","global step:1443944, episode return:0.014517081903367389 \n","global step:1445536, episode return:0.14249503884787165 \n","global step:1447128, episode return:0.19482181789646036 \n","global step:1448720, episode return:0.0737229199389451 \n","global step:1450312, episode return:0.16365198082061794 \n","global step:1451904, episode return:0.09561319014187702 \n","global step:1453496, episode return:0.10372602780496637 \n","global step:1455088, episode return:0.14680624214068494 \n","global step:1456680, episode return:0.047184505139732796 \n","global step:1458272, episode return:0.12320707906614198 \n","global step:1459864, episode return:0.09800171174668124 \n","global step:1461456, episode return:0.136451554737129 \n","global step:1463048, episode return:0.0980277778928953 \n","global step:1464640, episode return:0.23721514422840262 \n","global step:1466232, episode return:0.06347423246595242 \n","global step:1467824, episode return:0.10211040094488154 \n","global step:1469416, episode return:0.1071529474582184 \n","global step:1471008, episode return:0.5688420626068701 \n","global step:1472600, episode return:0.1975938094457267 \n","global step:1474192, episode return:0.07689723963803258 \n","global step:1475784, episode return:0.1485010857837327 \n","global step:1477376, episode return:0.04347348281526172 \n","global step:1478968, episode return:0.22260597436086052 \n","global step:1480560, episode return:0.1765167914028584 \n","global step:1482152, episode return:0.08144702038269407 \n","global step:1483744, episode return:0.23497170663775638 \n","global step:1485336, episode return:0.14024200387578653 \n","global step:1486928, episode return:0.0385546234945318 \n","global step:1488520, episode return:0.176449680382189 \n","global step:1490112, episode return:0.06260182464398045 \n","global step:1491704, episode return:0.21185395860133233 \n","global step:1493296, episode return:0.11930268805398245 \n","global step:1494888, episode return:0.1873222145167303 \n","global step:1496480, episode return:0.08253284989908731 \n","global step:1498072, episode return:0.0706550485473144 \n","global step:1499664, episode return:0.010429212940504376 \n","global step:1501256, episode return:0.10196020061969778 \n","global step:1502848, episode return:0.02685836123117975 \n","global step:1504440, episode return:0.07969057589267649 \n","global step:1506032, episode return:0.11142227565017196 \n","global step:1507624, episode return:0.15976726152113163 \n","global step:1509216, episode return:0.15323314390537346 \n","global step:1510808, episode return:0.07931473824167304 \n","global step:1512400, episode return:0.10246268593341996 \n","global step:1513992, episode return:0.20006087080936225 \n","global step:1515584, episode return:0.1612284685847066 \n","global step:1517176, episode return:0.1611129072993742 \n","global step:1518768, episode return:0.09505647250348659 \n","global step:1520360, episode return:0.16700547103333516 \n","global step:1521952, episode return:0.05297157272095877 \n","global step:1523544, episode return:0.07218589431189633 \n","global step:1525136, episode return:0.12775057383671018 \n","global step:1526728, episode return:0.13392123040751047 \n","global step:1528320, episode return:0.11126218864835838 \n","global step:1529912, episode return:0.13794351196146978 \n","global step:1531504, episode return:0.1497639886195883 \n","global step:1533096, episode return:0.07685767629361454 \n","global step:1534688, episode return:0.09915001133240159 \n","global step:1536280, episode return:0.09958776031061933 \n","global step:1537872, episode return:0.05988924460838717 \n","global step:1539464, episode return:0.11030269808577747 \n","global step:1541056, episode return:0.14874346684628917 \n","global step:1542648, episode return:0.14452541264575383 \n","global step:1544240, episode return:0.07821553232675729 \n","global step:1545832, episode return:0.16157036636741784 \n","global step:1547424, episode return:0.11354711609162114 \n","global step:1549016, episode return:0.04307687111461414 \n","global step:1550608, episode return:0.1868308381706335 \n","global step:1552200, episode return:0.2608437048729232 \n","global step:1553792, episode return:0.10351180551715991 \n","global step:1555384, episode return:0.073293077090702 \n","global step:1556976, episode return:0.5752416332506168 \n","global step:1558568, episode return:0.134303638756239 \n","global step:1560160, episode return:0.10411196026852228 \n","global step:1561752, episode return:0.06601072921291115 \n","global step:1563344, episode return:0.38438671935286145 \n","global step:1564936, episode return:0.20258440381716078 \n","global step:1566528, episode return:0.20248804032818885 \n","global step:1568120, episode return:0.44442968545119516 \n","global step:1569712, episode return:0.11503767571249163 \n","global step:1571304, episode return:0.16376942187163027 \n","global step:1572896, episode return:0.11891378342593915 \n","global step:1574488, episode return:0.12336688096502159 \n","global step:1576080, episode return:0.10981081913470335 \n","global step:1577672, episode return:0.027044690796690075 \n","global step:1579264, episode return:0.08516810069714266 \n","global step:1580856, episode return:0.08077060863425674 \n","global step:1582448, episode return:0.06126143361421725 \n","global step:1584040, episode return:0.11823386699745235 \n","global step:1585632, episode return:0.16525618799404984 \n","global step:1587224, episode return:0.17085619151284498 \n","global step:1588816, episode return:0.0882533193237325 \n","global step:1590408, episode return:0.17781928220853052 \n","global step:1592000, episode return:0.1080403082135199 \n","global step:1593592, episode return:0.28923716316225545 \n","global step:1595184, episode return:0.12649765064645102 \n","global step:1596776, episode return:-0.009611489070198282 \n","global step:1598368, episode return:0.16434834669033102 \n","global step:1599960, episode return:0.13182392503387963 \n","global step:1601552, episode return:0.24526481956338736 \n","global step:1603144, episode return:0.11176468534896242 \n","global step:1604736, episode return:0.02189027775959855 \n","global step:1606328, episode return:0.2159929150725787 \n","global step:1607920, episode return:0.15910293739461012 \n","global step:1609512, episode return:0.06905194767086524 \n","global step:1611104, episode return:0.08242751987842187 \n","global step:1612696, episode return:0.11236281505923548 \n","global step:1614288, episode return:0.10056241646309702 \n"]}]},{"cell_type":"code","source":[""],"metadata":{"id":"gizKimZllcSs"},"execution_count":null,"outputs":[]}]}